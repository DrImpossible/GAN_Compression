Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar10', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=2, from_modelzoo=False, growth=8, layers=40, learningratescheduler='cifarschedular', logdir='../logs/densenetbc_cifar10_40_8', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenet_cifar', momentum=0.9, name='densenetbc_cifar10_40_8', nclasses=10, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(40, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(40, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(72, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(40, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(72, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(80, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (88 -> 10)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 20.790	Data 0.246	Loss 1.470	Prec@1 45.6360	Prec@5 90.7640	
Val: [0]	Time 1.519	Data 0.126	Loss 1.743	Prec@1 43.7300	Prec@5 93.5400	
Best Prec@1: [43.730]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 19.367	Data 0.252	Loss 1.013	Prec@1 63.9420	Prec@5 96.6120	
Val: [1]	Time 1.503	Data 0.099	Loss 0.919	Prec@1 66.9500	Prec@5 97.3300	
Best Prec@1: [66.950]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 19.626	Data 0.224	Loss 0.854	Prec@1 69.8980	Prec@5 97.6760	
Val: [2]	Time 1.534	Data 0.103	Loss 0.888	Prec@1 69.2800	Prec@5 97.8800	
Best Prec@1: [69.280]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 20.031	Data 0.270	Loss 0.752	Prec@1 73.7020	Prec@5 98.2220	
Val: [3]	Time 1.490	Data 0.109	Loss 1.055	Prec@1 65.5500	Prec@5 96.6500	
Best Prec@1: [69.280]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 19.783	Data 0.255	Loss 0.692	Prec@1 76.0380	Prec@5 98.5900	
Val: [4]	Time 1.529	Data 0.112	Loss 1.101	Prec@1 66.1400	Prec@5 97.4800	
Best Prec@1: [69.280]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 19.915	Data 0.264	Loss 0.640	Prec@1 77.8320	Prec@5 98.8020	
Val: [5]	Time 1.579	Data 0.121	Loss 0.768	Prec@1 74.6300	Prec@5 98.6700	
Best Prec@1: [74.630]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 19.624	Data 0.275	Loss 0.608	Prec@1 78.9040	Prec@5 98.8780	
Val: [6]	Time 1.557	Data 0.119	Loss 0.744	Prec@1 75.6400	Prec@5 98.1000	
Best Prec@1: [75.640]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 19.825	Data 0.290	Loss 0.581	Prec@1 79.9360	Prec@5 98.9980	
Val: [7]	Time 1.562	Data 0.104	Loss 0.772	Prec@1 73.5900	Prec@5 98.7400	
Best Prec@1: [75.640]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 19.777	Data 0.260	Loss 0.566	Prec@1 80.4920	Prec@5 98.9720	
Val: [8]	Time 1.610	Data 0.111	Loss 0.657	Prec@1 78.4400	Prec@5 98.4400	
Best Prec@1: [78.440]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 19.580	Data 0.276	Loss 0.545	Prec@1 81.2560	Prec@5 99.0880	
Val: [9]	Time 1.512	Data 0.118	Loss 0.813	Prec@1 73.2500	Prec@5 98.4800	
Best Prec@1: [78.440]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 19.565	Data 0.255	Loss 0.532	Prec@1 81.6680	Prec@5 99.1140	
Val: [10]	Time 1.463	Data 0.095	Loss 0.618	Prec@1 79.5700	Prec@5 99.0200	
Best Prec@1: [79.570]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 19.610	Data 0.226	Loss 0.522	Prec@1 81.9340	Prec@5 99.1560	
Val: [11]	Time 1.540	Data 0.115	Loss 0.673	Prec@1 77.5700	Prec@5 98.3900	
Best Prec@1: [79.570]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 19.579	Data 0.241	Loss 0.512	Prec@1 82.3780	Prec@5 99.2260	
Val: [12]	Time 1.571	Data 0.125	Loss 0.589	Prec@1 79.8400	Prec@5 99.0600	
Best Prec@1: [79.840]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 19.581	Data 0.227	Loss 0.499	Prec@1 82.6980	Prec@5 99.2420	
Val: [13]	Time 1.503	Data 0.103	Loss 0.603	Prec@1 79.8100	Prec@5 99.0100	
Best Prec@1: [79.840]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 19.540	Data 0.234	Loss 0.499	Prec@1 82.9520	Prec@5 99.2360	
Val: [14]	Time 1.613	Data 0.135	Loss 0.523	Prec@1 82.2300	Prec@5 99.1900	
Best Prec@1: [82.230]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 19.527	Data 0.227	Loss 0.487	Prec@1 83.1720	Prec@5 99.3220	
Val: [15]	Time 1.562	Data 0.119	Loss 0.535	Prec@1 81.6900	Prec@5 99.1300	
Best Prec@1: [82.230]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 19.602	Data 0.240	Loss 0.483	Prec@1 83.3800	Prec@5 99.2320	
Val: [16]	Time 1.538	Data 0.115	Loss 0.800	Prec@1 73.9300	Prec@5 98.6600	
Best Prec@1: [82.230]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 19.646	Data 0.234	Loss 0.480	Prec@1 83.6120	Prec@5 99.2800	
Val: [17]	Time 1.615	Data 0.102	Loss 0.589	Prec@1 80.3400	Prec@5 98.8300	
Best Prec@1: [82.230]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 19.818	Data 0.238	Loss 0.466	Prec@1 83.7780	Prec@5 99.3060	
Val: [18]	Time 1.520	Data 0.095	Loss 0.615	Prec@1 79.9400	Prec@5 99.0000	
Best Prec@1: [82.230]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 19.664	Data 0.221	Loss 0.466	Prec@1 84.0760	Prec@5 99.3700	
Val: [19]	Time 1.539	Data 0.110	Loss 0.595	Prec@1 79.9700	Prec@5 99.0600	
Best Prec@1: [82.230]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 19.498	Data 0.222	Loss 0.463	Prec@1 84.0640	Prec@5 99.2820	
Val: [20]	Time 1.546	Data 0.107	Loss 0.575	Prec@1 81.0600	Prec@5 98.8200	
Best Prec@1: [82.230]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 19.532	Data 0.233	Loss 0.454	Prec@1 84.2160	Prec@5 99.3200	
Val: [21]	Time 1.534	Data 0.110	Loss 0.586	Prec@1 80.8000	Prec@5 99.0600	
Best Prec@1: [82.230]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 19.581	Data 0.239	Loss 0.450	Prec@1 84.4000	Prec@5 99.3860	
Val: [22]	Time 1.501	Data 0.109	Loss 0.622	Prec@1 79.9400	Prec@5 98.6600	
Best Prec@1: [82.230]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 19.654	Data 0.244	Loss 0.449	Prec@1 84.5440	Prec@5 99.4060	
Val: [23]	Time 1.557	Data 0.108	Loss 0.499	Prec@1 83.2600	Prec@5 98.9800	
Best Prec@1: [83.260]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 19.544	Data 0.229	Loss 0.448	Prec@1 84.6400	Prec@5 99.3260	
Val: [24]	Time 1.497	Data 0.101	Loss 0.618	Prec@1 79.3600	Prec@5 99.0500	
Best Prec@1: [83.260]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 19.706	Data 0.228	Loss 0.438	Prec@1 84.8480	Prec@5 99.4380	
Val: [25]	Time 1.544	Data 0.112	Loss 0.581	Prec@1 81.9000	Prec@5 99.1700	
Best Prec@1: [83.260]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 19.749	Data 0.237	Loss 0.437	Prec@1 84.9260	Prec@5 99.4040	
Val: [26]	Time 1.624	Data 0.142	Loss 0.618	Prec@1 79.8800	Prec@5 98.9300	
Best Prec@1: [83.260]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 19.602	Data 0.234	Loss 0.433	Prec@1 85.0080	Prec@5 99.4080	
Val: [27]	Time 1.586	Data 0.106	Loss 0.796	Prec@1 75.2600	Prec@5 98.5100	
Best Prec@1: [83.260]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 19.547	Data 0.233	Loss 0.432	Prec@1 85.1100	Prec@5 99.4400	
Val: [28]	Time 1.596	Data 0.136	Loss 0.545	Prec@1 82.1500	Prec@5 99.2500	
Best Prec@1: [83.260]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 19.655	Data 0.236	Loss 0.429	Prec@1 85.2000	Prec@5 99.4900	
Val: [29]	Time 1.449	Data 0.107	Loss 0.498	Prec@1 83.4900	Prec@5 99.3300	
Best Prec@1: [83.490]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 19.509	Data 0.242	Loss 0.429	Prec@1 85.1900	Prec@5 99.4360	
Val: [30]	Time 1.507	Data 0.098	Loss 0.490	Prec@1 83.7800	Prec@5 99.3000	
Best Prec@1: [83.780]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 19.382	Data 0.224	Loss 0.418	Prec@1 85.6040	Prec@5 99.4480	
Val: [31]	Time 1.592	Data 0.128	Loss 0.523	Prec@1 82.8300	Prec@5 99.1800	
Best Prec@1: [83.780]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 19.579	Data 0.227	Loss 0.422	Prec@1 85.4180	Prec@5 99.4860	
Val: [32]	Time 1.653	Data 0.118	Loss 0.566	Prec@1 81.7300	Prec@5 99.3400	
Best Prec@1: [83.780]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 19.649	Data 0.252	Loss 0.423	Prec@1 85.4480	Prec@5 99.4140	
Val: [33]	Time 1.553	Data 0.112	Loss 0.584	Prec@1 80.9500	Prec@5 98.8900	
Best Prec@1: [83.780]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 19.762	Data 0.232	Loss 0.414	Prec@1 85.7120	Prec@5 99.4900	
Val: [34]	Time 1.510	Data 0.101	Loss 0.520	Prec@1 83.1800	Prec@5 99.3500	
Best Prec@1: [83.780]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 19.550	Data 0.237	Loss 0.415	Prec@1 85.6840	Prec@5 99.4800	
Val: [35]	Time 1.598	Data 0.111	Loss 0.524	Prec@1 82.5200	Prec@5 99.0900	
Best Prec@1: [83.780]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 19.700	Data 0.218	Loss 0.413	Prec@1 85.8440	Prec@5 99.5060	
Val: [36]	Time 1.506	Data 0.106	Loss 0.518	Prec@1 82.9000	Prec@5 99.0600	
Best Prec@1: [83.780]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 19.799	Data 0.232	Loss 0.413	Prec@1 85.8340	Prec@5 99.4920	
Val: [37]	Time 1.543	Data 0.115	Loss 0.677	Prec@1 78.3300	Prec@5 98.8800	
Best Prec@1: [83.780]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 19.613	Data 0.230	Loss 0.411	Prec@1 85.7100	Prec@5 99.4820	
Val: [38]	Time 1.601	Data 0.129	Loss 0.569	Prec@1 81.2500	Prec@5 99.1000	
Best Prec@1: [83.780]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 19.430	Data 0.217	Loss 0.407	Prec@1 85.9860	Prec@5 99.4660	
Val: [39]	Time 1.536	Data 0.107	Loss 0.492	Prec@1 83.6900	Prec@5 99.3500	
Best Prec@1: [83.780]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 19.302	Data 0.223	Loss 0.406	Prec@1 85.9800	Prec@5 99.4620	
Val: [40]	Time 1.458	Data 0.107	Loss 0.591	Prec@1 81.2200	Prec@5 98.8300	
Best Prec@1: [83.780]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 19.678	Data 0.230	Loss 0.406	Prec@1 85.9080	Prec@5 99.4800	
Val: [41]	Time 1.538	Data 0.111	Loss 0.468	Prec@1 84.7400	Prec@5 99.3600	
Best Prec@1: [84.740]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 19.662	Data 0.235	Loss 0.401	Prec@1 86.1180	Prec@5 99.5480	
Val: [42]	Time 1.581	Data 0.112	Loss 0.490	Prec@1 84.0300	Prec@5 99.1400	
Best Prec@1: [84.740]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 19.683	Data 0.232	Loss 0.399	Prec@1 86.2620	Prec@5 99.4900	
Val: [43]	Time 1.508	Data 0.107	Loss 0.548	Prec@1 81.5800	Prec@5 99.1800	
Best Prec@1: [84.740]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 19.483	Data 0.237	Loss 0.402	Prec@1 86.1940	Prec@5 99.4960	
Val: [44]	Time 1.596	Data 0.126	Loss 0.523	Prec@1 83.1600	Prec@5 98.8200	
Best Prec@1: [84.740]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 19.660	Data 0.231	Loss 0.398	Prec@1 86.2300	Prec@5 99.5020	
Val: [45]	Time 1.510	Data 0.110	Loss 0.604	Prec@1 80.8500	Prec@5 98.9100	
Best Prec@1: [84.740]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 19.535	Data 0.221	Loss 0.398	Prec@1 86.2940	Prec@5 99.4960	
Val: [46]	Time 1.541	Data 0.098	Loss 0.488	Prec@1 83.4300	Prec@5 99.3500	
Best Prec@1: [84.740]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 19.674	Data 0.232	Loss 0.393	Prec@1 86.5200	Prec@5 99.5240	
Val: [47]	Time 1.572	Data 0.106	Loss 0.568	Prec@1 81.6800	Prec@5 99.2000	
Best Prec@1: [84.740]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 19.638	Data 0.234	Loss 0.394	Prec@1 86.3300	Prec@5 99.4840	
Val: [48]	Time 1.528	Data 0.102	Loss 0.704	Prec@1 78.8100	Prec@5 97.8300	
Best Prec@1: [84.740]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 19.474	Data 0.226	Loss 0.397	Prec@1 86.2960	Prec@5 99.4900	
Val: [49]	Time 1.510	Data 0.112	Loss 0.493	Prec@1 83.5400	Prec@5 99.1600	
Best Prec@1: [84.740]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 19.633	Data 0.235	Loss 0.392	Prec@1 86.5200	Prec@5 99.4840	
Val: [50]	Time 1.606	Data 0.119	Loss 0.547	Prec@1 81.9600	Prec@5 99.1600	
Best Prec@1: [84.740]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 19.640	Data 0.240	Loss 0.393	Prec@1 86.4620	Prec@5 99.4840	
Val: [51]	Time 1.564	Data 0.127	Loss 0.519	Prec@1 83.1500	Prec@5 99.2200	
Best Prec@1: [84.740]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 19.677	Data 0.218	Loss 0.390	Prec@1 86.5620	Prec@5 99.5460	
Val: [52]	Time 1.549	Data 0.118	Loss 0.662	Prec@1 79.1600	Prec@5 98.7100	
Best Prec@1: [84.740]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 19.666	Data 0.231	Loss 0.388	Prec@1 86.6680	Prec@5 99.5180	
Val: [53]	Time 1.537	Data 0.100	Loss 0.528	Prec@1 82.6500	Prec@5 99.0300	
Best Prec@1: [84.740]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 19.581	Data 0.224	Loss 0.387	Prec@1 86.6080	Prec@5 99.5260	
Val: [54]	Time 1.503	Data 0.106	Loss 0.624	Prec@1 80.9000	Prec@5 99.0800	
Best Prec@1: [84.740]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 19.629	Data 0.233	Loss 0.386	Prec@1 86.6160	Prec@5 99.5320	
Val: [55]	Time 1.521	Data 0.126	Loss 0.689	Prec@1 79.7900	Prec@5 99.1900	
Best Prec@1: [84.740]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 19.397	Data 0.215	Loss 0.386	Prec@1 86.6860	Prec@5 99.5500	
Val: [56]	Time 1.593	Data 0.109	Loss 0.448	Prec@1 84.4600	Prec@5 99.4300	
Best Prec@1: [84.740]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 19.759	Data 0.237	Loss 0.386	Prec@1 86.7520	Prec@5 99.5120	
Val: [57]	Time 1.518	Data 0.103	Loss 0.519	Prec@1 82.8600	Prec@5 99.1200	
Best Prec@1: [84.740]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 19.604	Data 0.216	Loss 0.385	Prec@1 86.8040	Prec@5 99.5260	
Val: [58]	Time 1.560	Data 0.125	Loss 0.486	Prec@1 84.1900	Prec@5 99.2600	
Best Prec@1: [84.740]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 19.816	Data 0.234	Loss 0.385	Prec@1 86.6720	Prec@5 99.4860	
Val: [59]	Time 1.602	Data 0.115	Loss 0.506	Prec@1 82.6300	Prec@5 99.2900	
Best Prec@1: [84.740]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 19.672	Data 0.227	Loss 0.382	Prec@1 86.8020	Prec@5 99.5160	
Val: [60]	Time 1.560	Data 0.116	Loss 0.662	Prec@1 78.1400	Prec@5 99.2000	
Best Prec@1: [84.740]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 19.793	Data 0.231	Loss 0.382	Prec@1 86.9420	Prec@5 99.5600	
Val: [61]	Time 1.560	Data 0.115	Loss 0.532	Prec@1 81.7300	Prec@5 99.0900	
Best Prec@1: [84.740]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 19.624	Data 0.223	Loss 0.378	Prec@1 86.9020	Prec@5 99.5620	
Val: [62]	Time 1.581	Data 0.114	Loss 0.498	Prec@1 84.1900	Prec@5 99.1400	
Best Prec@1: [84.740]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 19.595	Data 0.236	Loss 0.380	Prec@1 86.9360	Prec@5 99.4960	
Val: [63]	Time 1.533	Data 0.110	Loss 0.490	Prec@1 83.7100	Prec@5 99.2100	
Best Prec@1: [84.740]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 19.720	Data 0.234	Loss 0.380	Prec@1 86.7300	Prec@5 99.5540	
Val: [64]	Time 1.469	Data 0.096	Loss 0.425	Prec@1 85.4500	Prec@5 99.4800	
Best Prec@1: [85.450]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 19.529	Data 0.241	Loss 0.378	Prec@1 86.9820	Prec@5 99.5680	
Val: [65]	Time 1.541	Data 0.108	Loss 0.462	Prec@1 84.3500	Prec@5 99.4400	
Best Prec@1: [85.450]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 19.431	Data 0.232	Loss 0.378	Prec@1 86.8820	Prec@5 99.5620	
Val: [66]	Time 1.534	Data 0.113	Loss 0.606	Prec@1 80.7200	Prec@5 99.2500	
Best Prec@1: [85.450]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 19.664	Data 0.228	Loss 0.378	Prec@1 87.0620	Prec@5 99.5740	
Val: [67]	Time 1.531	Data 0.121	Loss 0.590	Prec@1 81.2800	Prec@5 99.1500	
Best Prec@1: [85.450]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 19.406	Data 0.224	Loss 0.373	Prec@1 87.1440	Prec@5 99.6220	
Val: [68]	Time 1.581	Data 0.129	Loss 0.536	Prec@1 82.9800	Prec@5 99.2500	
Best Prec@1: [85.450]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 19.804	Data 0.239	Loss 0.377	Prec@1 87.0760	Prec@5 99.5520	
Val: [69]	Time 1.513	Data 0.111	Loss 0.549	Prec@1 82.4900	Prec@5 99.3900	
Best Prec@1: [85.450]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 19.580	Data 0.218	Loss 0.377	Prec@1 87.0420	Prec@5 99.5560	
Val: [70]	Time 1.562	Data 0.128	Loss 0.627	Prec@1 80.8100	Prec@5 98.2500	
Best Prec@1: [85.450]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 19.470	Data 0.236	Loss 0.375	Prec@1 87.1060	Prec@5 99.5820	
Val: [71]	Time 1.499	Data 0.122	Loss 0.469	Prec@1 84.8300	Prec@5 99.1300	
Best Prec@1: [85.450]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 19.614	Data 0.218	Loss 0.373	Prec@1 87.0340	Prec@5 99.5480	
Val: [72]	Time 1.507	Data 0.100	Loss 0.467	Prec@1 84.8300	Prec@5 99.2700	
Best Prec@1: [85.450]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 19.488	Data 0.232	Loss 0.375	Prec@1 87.1180	Prec@5 99.5500	
Val: [73]	Time 1.618	Data 0.126	Loss 0.579	Prec@1 80.6700	Prec@5 99.3700	
Best Prec@1: [85.450]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 19.474	Data 0.218	Loss 0.377	Prec@1 86.9860	Prec@5 99.5900	
Val: [74]	Time 1.521	Data 0.107	Loss 0.578	Prec@1 81.4700	Prec@5 99.1100	
Best Prec@1: [85.450]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 19.713	Data 0.237	Loss 0.368	Prec@1 87.2740	Prec@5 99.5780	
Val: [75]	Time 1.542	Data 0.109	Loss 0.444	Prec@1 84.7200	Prec@5 99.4200	
Best Prec@1: [85.450]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 19.608	Data 0.235	Loss 0.370	Prec@1 87.3340	Prec@5 99.5320	
Val: [76]	Time 1.540	Data 0.122	Loss 0.617	Prec@1 80.5700	Prec@5 98.9400	
Best Prec@1: [85.450]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 19.602	Data 0.240	Loss 0.371	Prec@1 87.1520	Prec@5 99.5660	
Val: [77]	Time 1.568	Data 0.114	Loss 0.569	Prec@1 81.6000	Prec@5 99.0600	
Best Prec@1: [85.450]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 19.842	Data 0.230	Loss 0.365	Prec@1 87.3960	Prec@5 99.6120	
Val: [78]	Time 1.543	Data 0.112	Loss 0.591	Prec@1 80.9600	Prec@5 99.1600	
Best Prec@1: [85.450]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 19.519	Data 0.229	Loss 0.373	Prec@1 86.9960	Prec@5 99.5920	
Val: [79]	Time 1.517	Data 0.114	Loss 0.741	Prec@1 77.7100	Prec@5 98.8200	
Best Prec@1: [85.450]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 19.467	Data 0.235	Loss 0.368	Prec@1 87.3620	Prec@5 99.6040	
Val: [80]	Time 1.566	Data 0.110	Loss 0.485	Prec@1 83.9300	Prec@5 99.3000	
Best Prec@1: [85.450]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 19.692	Data 0.232	Loss 0.374	Prec@1 87.1900	Prec@5 99.5980	
Val: [81]	Time 1.540	Data 0.110	Loss 0.471	Prec@1 84.7800	Prec@5 99.0900	
Best Prec@1: [85.450]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 19.767	Data 0.234	Loss 0.370	Prec@1 87.1200	Prec@5 99.6200	
Val: [82]	Time 1.540	Data 0.108	Loss 0.431	Prec@1 85.5200	Prec@5 99.2400	
Best Prec@1: [85.520]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 19.811	Data 0.240	Loss 0.365	Prec@1 87.3500	Prec@5 99.6020	
Val: [83]	Time 1.580	Data 0.146	Loss 0.537	Prec@1 82.6700	Prec@5 99.2600	
Best Prec@1: [85.520]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 19.588	Data 0.229	Loss 0.369	Prec@1 87.1360	Prec@5 99.6060	
Val: [84]	Time 1.525	Data 0.111	Loss 0.454	Prec@1 84.8400	Prec@5 99.2600	
Best Prec@1: [85.520]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 19.646	Data 0.227	Loss 0.364	Prec@1 87.3560	Prec@5 99.6200	
Val: [85]	Time 1.477	Data 0.100	Loss 0.453	Prec@1 85.1500	Prec@5 99.4300	
Best Prec@1: [85.520]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 19.594	Data 0.236	Loss 0.365	Prec@1 87.4960	Prec@5 99.5800	
Val: [86]	Time 1.622	Data 0.121	Loss 0.469	Prec@1 84.2900	Prec@5 99.2500	
Best Prec@1: [85.520]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 19.747	Data 0.220	Loss 0.361	Prec@1 87.5920	Prec@5 99.6180	
Val: [87]	Time 1.561	Data 0.108	Loss 0.479	Prec@1 84.3000	Prec@5 99.2400	
Best Prec@1: [85.520]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 19.712	Data 0.221	Loss 0.362	Prec@1 87.5040	Prec@5 99.5640	
Val: [88]	Time 1.549	Data 0.108	Loss 0.619	Prec@1 81.3200	Prec@5 99.0800	
Best Prec@1: [85.520]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 19.452	Data 0.232	Loss 0.364	Prec@1 87.5820	Prec@5 99.5680	
Val: [89]	Time 1.561	Data 0.114	Loss 0.466	Prec@1 84.6500	Prec@5 99.3500	
Best Prec@1: [85.520]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 19.526	Data 0.232	Loss 0.364	Prec@1 87.4500	Prec@5 99.6260	
Val: [90]	Time 1.527	Data 0.099	Loss 0.595	Prec@1 80.9500	Prec@5 99.0600	
Best Prec@1: [85.520]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 19.411	Data 0.231	Loss 0.363	Prec@1 87.5300	Prec@5 99.6080	
Val: [91]	Time 1.481	Data 0.109	Loss 0.613	Prec@1 80.6500	Prec@5 99.1900	
Best Prec@1: [85.520]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 19.457	Data 0.229	Loss 0.360	Prec@1 87.6180	Prec@5 99.6260	
Val: [92]	Time 1.697	Data 0.117	Loss 0.541	Prec@1 82.6000	Prec@5 99.3500	
Best Prec@1: [85.520]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 19.521	Data 0.235	Loss 0.364	Prec@1 87.3160	Prec@5 99.6280	
Val: [93]	Time 1.556	Data 0.114	Loss 0.489	Prec@1 84.1800	Prec@5 99.3200	
Best Prec@1: [85.520]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 19.617	Data 0.238	Loss 0.359	Prec@1 87.6560	Prec@5 99.5860	
Val: [94]	Time 1.515	Data 0.106	Loss 0.635	Prec@1 81.4000	Prec@5 98.7600	
Best Prec@1: [85.520]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 19.503	Data 0.221	Loss 0.358	Prec@1 87.7100	Prec@5 99.6280	
Val: [95]	Time 1.592	Data 0.121	Loss 0.526	Prec@1 83.2900	Prec@5 99.3500	
Best Prec@1: [85.520]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 19.507	Data 0.234	Loss 0.362	Prec@1 87.6180	Prec@5 99.6020	
Val: [96]	Time 1.525	Data 0.110	Loss 0.459	Prec@1 85.0100	Prec@5 99.1900	
Best Prec@1: [85.520]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 19.638	Data 0.233	Loss 0.360	Prec@1 87.7320	Prec@5 99.5920	
Val: [97]	Time 1.519	Data 0.121	Loss 0.523	Prec@1 82.8600	Prec@5 99.2500	
Best Prec@1: [85.520]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 19.632	Data 0.216	Loss 0.358	Prec@1 87.6800	Prec@5 99.5920	
Val: [98]	Time 1.536	Data 0.106	Loss 0.606	Prec@1 80.6700	Prec@5 99.1600	
Best Prec@1: [85.520]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 19.527	Data 0.223	Loss 0.364	Prec@1 87.4260	Prec@5 99.5700	
Val: [99]	Time 1.552	Data 0.108	Loss 0.436	Prec@1 85.0100	Prec@5 99.4600	
Best Prec@1: [85.520]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 19.407	Data 0.217	Loss 0.358	Prec@1 87.7140	Prec@5 99.6120	
Val: [100]	Time 1.448	Data 0.095	Loss 0.524	Prec@1 83.2200	Prec@5 98.9800	
Best Prec@1: [85.520]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 19.431	Data 0.245	Loss 0.360	Prec@1 87.5380	Prec@5 99.5680	
Val: [101]	Time 1.542	Data 0.109	Loss 0.578	Prec@1 82.3500	Prec@5 99.2300	
Best Prec@1: [85.520]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 19.632	Data 0.234	Loss 0.358	Prec@1 87.5180	Prec@5 99.6040	
Val: [102]	Time 1.585	Data 0.118	Loss 0.410	Prec@1 86.2900	Prec@5 99.4000	
Best Prec@1: [86.290]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 19.576	Data 0.235	Loss 0.358	Prec@1 87.6840	Prec@5 99.6420	
Val: [103]	Time 1.495	Data 0.105	Loss 0.478	Prec@1 84.5200	Prec@5 99.0500	
Best Prec@1: [86.290]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 19.544	Data 0.242	Loss 0.357	Prec@1 87.5900	Prec@5 99.6280	
Val: [104]	Time 1.555	Data 0.112	Loss 0.613	Prec@1 81.0400	Prec@5 98.7900	
Best Prec@1: [86.290]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 19.508	Data 0.229	Loss 0.358	Prec@1 87.5640	Prec@5 99.6160	
Val: [105]	Time 1.476	Data 0.113	Loss 0.496	Prec@1 83.7500	Prec@5 99.3700	
Best Prec@1: [86.290]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 19.619	Data 0.215	Loss 0.352	Prec@1 87.7840	Prec@5 99.6220	
Val: [106]	Time 1.520	Data 0.097	Loss 0.440	Prec@1 85.5300	Prec@5 99.4700	
Best Prec@1: [86.290]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 19.546	Data 0.231	Loss 0.358	Prec@1 87.8360	Prec@5 99.5840	
Val: [107]	Time 1.494	Data 0.097	Loss 0.548	Prec@1 82.4500	Prec@5 99.2200	
Best Prec@1: [86.290]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 19.547	Data 0.241	Loss 0.358	Prec@1 87.6420	Prec@5 99.5940	
Val: [108]	Time 1.590	Data 0.115	Loss 0.546	Prec@1 82.8000	Prec@5 99.0400	
Best Prec@1: [86.290]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 19.513	Data 0.231	Loss 0.356	Prec@1 87.7960	Prec@5 99.6180	
Val: [109]	Time 1.522	Data 0.098	Loss 0.502	Prec@1 83.6100	Prec@5 99.2300	
Best Prec@1: [86.290]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 19.589	Data 0.220	Loss 0.353	Prec@1 87.7880	Prec@5 99.6260	
Val: [110]	Time 1.554	Data 0.113	Loss 0.588	Prec@1 81.5100	Prec@5 98.9800	
Best Prec@1: [86.290]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 19.520	Data 0.223	Loss 0.357	Prec@1 87.4520	Prec@5 99.6580	
Val: [111]	Time 1.497	Data 0.103	Loss 0.490	Prec@1 83.8200	Prec@5 99.2300	
Best Prec@1: [86.290]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 19.492	Data 0.219	Loss 0.353	Prec@1 87.7400	Prec@5 99.6460	
Val: [112]	Time 1.522	Data 0.106	Loss 0.571	Prec@1 82.0300	Prec@5 99.0500	
Best Prec@1: [86.290]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 19.766	Data 0.236	Loss 0.350	Prec@1 87.9540	Prec@5 99.6180	
Val: [113]	Time 1.552	Data 0.107	Loss 0.437	Prec@1 85.6200	Prec@5 99.5300	
Best Prec@1: [86.290]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 19.554	Data 0.235	Loss 0.352	Prec@1 87.7440	Prec@5 99.6380	
Val: [114]	Time 1.539	Data 0.122	Loss 0.473	Prec@1 84.9000	Prec@5 99.4100	
Best Prec@1: [86.290]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 19.565	Data 0.220	Loss 0.354	Prec@1 88.0280	Prec@5 99.5740	
Val: [115]	Time 1.532	Data 0.098	Loss 0.447	Prec@1 85.4100	Prec@5 99.2800	
Best Prec@1: [86.290]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 19.635	Data 0.221	Loss 0.357	Prec@1 87.6480	Prec@5 99.6360	
Val: [116]	Time 1.547	Data 0.105	Loss 0.462	Prec@1 85.4300	Prec@5 99.3800	
Best Prec@1: [86.290]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 19.331	Data 0.227	Loss 0.350	Prec@1 87.8700	Prec@5 99.6280	
Val: [117]	Time 1.516	Data 0.106	Loss 0.464	Prec@1 85.0200	Prec@5 99.4100	
Best Prec@1: [86.290]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 19.507	Data 0.223	Loss 0.350	Prec@1 87.9620	Prec@5 99.6640	
Val: [118]	Time 1.537	Data 0.112	Loss 0.476	Prec@1 84.6100	Prec@5 99.2800	
Best Prec@1: [86.290]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 19.510	Data 0.234	Loss 0.355	Prec@1 87.7820	Prec@5 99.5780	
Val: [119]	Time 1.502	Data 0.107	Loss 0.544	Prec@1 82.6500	Prec@5 98.9300	
Best Prec@1: [86.290]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 19.528	Data 0.219	Loss 0.351	Prec@1 87.8980	Prec@5 99.6480	
Val: [120]	Time 1.561	Data 0.101	Loss 0.535	Prec@1 82.5000	Prec@5 99.1900	
Best Prec@1: [86.290]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 19.598	Data 0.234	Loss 0.353	Prec@1 87.7000	Prec@5 99.6580	
Val: [121]	Time 1.494	Data 0.107	Loss 0.436	Prec@1 85.4900	Prec@5 99.6000	
Best Prec@1: [86.290]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 19.713	Data 0.218	Loss 0.346	Prec@1 87.9980	Prec@5 99.6020	
Val: [122]	Time 1.489	Data 0.098	Loss 0.501	Prec@1 83.0600	Prec@5 99.3600	
Best Prec@1: [86.290]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 19.359	Data 0.231	Loss 0.351	Prec@1 87.9560	Prec@5 99.6300	
Val: [123]	Time 1.518	Data 0.106	Loss 0.436	Prec@1 85.7900	Prec@5 99.3600	
Best Prec@1: [86.290]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 19.522	Data 0.214	Loss 0.350	Prec@1 88.0160	Prec@5 99.6640	
Val: [124]	Time 1.488	Data 0.106	Loss 0.462	Prec@1 84.7300	Prec@5 99.1200	
Best Prec@1: [86.290]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 19.540	Data 0.225	Loss 0.348	Prec@1 87.9700	Prec@5 99.6480	
Val: [125]	Time 1.519	Data 0.128	Loss 0.683	Prec@1 79.3100	Prec@5 98.9100	
Best Prec@1: [86.290]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 19.439	Data 0.222	Loss 0.351	Prec@1 87.8360	Prec@5 99.6280	
Val: [126]	Time 1.479	Data 0.112	Loss 0.561	Prec@1 81.9500	Prec@5 99.4600	
Best Prec@1: [86.290]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 19.537	Data 0.220	Loss 0.349	Prec@1 87.9080	Prec@5 99.6280	
Val: [127]	Time 1.582	Data 0.108	Loss 0.569	Prec@1 81.8500	Prec@5 99.0000	
Best Prec@1: [86.290]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 19.541	Data 0.224	Loss 0.345	Prec@1 87.9620	Prec@5 99.6620	
Val: [128]	Time 1.515	Data 0.109	Loss 0.498	Prec@1 83.9500	Prec@5 99.2800	
Best Prec@1: [86.290]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 19.470	Data 0.228	Loss 0.350	Prec@1 87.7500	Prec@5 99.6400	
Val: [129]	Time 1.474	Data 0.104	Loss 0.418	Prec@1 86.3300	Prec@5 99.4200	
Best Prec@1: [86.330]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 19.701	Data 0.225	Loss 0.344	Prec@1 88.0820	Prec@5 99.6260	
Val: [130]	Time 1.557	Data 0.108	Loss 0.424	Prec@1 85.8900	Prec@5 99.4000	
Best Prec@1: [86.330]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 19.438	Data 0.232	Loss 0.345	Prec@1 87.9720	Prec@5 99.6440	
Val: [131]	Time 1.550	Data 0.097	Loss 0.555	Prec@1 83.0700	Prec@5 99.2800	
Best Prec@1: [86.330]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 19.525	Data 0.220	Loss 0.351	Prec@1 87.9220	Prec@5 99.6220	
Val: [132]	Time 1.544	Data 0.111	Loss 0.390	Prec@1 86.9800	Prec@5 99.5600	
Best Prec@1: [86.980]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 19.523	Data 0.222	Loss 0.344	Prec@1 88.0260	Prec@5 99.6220	
Val: [133]	Time 1.536	Data 0.112	Loss 0.428	Prec@1 85.9600	Prec@5 99.5800	
Best Prec@1: [86.980]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 19.495	Data 0.234	Loss 0.342	Prec@1 88.1260	Prec@5 99.6660	
Val: [134]	Time 1.533	Data 0.106	Loss 0.563	Prec@1 82.7000	Prec@5 98.9000	
Best Prec@1: [86.980]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 19.481	Data 0.218	Loss 0.348	Prec@1 87.8940	Prec@5 99.5920	
Val: [135]	Time 1.525	Data 0.097	Loss 0.410	Prec@1 86.0900	Prec@5 99.5300	
Best Prec@1: [86.980]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 19.630	Data 0.239	Loss 0.344	Prec@1 88.1400	Prec@5 99.5840	
Val: [136]	Time 1.497	Data 0.103	Loss 0.516	Prec@1 82.7000	Prec@5 99.1500	
Best Prec@1: [86.980]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 19.580	Data 0.216	Loss 0.341	Prec@1 88.1760	Prec@5 99.6380	
Val: [137]	Time 1.533	Data 0.104	Loss 0.420	Prec@1 86.0900	Prec@5 99.3900	
Best Prec@1: [86.980]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 19.557	Data 0.221	Loss 0.345	Prec@1 88.1960	Prec@5 99.6280	
Val: [138]	Time 1.479	Data 0.104	Loss 0.614	Prec@1 81.3800	Prec@5 98.9800	
Best Prec@1: [86.980]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 19.572	Data 0.230	Loss 0.345	Prec@1 88.0400	Prec@5 99.6560	
Val: [139]	Time 1.544	Data 0.113	Loss 0.474	Prec@1 84.5700	Prec@5 99.2500	
Best Prec@1: [86.980]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 19.538	Data 0.215	Loss 0.345	Prec@1 88.0820	Prec@5 99.6380	
Val: [140]	Time 1.629	Data 0.111	Loss 0.554	Prec@1 82.5600	Prec@5 99.2400	
Best Prec@1: [86.980]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 19.582	Data 0.214	Loss 0.348	Prec@1 88.0020	Prec@5 99.6160	
Val: [141]	Time 1.598	Data 0.125	Loss 0.486	Prec@1 84.2800	Prec@5 99.4300	
Best Prec@1: [86.980]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 19.582	Data 0.241	Loss 0.345	Prec@1 88.0780	Prec@5 99.6280	
Val: [142]	Time 1.488	Data 0.106	Loss 0.485	Prec@1 83.8200	Prec@5 99.4600	
Best Prec@1: [86.980]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 19.442	Data 0.220	Loss 0.342	Prec@1 88.1240	Prec@5 99.6280	
Val: [143]	Time 1.585	Data 0.122	Loss 0.509	Prec@1 83.7800	Prec@5 99.2300	
Best Prec@1: [86.980]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 19.563	Data 0.233	Loss 0.346	Prec@1 88.1620	Prec@5 99.6300	
Val: [144]	Time 1.533	Data 0.112	Loss 0.494	Prec@1 83.7200	Prec@5 99.4400	
Best Prec@1: [86.980]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 19.593	Data 0.224	Loss 0.345	Prec@1 88.0540	Prec@5 99.6540	
Val: [145]	Time 1.533	Data 0.122	Loss 0.482	Prec@1 84.5200	Prec@5 99.5400	
Best Prec@1: [86.980]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 19.432	Data 0.240	Loss 0.340	Prec@1 88.2260	Prec@5 99.6460	
Val: [146]	Time 1.497	Data 0.101	Loss 0.470	Prec@1 84.8900	Prec@5 99.3700	
Best Prec@1: [86.980]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 19.525	Data 0.232	Loss 0.343	Prec@1 88.2940	Prec@5 99.6180	
Val: [147]	Time 1.491	Data 0.110	Loss 0.438	Prec@1 85.6200	Prec@5 99.4400	
Best Prec@1: [86.980]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 19.683	Data 0.217	Loss 0.345	Prec@1 88.1120	Prec@5 99.6620	
Val: [148]	Time 1.517	Data 0.116	Loss 0.553	Prec@1 82.2400	Prec@5 99.1600	
Best Prec@1: [86.980]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 19.521	Data 0.239	Loss 0.345	Prec@1 88.1780	Prec@5 99.6200	
Val: [149]	Time 1.531	Data 0.111	Loss 0.515	Prec@1 83.8700	Prec@5 99.4000	
Best Prec@1: [86.980]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 19.471	Data 0.221	Loss 0.233	Prec@1 92.1240	Prec@5 99.8260	
Val: [150]	Time 1.562	Data 0.110	Loss 0.276	Prec@1 90.9100	Prec@5 99.6700	
Best Prec@1: [90.910]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 19.549	Data 0.236	Loss 0.198	Prec@1 93.2760	Prec@5 99.8640	
Val: [151]	Time 1.488	Data 0.107	Loss 0.282	Prec@1 90.9600	Prec@5 99.7500	
Best Prec@1: [90.960]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 19.236	Data 0.220	Loss 0.186	Prec@1 93.6800	Prec@5 99.8600	
Val: [152]	Time 1.596	Data 0.110	Loss 0.272	Prec@1 91.2000	Prec@5 99.7300	
Best Prec@1: [91.200]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 19.603	Data 0.225	Loss 0.180	Prec@1 93.8360	Prec@5 99.9040	
Val: [153]	Time 1.564	Data 0.134	Loss 0.276	Prec@1 90.8800	Prec@5 99.7300	
Best Prec@1: [91.200]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 19.636	Data 0.217	Loss 0.175	Prec@1 94.0160	Prec@5 99.8880	
Val: [154]	Time 1.539	Data 0.107	Loss 0.263	Prec@1 91.5500	Prec@5 99.7400	
Best Prec@1: [91.550]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 19.553	Data 0.225	Loss 0.167	Prec@1 94.1760	Prec@5 99.9080	
Val: [155]	Time 1.606	Data 0.109	Loss 0.266	Prec@1 91.4600	Prec@5 99.7300	
Best Prec@1: [91.550]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 19.602	Data 0.235	Loss 0.164	Prec@1 94.4060	Prec@5 99.9120	
Val: [156]	Time 1.531	Data 0.115	Loss 0.275	Prec@1 91.0300	Prec@5 99.6800	
Best Prec@1: [91.550]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 19.763	Data 0.237	Loss 0.160	Prec@1 94.4600	Prec@5 99.9180	
Val: [157]	Time 1.510	Data 0.108	Loss 0.273	Prec@1 91.4400	Prec@5 99.7400	
Best Prec@1: [91.550]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 19.520	Data 0.223	Loss 0.157	Prec@1 94.6160	Prec@5 99.9240	
Val: [158]	Time 1.593	Data 0.122	Loss 0.268	Prec@1 91.6100	Prec@5 99.7700	
Best Prec@1: [91.610]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 19.545	Data 0.216	Loss 0.155	Prec@1 94.6900	Prec@5 99.9340	
Val: [159]	Time 1.524	Data 0.114	Loss 0.272	Prec@1 91.4200	Prec@5 99.7500	
Best Prec@1: [91.610]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 19.479	Data 0.227	Loss 0.150	Prec@1 94.8120	Prec@5 99.9180	
Val: [160]	Time 1.458	Data 0.112	Loss 0.271	Prec@1 91.3900	Prec@5 99.7200	
Best Prec@1: [91.610]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 19.640	Data 0.236	Loss 0.149	Prec@1 94.7920	Prec@5 99.9400	
Val: [161]	Time 1.552	Data 0.127	Loss 0.271	Prec@1 91.3200	Prec@5 99.7700	
Best Prec@1: [91.610]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 19.727	Data 0.221	Loss 0.150	Prec@1 94.8360	Prec@5 99.9480	
Val: [162]	Time 1.498	Data 0.104	Loss 0.276	Prec@1 91.6100	Prec@5 99.7400	
Best Prec@1: [91.610]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 19.732	Data 0.225	Loss 0.146	Prec@1 94.9260	Prec@5 99.9200	
Val: [163]	Time 1.524	Data 0.134	Loss 0.270	Prec@1 91.7100	Prec@5 99.7400	
Best Prec@1: [91.710]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 19.494	Data 0.222	Loss 0.145	Prec@1 94.9720	Prec@5 99.9500	
Val: [164]	Time 1.614	Data 0.121	Loss 0.274	Prec@1 91.4700	Prec@5 99.7000	
Best Prec@1: [91.710]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 19.481	Data 0.223	Loss 0.142	Prec@1 94.9640	Prec@5 99.9420	
Val: [165]	Time 1.515	Data 0.106	Loss 0.277	Prec@1 91.4200	Prec@5 99.7600	
Best Prec@1: [91.710]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 19.619	Data 0.222	Loss 0.140	Prec@1 95.1280	Prec@5 99.9300	
Val: [166]	Time 1.635	Data 0.199	Loss 0.281	Prec@1 91.3400	Prec@5 99.7500	
Best Prec@1: [91.710]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 19.473	Data 0.229	Loss 0.143	Prec@1 94.9380	Prec@5 99.9500	
Val: [167]	Time 1.606	Data 0.117	Loss 0.281	Prec@1 91.5100	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 19.602	Data 0.224	Loss 0.139	Prec@1 95.1320	Prec@5 99.9360	
Val: [168]	Time 1.534	Data 0.098	Loss 0.283	Prec@1 91.4900	Prec@5 99.7500	
Best Prec@1: [91.710]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 19.615	Data 0.230	Loss 0.137	Prec@1 95.2520	Prec@5 99.9380	
Val: [169]	Time 1.523	Data 0.109	Loss 0.281	Prec@1 91.5600	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 19.474	Data 0.218	Loss 0.136	Prec@1 95.3060	Prec@5 99.9360	
Val: [170]	Time 1.553	Data 0.116	Loss 0.282	Prec@1 91.4900	Prec@5 99.7000	
Best Prec@1: [91.710]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 19.450	Data 0.229	Loss 0.134	Prec@1 95.2700	Prec@5 99.9520	
Val: [171]	Time 1.507	Data 0.099	Loss 0.285	Prec@1 91.5400	Prec@5 99.7100	
Best Prec@1: [91.710]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 19.397	Data 0.216	Loss 0.135	Prec@1 95.2180	Prec@5 99.9460	
Val: [172]	Time 1.550	Data 0.115	Loss 0.278	Prec@1 91.3900	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 19.497	Data 0.220	Loss 0.135	Prec@1 95.2940	Prec@5 99.9540	
Val: [173]	Time 1.533	Data 0.119	Loss 0.288	Prec@1 91.3200	Prec@5 99.7900	
Best Prec@1: [91.710]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 19.583	Data 0.224	Loss 0.133	Prec@1 95.2800	Prec@5 99.9660	
Val: [174]	Time 1.482	Data 0.117	Loss 0.292	Prec@1 91.2200	Prec@5 99.7500	
Best Prec@1: [91.710]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 19.607	Data 0.227	Loss 0.131	Prec@1 95.3580	Prec@5 99.9640	
Val: [175]	Time 1.521	Data 0.111	Loss 0.284	Prec@1 91.4000	Prec@5 99.7700	
Best Prec@1: [91.710]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 19.631	Data 0.220	Loss 0.132	Prec@1 95.3400	Prec@5 99.9480	
Val: [176]	Time 1.571	Data 0.120	Loss 0.294	Prec@1 91.2700	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 19.435	Data 0.229	Loss 0.131	Prec@1 95.4420	Prec@5 99.9520	
Val: [177]	Time 1.531	Data 0.113	Loss 0.291	Prec@1 91.1900	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 19.691	Data 0.232	Loss 0.131	Prec@1 95.3240	Prec@5 99.9700	
Val: [178]	Time 1.526	Data 0.110	Loss 0.284	Prec@1 91.3100	Prec@5 99.7600	
Best Prec@1: [91.710]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 19.543	Data 0.223	Loss 0.131	Prec@1 95.5200	Prec@5 99.9640	
Val: [179]	Time 1.508	Data 0.107	Loss 0.298	Prec@1 90.9400	Prec@5 99.7000	
Best Prec@1: [91.710]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 19.515	Data 0.241	Loss 0.129	Prec@1 95.4840	Prec@5 99.9620	
Val: [180]	Time 1.520	Data 0.116	Loss 0.288	Prec@1 91.3200	Prec@5 99.7600	
Best Prec@1: [91.710]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 19.541	Data 0.238	Loss 0.126	Prec@1 95.5100	Prec@5 99.9700	
Val: [181]	Time 1.477	Data 0.105	Loss 0.301	Prec@1 91.0400	Prec@5 99.6900	
Best Prec@1: [91.710]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 19.769	Data 0.228	Loss 0.126	Prec@1 95.6260	Prec@5 99.9600	
Val: [182]	Time 1.498	Data 0.107	Loss 0.285	Prec@1 91.4200	Prec@5 99.7600	
Best Prec@1: [91.710]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 19.520	Data 0.221	Loss 0.125	Prec@1 95.6080	Prec@5 99.9600	
Val: [183]	Time 1.515	Data 0.112	Loss 0.299	Prec@1 90.9800	Prec@5 99.6900	
Best Prec@1: [91.710]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 19.800	Data 0.236	Loss 0.125	Prec@1 95.6080	Prec@5 99.9640	
Val: [184]	Time 1.509	Data 0.125	Loss 0.304	Prec@1 91.0800	Prec@5 99.7000	
Best Prec@1: [91.710]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 19.288	Data 0.227	Loss 0.126	Prec@1 95.5380	Prec@5 99.9500	
Val: [185]	Time 1.528	Data 0.126	Loss 0.304	Prec@1 90.9500	Prec@5 99.7000	
Best Prec@1: [91.710]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 19.642	Data 0.232	Loss 0.126	Prec@1 95.5320	Prec@5 99.9800	
Val: [186]	Time 1.462	Data 0.111	Loss 0.303	Prec@1 91.0600	Prec@5 99.6400	
Best Prec@1: [91.710]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 19.470	Data 0.223	Loss 0.125	Prec@1 95.6180	Prec@5 99.9620	
Val: [187]	Time 1.512	Data 0.119	Loss 0.307	Prec@1 90.9900	Prec@5 99.7400	
Best Prec@1: [91.710]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 19.799	Data 0.227	Loss 0.125	Prec@1 95.6520	Prec@5 99.9600	
Val: [188]	Time 1.582	Data 0.125	Loss 0.306	Prec@1 91.0700	Prec@5 99.7600	
Best Prec@1: [91.710]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 19.655	Data 0.233	Loss 0.125	Prec@1 95.6280	Prec@5 99.9640	
Val: [189]	Time 1.510	Data 0.108	Loss 0.302	Prec@1 90.8100	Prec@5 99.6800	
Best Prec@1: [91.710]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 19.591	Data 0.223	Loss 0.122	Prec@1 95.7600	Prec@5 99.9740	
Val: [190]	Time 1.572	Data 0.114	Loss 0.299	Prec@1 90.9700	Prec@5 99.7400	
Best Prec@1: [91.710]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 19.711	Data 0.231	Loss 0.121	Prec@1 95.7940	Prec@5 99.9720	
Val: [191]	Time 1.506	Data 0.123	Loss 0.327	Prec@1 90.7400	Prec@5 99.7600	
Best Prec@1: [91.710]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 19.636	Data 0.246	Loss 0.123	Prec@1 95.4920	Prec@5 99.9700	
Val: [192]	Time 1.591	Data 0.106	Loss 0.300	Prec@1 91.0100	Prec@5 99.7200	
Best Prec@1: [91.710]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 19.606	Data 0.230	Loss 0.125	Prec@1 95.5260	Prec@5 99.9700	
Val: [193]	Time 1.510	Data 0.107	Loss 0.307	Prec@1 90.9300	Prec@5 99.8300	
Best Prec@1: [91.710]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 19.412	Data 0.234	Loss 0.127	Prec@1 95.4480	Prec@5 99.9720	
Val: [194]	Time 1.524	Data 0.104	Loss 0.314	Prec@1 90.6500	Prec@5 99.7600	
Best Prec@1: [91.710]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 19.593	Data 0.231	Loss 0.126	Prec@1 95.5320	Prec@5 99.9740	
Val: [195]	Time 1.559	Data 0.113	Loss 0.318	Prec@1 90.8000	Prec@5 99.6200	
Best Prec@1: [91.710]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 19.708	Data 0.228	Loss 0.126	Prec@1 95.5240	Prec@5 99.9720	
Val: [196]	Time 1.544	Data 0.101	Loss 0.308	Prec@1 90.6800	Prec@5 99.7000	
Best Prec@1: [91.710]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 19.473	Data 0.236	Loss 0.126	Prec@1 95.4780	Prec@5 99.9560	
Val: [197]	Time 1.587	Data 0.124	Loss 0.308	Prec@1 91.0200	Prec@5 99.7500	
Best Prec@1: [91.710]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 19.543	Data 0.228	Loss 0.125	Prec@1 95.6300	Prec@5 99.9660	
Val: [198]	Time 1.500	Data 0.105	Loss 0.308	Prec@1 91.1600	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 19.474	Data 0.249	Loss 0.126	Prec@1 95.5740	Prec@5 99.9680	
Val: [199]	Time 1.583	Data 0.148	Loss 0.332	Prec@1 90.3400	Prec@5 99.6400	
Best Prec@1: [91.710]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 19.535	Data 0.225	Loss 0.128	Prec@1 95.5520	Prec@5 99.9640	
Val: [200]	Time 1.500	Data 0.093	Loss 0.317	Prec@1 90.7000	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 201 Learning rate: 0.010000000000000002
Train: [201]	Time 19.561	Data 0.221	Loss 0.124	Prec@1 95.5380	Prec@5 99.9780	
Val: [201]	Time 1.448	Data 0.097	Loss 0.331	Prec@1 90.5200	Prec@5 99.7200	
Best Prec@1: [91.710]	
Starting epoch number: 202 Learning rate: 0.010000000000000002
Train: [202]	Time 19.358	Data 0.233	Loss 0.124	Prec@1 95.5200	Prec@5 99.9820	
Val: [202]	Time 1.527	Data 0.108	Loss 0.319	Prec@1 90.6600	Prec@5 99.7100	
Best Prec@1: [91.710]	
Starting epoch number: 203 Learning rate: 0.010000000000000002
Train: [203]	Time 19.293	Data 0.222	Loss 0.123	Prec@1 95.6500	Prec@5 99.9660	
Val: [203]	Time 1.608	Data 0.112	Loss 0.339	Prec@1 90.1200	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 204 Learning rate: 0.010000000000000002
Train: [204]	Time 19.545	Data 0.233	Loss 0.128	Prec@1 95.3760	Prec@5 99.9720	
Val: [204]	Time 1.515	Data 0.101	Loss 0.331	Prec@1 90.5800	Prec@5 99.6600	
Best Prec@1: [91.710]	
Starting epoch number: 205 Learning rate: 0.010000000000000002
Train: [205]	Time 20.028	Data 0.235	Loss 0.126	Prec@1 95.6040	Prec@5 99.9720	
Val: [205]	Time 1.571	Data 0.121	Loss 0.348	Prec@1 90.1300	Prec@5 99.7100	
Best Prec@1: [91.710]	
Starting epoch number: 206 Learning rate: 0.010000000000000002
Train: [206]	Time 19.390	Data 0.220	Loss 0.125	Prec@1 95.5740	Prec@5 99.9660	
Val: [206]	Time 1.512	Data 0.102	Loss 0.322	Prec@1 90.3800	Prec@5 99.7400	
Best Prec@1: [91.710]	
Starting epoch number: 207 Learning rate: 0.010000000000000002
Train: [207]	Time 19.752	Data 0.217	Loss 0.125	Prec@1 95.5380	Prec@5 99.9680	
Val: [207]	Time 1.556	Data 0.112	Loss 0.330	Prec@1 90.2900	Prec@5 99.7200	
Best Prec@1: [91.710]	
Starting epoch number: 208 Learning rate: 0.010000000000000002
Train: [208]	Time 19.734	Data 0.221	Loss 0.126	Prec@1 95.5920	Prec@5 99.9660	
Val: [208]	Time 1.563	Data 0.108	Loss 0.325	Prec@1 90.2500	Prec@5 99.7600	
Best Prec@1: [91.710]	
Starting epoch number: 209 Learning rate: 0.010000000000000002
Train: [209]	Time 19.736	Data 0.232	Loss 0.122	Prec@1 95.6740	Prec@5 99.9740	
Val: [209]	Time 1.525	Data 0.120	Loss 0.316	Prec@1 90.4800	Prec@5 99.7000	
Best Prec@1: [91.710]	
Starting epoch number: 210 Learning rate: 0.010000000000000002
Train: [210]	Time 19.490	Data 0.221	Loss 0.126	Prec@1 95.4940	Prec@5 99.9620	
Val: [210]	Time 1.549	Data 0.117	Loss 0.305	Prec@1 90.9300	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 211 Learning rate: 0.010000000000000002
Train: [211]	Time 19.450	Data 0.233	Loss 0.127	Prec@1 95.4040	Prec@5 99.9680	
Val: [211]	Time 1.435	Data 0.101	Loss 0.333	Prec@1 90.4800	Prec@5 99.6500	
Best Prec@1: [91.710]	
Starting epoch number: 212 Learning rate: 0.010000000000000002
Train: [212]	Time 19.838	Data 0.241	Loss 0.124	Prec@1 95.5740	Prec@5 99.9700	
Val: [212]	Time 1.573	Data 0.121	Loss 0.334	Prec@1 90.2400	Prec@5 99.6800	
Best Prec@1: [91.710]	
Starting epoch number: 213 Learning rate: 0.010000000000000002
Train: [213]	Time 19.501	Data 0.235	Loss 0.125	Prec@1 95.6940	Prec@5 99.9580	
Val: [213]	Time 1.486	Data 0.113	Loss 0.344	Prec@1 90.0900	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 214 Learning rate: 0.010000000000000002
Train: [214]	Time 19.418	Data 0.231	Loss 0.122	Prec@1 95.7320	Prec@5 99.9680	
Val: [214]	Time 1.467	Data 0.102	Loss 0.358	Prec@1 90.0600	Prec@5 99.7200	
Best Prec@1: [91.710]	
Starting epoch number: 215 Learning rate: 0.010000000000000002
Train: [215]	Time 19.640	Data 0.230	Loss 0.124	Prec@1 95.6200	Prec@5 99.9680	
Val: [215]	Time 1.548	Data 0.121	Loss 0.313	Prec@1 90.6400	Prec@5 99.7500	
Best Prec@1: [91.710]	
Starting epoch number: 216 Learning rate: 0.010000000000000002
Train: [216]	Time 19.456	Data 0.229	Loss 0.124	Prec@1 95.6480	Prec@5 99.9600	
Val: [216]	Time 1.485	Data 0.107	Loss 0.321	Prec@1 90.8900	Prec@5 99.7200	
Best Prec@1: [91.710]	
Starting epoch number: 217 Learning rate: 0.010000000000000002
Train: [217]	Time 19.600	Data 0.231	Loss 0.123	Prec@1 95.6200	Prec@5 99.9660	
Val: [217]	Time 1.576	Data 0.098	Loss 0.328	Prec@1 90.5800	Prec@5 99.6100	
Best Prec@1: [91.710]	
Starting epoch number: 218 Learning rate: 0.010000000000000002
Train: [218]	Time 19.364	Data 0.223	Loss 0.125	Prec@1 95.6340	Prec@5 99.9620	
Val: [218]	Time 1.504	Data 0.110	Loss 0.317	Prec@1 91.0200	Prec@5 99.7200	
Best Prec@1: [91.710]	
Starting epoch number: 219 Learning rate: 0.010000000000000002
Train: [219]	Time 19.431	Data 0.230	Loss 0.124	Prec@1 95.6400	Prec@5 99.9620	
Val: [219]	Time 1.526	Data 0.107	Loss 0.312	Prec@1 90.9600	Prec@5 99.7100	
Best Prec@1: [91.710]	
Starting epoch number: 220 Learning rate: 0.010000000000000002
Train: [220]	Time 19.594	Data 0.234	Loss 0.124	Prec@1 95.6140	Prec@5 99.9740	
Val: [220]	Time 1.513	Data 0.114	Loss 0.327	Prec@1 90.7300	Prec@5 99.6900	
Best Prec@1: [91.710]	
Starting epoch number: 221 Learning rate: 0.010000000000000002
Train: [221]	Time 19.622	Data 0.228	Loss 0.125	Prec@1 95.5540	Prec@5 99.9680	
Val: [221]	Time 1.586	Data 0.111	Loss 0.336	Prec@1 90.4300	Prec@5 99.7300	
Best Prec@1: [91.710]	
Starting epoch number: 222 Learning rate: 0.010000000000000002
Train: [222]	Time 19.352	Data 0.242	Loss 0.126	Prec@1 95.4880	Prec@5 99.9640	
Val: [222]	Time 1.547	Data 0.112	Loss 0.329	Prec@1 90.5500	Prec@5 99.7100	
Best Prec@1: [91.710]	
Starting epoch number: 223 Learning rate: 0.010000000000000002
Train: [223]	Time 19.622	Data 0.224	Loss 0.123	Prec@1 95.6840	Prec@5 99.9660	
Val: [223]	Time 1.480	Data 0.108	Loss 0.320	Prec@1 90.5900	Prec@5 99.7100	
Best Prec@1: [91.710]	
Starting epoch number: 224 Learning rate: 0.010000000000000002
Train: [224]	Time 19.685	Data 0.236	Loss 0.128	Prec@1 95.4940	Prec@5 99.9560	
Val: [224]	Time 1.532	Data 0.103	Loss 0.339	Prec@1 90.1400	Prec@5 99.6500	
Best Prec@1: [91.710]	
Starting epoch number: 225 Learning rate: 0.0010000000000000002
Train: [225]	Time 19.491	Data 0.230	Loss 0.095	Prec@1 96.7380	Prec@5 99.9680	
Val: [225]	Time 1.546	Data 0.110	Loss 0.293	Prec@1 91.3700	Prec@5 99.7700	
Best Prec@1: [91.710]	
Starting epoch number: 226 Learning rate: 0.0010000000000000002
Train: [226]	Time 19.734	Data 0.218	Loss 0.084	Prec@1 97.1940	Prec@5 99.9860	
Val: [226]	Time 1.513	Data 0.112	Loss 0.282	Prec@1 91.8300	Prec@5 99.7500	
Best Prec@1: [91.830]	
Starting epoch number: 227 Learning rate: 0.0010000000000000002
Train: [227]	Time 19.508	Data 0.231	Loss 0.077	Prec@1 97.4560	Prec@5 99.9900	
Val: [227]	Time 1.562	Data 0.114	Loss 0.282	Prec@1 91.9800	Prec@5 99.8000	
Best Prec@1: [91.980]	
Starting epoch number: 228 Learning rate: 0.0010000000000000002
Train: [228]	Time 19.575	Data 0.219	Loss 0.075	Prec@1 97.4960	Prec@5 99.9800	
Val: [228]	Time 1.500	Data 0.120	Loss 0.278	Prec@1 91.7800	Prec@5 99.7800	
Best Prec@1: [91.980]	
Starting epoch number: 229 Learning rate: 0.0010000000000000002
Train: [229]	Time 19.615	Data 0.226	Loss 0.072	Prec@1 97.6760	Prec@5 99.9920	
Val: [229]	Time 1.534	Data 0.104	Loss 0.282	Prec@1 91.9100	Prec@5 99.8200	
Best Prec@1: [91.980]	
Starting epoch number: 230 Learning rate: 0.0010000000000000002
Train: [230]	Time 19.471	Data 0.232	Loss 0.071	Prec@1 97.6920	Prec@5 99.9840	
Val: [230]	Time 1.565	Data 0.118	Loss 0.281	Prec@1 91.9500	Prec@5 99.8100	
Best Prec@1: [91.980]	
Starting epoch number: 231 Learning rate: 0.0010000000000000002
Train: [231]	Time 19.327	Data 0.226	Loss 0.070	Prec@1 97.6260	Prec@5 99.9940	
Val: [231]	Time 1.551	Data 0.109	Loss 0.281	Prec@1 91.9900	Prec@5 99.7800	
Best Prec@1: [91.990]	
Starting epoch number: 232 Learning rate: 0.0010000000000000002
Train: [232]	Time 19.512	Data 0.227	Loss 0.070	Prec@1 97.6980	Prec@5 99.9940	
Val: [232]	Time 1.503	Data 0.107	Loss 0.284	Prec@1 91.9200	Prec@5 99.7900	
Best Prec@1: [91.990]	
Starting epoch number: 233 Learning rate: 0.0010000000000000002
Train: [233]	Time 19.518	Data 0.221	Loss 0.067	Prec@1 97.7860	Prec@5 100.0000	
Val: [233]	Time 1.510	Data 0.106	Loss 0.284	Prec@1 91.8400	Prec@5 99.8100	
Best Prec@1: [91.990]	
Starting epoch number: 234 Learning rate: 0.0010000000000000002
Train: [234]	Time 19.509	Data 0.216	Loss 0.067	Prec@1 97.8460	Prec@5 99.9960	
Val: [234]	Time 1.545	Data 0.101	Loss 0.282	Prec@1 91.9400	Prec@5 99.7700	
Best Prec@1: [91.990]	
Starting epoch number: 235 Learning rate: 0.0010000000000000002
Train: [235]	Time 19.821	Data 0.224	Loss 0.067	Prec@1 97.8080	Prec@5 99.9880	
Val: [235]	Time 1.515	Data 0.100	Loss 0.286	Prec@1 91.8600	Prec@5 99.7700	
Best Prec@1: [91.990]	
Starting epoch number: 236 Learning rate: 0.0010000000000000002
Train: [236]	Time 19.364	Data 0.226	Loss 0.066	Prec@1 97.8460	Prec@5 99.9900	
Val: [236]	Time 1.530	Data 0.108	Loss 0.282	Prec@1 92.0300	Prec@5 99.7900	
Best Prec@1: [92.030]	
Starting epoch number: 237 Learning rate: 0.0010000000000000002
Train: [237]	Time 19.497	Data 0.229	Loss 0.066	Prec@1 97.8800	Prec@5 99.9840	
Val: [237]	Time 1.490	Data 0.096	Loss 0.288	Prec@1 91.7400	Prec@5 99.8200	
Best Prec@1: [92.030]	
Starting epoch number: 238 Learning rate: 0.0010000000000000002
Train: [238]	Time 19.524	Data 0.228	Loss 0.065	Prec@1 97.8420	Prec@5 99.9920	
Val: [238]	Time 1.554	Data 0.127	Loss 0.285	Prec@1 91.9500	Prec@5 99.7800	
Best Prec@1: [92.030]	
Starting epoch number: 239 Learning rate: 0.0010000000000000002
Train: [239]	Time 19.706	Data 0.220	Loss 0.063	Prec@1 97.9620	Prec@5 99.9960	
Val: [239]	Time 1.500	Data 0.094	Loss 0.291	Prec@1 91.9100	Prec@5 99.8300	
Best Prec@1: [92.030]	
Starting epoch number: 240 Learning rate: 0.0010000000000000002
Train: [240]	Time 19.685	Data 0.246	Loss 0.063	Prec@1 97.9600	Prec@5 99.9960	
Val: [240]	Time 1.621	Data 0.146	Loss 0.293	Prec@1 91.7500	Prec@5 99.8000	
Best Prec@1: [92.030]	
Starting epoch number: 241 Learning rate: 0.0010000000000000002
Train: [241]	Time 19.620	Data 0.232	Loss 0.061	Prec@1 98.0780	Prec@5 99.9960	
Val: [241]	Time 1.459	Data 0.100	Loss 0.290	Prec@1 92.0200	Prec@5 99.7500	
Best Prec@1: [92.030]	
Starting epoch number: 242 Learning rate: 0.0010000000000000002
Train: [242]	Time 19.358	Data 0.230	Loss 0.063	Prec@1 97.9700	Prec@5 99.9920	
Val: [242]	Time 1.565	Data 0.109	Loss 0.285	Prec@1 91.9600	Prec@5 99.7600	
Best Prec@1: [92.030]	
Starting epoch number: 243 Learning rate: 0.0010000000000000002
Train: [243]	Time 19.367	Data 0.240	Loss 0.062	Prec@1 98.0160	Prec@5 99.9920	
Val: [243]	Time 1.485	Data 0.106	Loss 0.285	Prec@1 91.9700	Prec@5 99.8000	
Best Prec@1: [92.030]	
Starting epoch number: 244 Learning rate: 0.0010000000000000002
Train: [244]	Time 19.475	Data 0.232	Loss 0.059	Prec@1 98.1220	Prec@5 99.9940	
Val: [244]	Time 1.527	Data 0.101	Loss 0.290	Prec@1 91.9100	Prec@5 99.7800	
Best Prec@1: [92.030]	
Starting epoch number: 245 Learning rate: 0.0010000000000000002
Train: [245]	Time 19.376	Data 0.226	Loss 0.061	Prec@1 98.0280	Prec@5 99.9960	
Val: [245]	Time 1.585	Data 0.116	Loss 0.292	Prec@1 91.8400	Prec@5 99.7900	
Best Prec@1: [92.030]	
Starting epoch number: 246 Learning rate: 0.0010000000000000002
Train: [246]	Time 19.420	Data 0.218	Loss 0.060	Prec@1 98.1080	Prec@5 99.9900	
Val: [246]	Time 1.550	Data 0.107	Loss 0.291	Prec@1 91.7100	Prec@5 99.7600	
Best Prec@1: [92.030]	
Starting epoch number: 247 Learning rate: 0.0010000000000000002
Train: [247]	Time 19.515	Data 0.227	Loss 0.062	Prec@1 97.9740	Prec@5 99.9980	
Val: [247]	Time 1.513	Data 0.099	Loss 0.295	Prec@1 91.8000	Prec@5 99.7500	
Best Prec@1: [92.030]	
Starting epoch number: 248 Learning rate: 0.0010000000000000002
Train: [248]	Time 19.499	Data 0.222	Loss 0.059	Prec@1 98.1320	Prec@5 99.9980	
Val: [248]	Time 1.511	Data 0.105	Loss 0.294	Prec@1 91.9500	Prec@5 99.8400	
Best Prec@1: [92.030]	
Starting epoch number: 249 Learning rate: 0.0010000000000000002
Train: [249]	Time 19.352	Data 0.218	Loss 0.059	Prec@1 98.0780	Prec@5 99.9980	
Val: [249]	Time 1.690	Data 0.236	Loss 0.296	Prec@1 91.8900	Prec@5 99.8000	
Best Prec@1: [92.030]	
Starting epoch number: 250 Learning rate: 0.0010000000000000002
Train: [250]	Time 19.778	Data 0.226	Loss 0.056	Prec@1 98.2260	Prec@5 99.9960	
Val: [250]	Time 1.533	Data 0.121	Loss 0.295	Prec@1 92.0000	Prec@5 99.8000	
Best Prec@1: [92.030]	
Starting epoch number: 251 Learning rate: 0.0010000000000000002
Train: [251]	Time 19.521	Data 0.229	Loss 0.058	Prec@1 98.0820	Prec@5 99.9960	
Val: [251]	Time 1.508	Data 0.113	Loss 0.293	Prec@1 91.8700	Prec@5 99.7600	
Best Prec@1: [92.030]	
Starting epoch number: 252 Learning rate: 0.0010000000000000002
Train: [252]	Time 19.641	Data 0.219	Loss 0.057	Prec@1 98.1860	Prec@5 99.9980	
Val: [252]	Time 1.552	Data 0.121	Loss 0.295	Prec@1 91.8800	Prec@5 99.8200	
Best Prec@1: [92.030]	
Starting epoch number: 253 Learning rate: 0.0010000000000000002
Train: [253]	Time 19.591	Data 0.228	Loss 0.057	Prec@1 98.1080	Prec@5 99.9960	
Val: [253]	Time 1.493	Data 0.104	Loss 0.294	Prec@1 91.8500	Prec@5 99.7200	
Best Prec@1: [92.030]	
Starting epoch number: 254 Learning rate: 0.0010000000000000002
Train: [254]	Time 19.478	Data 0.222	Loss 0.057	Prec@1 98.1220	Prec@5 100.0000	
Val: [254]	Time 1.523	Data 0.103	Loss 0.294	Prec@1 92.0400	Prec@5 99.7700	
Best Prec@1: [92.040]	
Starting epoch number: 255 Learning rate: 0.0010000000000000002
Train: [255]	Time 19.348	Data 0.221	Loss 0.055	Prec@1 98.2520	Prec@5 100.0000	
Val: [255]	Time 1.561	Data 0.128	Loss 0.299	Prec@1 91.9100	Prec@5 99.7600	
Best Prec@1: [92.040]	
Starting epoch number: 256 Learning rate: 0.0010000000000000002
Train: [256]	Time 19.429	Data 0.229	Loss 0.056	Prec@1 98.2000	Prec@5 99.9980	
Val: [256]	Time 1.542	Data 0.112	Loss 0.300	Prec@1 91.8100	Prec@5 99.8300	
Best Prec@1: [92.040]	
Starting epoch number: 257 Learning rate: 0.0010000000000000002
Train: [257]	Time 19.540	Data 0.238	Loss 0.056	Prec@1 98.1260	Prec@5 99.9960	
Val: [257]	Time 1.525	Data 0.091	Loss 0.297	Prec@1 91.9300	Prec@5 99.7800	
Best Prec@1: [92.040]	
Starting epoch number: 258 Learning rate: 0.0010000000000000002
Train: [258]	Time 19.780	Data 0.224	Loss 0.055	Prec@1 98.1840	Prec@5 99.9960	
Val: [258]	Time 1.546	Data 0.106	Loss 0.295	Prec@1 92.1000	Prec@5 99.8300	
Best Prec@1: [92.100]	
Starting epoch number: 259 Learning rate: 0.0010000000000000002
Train: [259]	Time 19.617	Data 0.243	Loss 0.056	Prec@1 98.1720	Prec@5 99.9980	
Val: [259]	Time 1.513	Data 0.100	Loss 0.298	Prec@1 91.9800	Prec@5 99.7900	
Best Prec@1: [92.100]	
Starting epoch number: 260 Learning rate: 0.0010000000000000002
Train: [260]	Time 19.637	Data 0.223	Loss 0.055	Prec@1 98.2000	Prec@5 100.0000	
Val: [260]	Time 1.563	Data 0.123	Loss 0.299	Prec@1 92.0200	Prec@5 99.7800	
Best Prec@1: [92.100]	
Starting epoch number: 261 Learning rate: 0.0010000000000000002
Train: [261]	Time 19.413	Data 0.232	Loss 0.057	Prec@1 98.0700	Prec@5 99.9960	
Val: [261]	Time 1.541	Data 0.104	Loss 0.300	Prec@1 91.9900	Prec@5 99.7600	
Best Prec@1: [92.100]	
Starting epoch number: 262 Learning rate: 0.0010000000000000002
Train: [262]	Time 19.714	Data 0.238	Loss 0.054	Prec@1 98.2060	Prec@5 99.9960	
Val: [262]	Time 1.509	Data 0.103	Loss 0.297	Prec@1 91.9400	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 263 Learning rate: 0.0010000000000000002
Train: [263]	Time 19.591	Data 0.224	Loss 0.054	Prec@1 98.2660	Prec@5 99.9980	
Val: [263]	Time 1.489	Data 0.120	Loss 0.306	Prec@1 91.8900	Prec@5 99.8200	
Best Prec@1: [92.100]	
Starting epoch number: 264 Learning rate: 0.0010000000000000002
Train: [264]	Time 19.491	Data 0.224	Loss 0.054	Prec@1 98.2080	Prec@5 99.9940	
Val: [264]	Time 1.511	Data 0.096	Loss 0.307	Prec@1 91.9400	Prec@5 99.7800	
Best Prec@1: [92.100]	
Starting epoch number: 265 Learning rate: 0.0010000000000000002
Train: [265]	Time 19.462	Data 0.228	Loss 0.055	Prec@1 98.1440	Prec@5 99.9980	
Val: [265]	Time 1.480	Data 0.110	Loss 0.306	Prec@1 91.8300	Prec@5 99.7500	
Best Prec@1: [92.100]	
Starting epoch number: 266 Learning rate: 0.0010000000000000002
Train: [266]	Time 19.557	Data 0.237	Loss 0.056	Prec@1 98.1740	Prec@5 99.9920	
Val: [266]	Time 1.498	Data 0.094	Loss 0.306	Prec@1 91.7900	Prec@5 99.7500	
Best Prec@1: [92.100]	
Starting epoch number: 267 Learning rate: 0.0010000000000000002
Train: [267]	Time 19.443	Data 0.220	Loss 0.053	Prec@1 98.3140	Prec@5 100.0000	
Val: [267]	Time 1.470	Data 0.113	Loss 0.307	Prec@1 91.9100	Prec@5 99.7300	
Best Prec@1: [92.100]	
Starting epoch number: 268 Learning rate: 0.0010000000000000002
Train: [268]	Time 19.578	Data 0.224	Loss 0.053	Prec@1 98.2960	Prec@5 99.9920	
Val: [268]	Time 1.564	Data 0.117	Loss 0.310	Prec@1 91.6500	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 269 Learning rate: 0.0010000000000000002
Train: [269]	Time 19.573	Data 0.235	Loss 0.055	Prec@1 98.1940	Prec@5 99.9920	
Val: [269]	Time 1.558	Data 0.102	Loss 0.310	Prec@1 91.8300	Prec@5 99.7100	
Best Prec@1: [92.100]	
Starting epoch number: 270 Learning rate: 0.0010000000000000002
Train: [270]	Time 19.613	Data 0.244	Loss 0.053	Prec@1 98.3180	Prec@5 100.0000	
Val: [270]	Time 1.546	Data 0.116	Loss 0.307	Prec@1 91.8700	Prec@5 99.8000	
Best Prec@1: [92.100]	
Starting epoch number: 271 Learning rate: 0.0010000000000000002
Train: [271]	Time 19.411	Data 0.228	Loss 0.052	Prec@1 98.3340	Prec@5 99.9920	
Val: [271]	Time 1.542	Data 0.114	Loss 0.309	Prec@1 91.7400	Prec@5 99.8000	
Best Prec@1: [92.100]	
Starting epoch number: 272 Learning rate: 0.0010000000000000002
Train: [272]	Time 19.419	Data 0.231	Loss 0.052	Prec@1 98.2960	Prec@5 99.9940	
Val: [272]	Time 1.565	Data 0.122	Loss 0.313	Prec@1 91.7000	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 273 Learning rate: 0.0010000000000000002
Train: [273]	Time 19.612	Data 0.218	Loss 0.054	Prec@1 98.2340	Prec@5 99.9960	
Val: [273]	Time 1.575	Data 0.118	Loss 0.303	Prec@1 91.8800	Prec@5 99.8400	
Best Prec@1: [92.100]	
Starting epoch number: 274 Learning rate: 0.0010000000000000002
Train: [274]	Time 19.671	Data 0.233	Loss 0.053	Prec@1 98.3160	Prec@5 99.9980	
Val: [274]	Time 1.497	Data 0.106	Loss 0.301	Prec@1 91.9500	Prec@5 99.7900	
Best Prec@1: [92.100]	
Starting epoch number: 275 Learning rate: 0.0010000000000000002
Train: [275]	Time 19.437	Data 0.227	Loss 0.051	Prec@1 98.3640	Prec@5 100.0000	
Val: [275]	Time 1.597	Data 0.113	Loss 0.310	Prec@1 91.7100	Prec@5 99.7600	
Best Prec@1: [92.100]	
Starting epoch number: 276 Learning rate: 0.0010000000000000002
Train: [276]	Time 19.415	Data 0.234	Loss 0.052	Prec@1 98.3120	Prec@5 99.9920	
Val: [276]	Time 1.527	Data 0.110	Loss 0.315	Prec@1 91.5600	Prec@5 99.7000	
Best Prec@1: [92.100]	
Starting epoch number: 277 Learning rate: 0.0010000000000000002
Train: [277]	Time 19.772	Data 0.236	Loss 0.051	Prec@1 98.3040	Prec@5 99.9960	
Val: [277]	Time 1.491	Data 0.103	Loss 0.316	Prec@1 91.4100	Prec@5 99.7400	
Best Prec@1: [92.100]	
Starting epoch number: 278 Learning rate: 0.0010000000000000002
Train: [278]	Time 19.375	Data 0.221	Loss 0.050	Prec@1 98.3680	Prec@5 100.0000	
Val: [278]	Time 1.512	Data 0.109	Loss 0.313	Prec@1 91.6200	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 279 Learning rate: 0.0010000000000000002
Train: [279]	Time 19.343	Data 0.222	Loss 0.051	Prec@1 98.3640	Prec@5 99.9980	
Val: [279]	Time 1.498	Data 0.125	Loss 0.318	Prec@1 91.6300	Prec@5 99.7800	
Best Prec@1: [92.100]	
Starting epoch number: 280 Learning rate: 0.0010000000000000002
Train: [280]	Time 19.422	Data 0.231	Loss 0.052	Prec@1 98.3440	Prec@5 100.0000	
Val: [280]	Time 1.493	Data 0.107	Loss 0.307	Prec@1 91.8700	Prec@5 99.7400	
Best Prec@1: [92.100]	
Starting epoch number: 281 Learning rate: 0.0010000000000000002
Train: [281]	Time 19.404	Data 0.225	Loss 0.050	Prec@1 98.4000	Prec@5 99.9960	
Val: [281]	Time 1.519	Data 0.111	Loss 0.310	Prec@1 91.7800	Prec@5 99.7500	
Best Prec@1: [92.100]	
Starting epoch number: 282 Learning rate: 0.0010000000000000002
Train: [282]	Time 19.526	Data 0.229	Loss 0.051	Prec@1 98.2800	Prec@5 99.9940	
Val: [282]	Time 1.500	Data 0.107	Loss 0.310	Prec@1 91.7500	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 283 Learning rate: 0.0010000000000000002
Train: [283]	Time 19.576	Data 0.228	Loss 0.051	Prec@1 98.3320	Prec@5 100.0000	
Val: [283]	Time 1.538	Data 0.111	Loss 0.309	Prec@1 91.9600	Prec@5 99.8100	
Best Prec@1: [92.100]	
Starting epoch number: 284 Learning rate: 0.0010000000000000002
Train: [284]	Time 19.397	Data 0.216	Loss 0.051	Prec@1 98.3420	Prec@5 99.9980	
Val: [284]	Time 1.513	Data 0.096	Loss 0.311	Prec@1 91.6900	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 285 Learning rate: 0.0010000000000000002
Train: [285]	Time 19.516	Data 0.220	Loss 0.050	Prec@1 98.3840	Prec@5 99.9960	
Val: [285]	Time 1.620	Data 0.138	Loss 0.317	Prec@1 91.6400	Prec@5 99.7400	
Best Prec@1: [92.100]	
Starting epoch number: 286 Learning rate: 0.0010000000000000002
Train: [286]	Time 19.579	Data 0.223	Loss 0.048	Prec@1 98.4560	Prec@5 99.9980	
Val: [286]	Time 1.473	Data 0.089	Loss 0.312	Prec@1 91.7600	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 287 Learning rate: 0.0010000000000000002
Train: [287]	Time 19.383	Data 0.217	Loss 0.049	Prec@1 98.4280	Prec@5 99.9980	
Val: [287]	Time 1.528	Data 0.108	Loss 0.314	Prec@1 91.7100	Prec@5 99.8100	
Best Prec@1: [92.100]	
Starting epoch number: 288 Learning rate: 0.0010000000000000002
Train: [288]	Time 19.501	Data 0.229	Loss 0.048	Prec@1 98.4140	Prec@5 99.9980	
Val: [288]	Time 1.476	Data 0.110	Loss 0.318	Prec@1 91.6700	Prec@5 99.7900	
Best Prec@1: [92.100]	
Starting epoch number: 289 Learning rate: 0.0010000000000000002
Train: [289]	Time 19.574	Data 0.238	Loss 0.049	Prec@1 98.4340	Prec@5 99.9980	
Val: [289]	Time 1.590	Data 0.112	Loss 0.309	Prec@1 91.8400	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 290 Learning rate: 0.0010000000000000002
Train: [290]	Time 19.217	Data 0.219	Loss 0.049	Prec@1 98.3900	Prec@5 99.9980	
Val: [290]	Time 1.519	Data 0.106	Loss 0.314	Prec@1 91.7500	Prec@5 99.7600	
Best Prec@1: [92.100]	
Starting epoch number: 291 Learning rate: 0.0010000000000000002
Train: [291]	Time 19.494	Data 0.219	Loss 0.049	Prec@1 98.3620	Prec@5 100.0000	
Val: [291]	Time 1.489	Data 0.105	Loss 0.319	Prec@1 91.4600	Prec@5 99.7600	
Best Prec@1: [92.100]	
Starting epoch number: 292 Learning rate: 0.0010000000000000002
Train: [292]	Time 19.428	Data 0.217	Loss 0.049	Prec@1 98.4880	Prec@5 99.9920	
Val: [292]	Time 1.492	Data 0.103	Loss 0.309	Prec@1 91.9700	Prec@5 99.7500	
Best Prec@1: [92.100]	
Starting epoch number: 293 Learning rate: 0.0010000000000000002
Train: [293]	Time 19.751	Data 0.230	Loss 0.049	Prec@1 98.3760	Prec@5 99.9960	
Val: [293]	Time 1.430	Data 0.095	Loss 0.317	Prec@1 91.8300	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 294 Learning rate: 0.0010000000000000002
Train: [294]	Time 19.469	Data 0.236	Loss 0.049	Prec@1 98.3700	Prec@5 99.9980	
Val: [294]	Time 1.534	Data 0.119	Loss 0.319	Prec@1 91.6700	Prec@5 99.7700	
Best Prec@1: [92.100]	
Starting epoch number: 295 Learning rate: 0.0010000000000000002
Train: [295]	Time 19.266	Data 0.232	Loss 0.049	Prec@1 98.3860	Prec@5 99.9980	
Val: [295]	Time 1.451	Data 0.105	Loss 0.314	Prec@1 91.8300	Prec@5 99.7500	
Best Prec@1: [92.100]	
Starting epoch number: 296 Learning rate: 0.0010000000000000002
Train: [296]	Time 19.394	Data 0.229	Loss 0.049	Prec@1 98.4040	Prec@5 99.9980	
Val: [296]	Time 1.611	Data 0.105	Loss 0.313	Prec@1 91.8200	Prec@5 99.7300	
Best Prec@1: [92.100]	
Starting epoch number: 297 Learning rate: 0.0010000000000000002
Train: [297]	Time 19.510	Data 0.217	Loss 0.049	Prec@1 98.4360	Prec@5 100.0000	
Val: [297]	Time 1.513	Data 0.102	Loss 0.325	Prec@1 91.6500	Prec@5 99.7300	
Best Prec@1: [92.100]	
Starting epoch number: 298 Learning rate: 0.0010000000000000002
Train: [298]	Time 19.484	Data 0.247	Loss 0.050	Prec@1 98.3620	Prec@5 99.9980	
Val: [298]	Time 1.554	Data 0.107	Loss 0.316	Prec@1 91.7400	Prec@5 99.7600	
Best Prec@1: [92.100]	
Starting epoch number: 299 Learning rate: 0.0010000000000000002
Train: [299]	Time 19.593	Data 0.223	Loss 0.050	Prec@1 98.3260	Prec@5 100.0000	
Val: [299]	Time 1.504	Data 0.112	Loss 0.319	Prec@1 91.6100	Prec@5 99.7800	
Best Prec@1: [92.100]	
