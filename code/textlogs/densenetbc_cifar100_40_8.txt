Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar100', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=2, from_modelzoo=False, growth=8, layers=40, learningratescheduler='cifarschedular', logdir='../logs/densenetbc_cifar100_40_8', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenet_cifar', momentum=0.9, name='densenetbc_cifar100_40_8', nclasses=100, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(40, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(40, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(72, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(80, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(40, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(56, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(72, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(80, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (88 -> 100)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 20.111	Data 0.230	Loss 3.910	Prec@1 9.5600	Prec@5 29.6220	
Val: [0]	Time 1.462	Data 0.128	Loss 3.724	Prec@1 13.3900	Prec@5 38.1100	
Best Prec@1: [13.390]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 19.276	Data 0.295	Loss 3.292	Prec@1 19.2960	Prec@5 47.7760	
Val: [1]	Time 1.598	Data 0.150	Loss 3.113	Prec@1 22.7500	Prec@5 53.6200	
Best Prec@1: [22.750]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 19.189	Data 0.212	Loss 2.821	Prec@1 27.5360	Prec@5 60.3540	
Val: [2]	Time 1.597	Data 0.166	Loss 3.007	Prec@1 26.5900	Prec@5 59.3600	
Best Prec@1: [26.590]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 19.102	Data 0.216	Loss 2.532	Prec@1 33.3860	Prec@5 66.9640	
Val: [3]	Time 1.430	Data 0.140	Loss 2.768	Prec@1 30.7200	Prec@5 63.9300	
Best Prec@1: [30.720]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 19.125	Data 0.213	Loss 2.337	Prec@1 37.7220	Prec@5 71.2980	
Val: [4]	Time 1.203	Data 0.091	Loss 2.458	Prec@1 36.8000	Prec@5 69.9200	
Best Prec@1: [36.800]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 19.179	Data 0.225	Loss 2.206	Prec@1 40.7880	Prec@5 73.8980	
Val: [5]	Time 1.417	Data 0.109	Loss 2.265	Prec@1 40.5400	Prec@5 73.8200	
Best Prec@1: [40.540]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 19.084	Data 0.223	Loss 2.112	Prec@1 42.8840	Prec@5 75.8020	
Val: [6]	Time 1.253	Data 0.107	Loss 2.235	Prec@1 41.2400	Prec@5 74.1500	
Best Prec@1: [41.240]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 19.124	Data 0.223	Loss 2.033	Prec@1 44.7640	Prec@5 77.4480	
Val: [7]	Time 1.379	Data 0.097	Loss 2.261	Prec@1 41.8200	Prec@5 75.0100	
Best Prec@1: [41.820]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 19.238	Data 0.230	Loss 1.972	Prec@1 45.9620	Prec@5 78.6880	
Val: [8]	Time 1.365	Data 0.103	Loss 2.412	Prec@1 39.4600	Prec@5 71.5600	
Best Prec@1: [41.820]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 19.165	Data 0.222	Loss 1.919	Prec@1 47.3640	Prec@5 79.2160	
Val: [9]	Time 1.284	Data 0.102	Loss 2.280	Prec@1 42.4500	Prec@5 74.9300	
Best Prec@1: [42.450]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 19.107	Data 0.212	Loss 1.871	Prec@1 48.6640	Prec@5 80.3940	
Val: [10]	Time 1.542	Data 0.134	Loss 2.192	Prec@1 43.2000	Prec@5 76.1700	
Best Prec@1: [43.200]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 19.144	Data 0.213	Loss 1.834	Prec@1 49.4400	Prec@5 80.8940	
Val: [11]	Time 1.325	Data 0.109	Loss 2.303	Prec@1 42.6100	Prec@5 75.4400	
Best Prec@1: [43.200]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 19.167	Data 0.208	Loss 1.806	Prec@1 50.0740	Prec@5 81.5360	
Val: [12]	Time 1.397	Data 0.128	Loss 1.854	Prec@1 49.7400	Prec@5 80.4900	
Best Prec@1: [49.740]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 19.161	Data 0.219	Loss 1.770	Prec@1 51.2020	Prec@5 81.9660	
Val: [13]	Time 1.408	Data 0.113	Loss 2.089	Prec@1 45.6400	Prec@5 77.1600	
Best Prec@1: [49.740]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 19.107	Data 0.223	Loss 1.751	Prec@1 51.5340	Prec@5 82.4680	
Val: [14]	Time 1.298	Data 0.100	Loss 1.998	Prec@1 47.8300	Prec@5 79.4400	
Best Prec@1: [49.740]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 19.068	Data 0.225	Loss 1.719	Prec@1 52.2260	Prec@5 83.0640	
Val: [15]	Time 1.316	Data 0.106	Loss 2.058	Prec@1 46.0400	Prec@5 78.4300	
Best Prec@1: [49.740]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 19.175	Data 0.223	Loss 1.699	Prec@1 52.7820	Prec@5 83.3000	
Val: [16]	Time 1.244	Data 0.109	Loss 1.967	Prec@1 48.1800	Prec@5 79.5500	
Best Prec@1: [49.740]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 19.060	Data 0.222	Loss 1.688	Prec@1 52.9300	Prec@5 83.3860	
Val: [17]	Time 1.423	Data 0.129	Loss 2.103	Prec@1 46.7100	Prec@5 78.6100	
Best Prec@1: [49.740]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 19.114	Data 0.228	Loss 1.662	Prec@1 53.8100	Prec@5 84.0400	
Val: [18]	Time 1.304	Data 0.098	Loss 1.892	Prec@1 49.5700	Prec@5 81.0200	
Best Prec@1: [49.740]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 19.175	Data 0.210	Loss 1.646	Prec@1 54.0060	Prec@5 84.0580	
Val: [19]	Time 1.383	Data 0.103	Loss 1.844	Prec@1 51.0400	Prec@5 81.9700	
Best Prec@1: [51.040]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 19.073	Data 0.222	Loss 1.638	Prec@1 54.2860	Prec@5 84.3000	
Val: [20]	Time 1.335	Data 0.116	Loss 1.846	Prec@1 50.1800	Prec@5 81.7800	
Best Prec@1: [51.040]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 19.070	Data 0.222	Loss 1.615	Prec@1 54.5240	Prec@5 84.6020	
Val: [21]	Time 1.474	Data 0.141	Loss 1.932	Prec@1 49.3300	Prec@5 79.5900	
Best Prec@1: [51.040]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 19.119	Data 0.224	Loss 1.609	Prec@1 54.7620	Prec@5 84.7520	
Val: [22]	Time 1.294	Data 0.096	Loss 1.861	Prec@1 50.8900	Prec@5 81.5500	
Best Prec@1: [51.040]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 19.121	Data 0.217	Loss 1.589	Prec@1 55.7180	Prec@5 85.0880	
Val: [23]	Time 1.290	Data 0.105	Loss 2.050	Prec@1 47.7900	Prec@5 79.7000	
Best Prec@1: [51.040]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 19.145	Data 0.213	Loss 1.587	Prec@1 55.4760	Prec@5 85.2120	
Val: [24]	Time 1.485	Data 0.117	Loss 1.967	Prec@1 48.7800	Prec@5 80.9100	
Best Prec@1: [51.040]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 19.027	Data 0.219	Loss 1.579	Prec@1 55.4620	Prec@5 85.3100	
Val: [25]	Time 1.342	Data 0.100	Loss 1.897	Prec@1 50.3500	Prec@5 80.9500	
Best Prec@1: [51.040]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 18.953	Data 0.224	Loss 1.567	Prec@1 56.1720	Prec@5 85.4500	
Val: [26]	Time 1.338	Data 0.102	Loss 1.815	Prec@1 51.6800	Prec@5 81.8500	
Best Prec@1: [51.680]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 19.133	Data 0.226	Loss 1.558	Prec@1 56.1640	Prec@5 85.6700	
Val: [27]	Time 1.314	Data 0.099	Loss 1.873	Prec@1 49.9800	Prec@5 80.7100	
Best Prec@1: [51.680]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 19.168	Data 0.223	Loss 1.543	Prec@1 56.5140	Prec@5 85.7340	
Val: [28]	Time 1.482	Data 0.125	Loss 1.875	Prec@1 50.6300	Prec@5 80.4600	
Best Prec@1: [51.680]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 19.000	Data 0.222	Loss 1.540	Prec@1 56.7040	Prec@5 85.8440	
Val: [29]	Time 1.377	Data 0.103	Loss 1.790	Prec@1 52.1700	Prec@5 82.5400	
Best Prec@1: [52.170]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 19.086	Data 0.213	Loss 1.531	Prec@1 56.6820	Prec@5 86.1720	
Val: [30]	Time 1.424	Data 0.140	Loss 1.991	Prec@1 49.5700	Prec@5 79.8600	
Best Prec@1: [52.170]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 18.877	Data 0.211	Loss 1.525	Prec@1 57.0720	Prec@5 86.1040	
Val: [31]	Time 1.268	Data 0.106	Loss 1.791	Prec@1 52.0100	Prec@5 82.7300	
Best Prec@1: [52.170]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 18.846	Data 0.220	Loss 1.523	Prec@1 57.0260	Prec@5 86.1760	
Val: [32]	Time 1.330	Data 0.109	Loss 1.907	Prec@1 50.1000	Prec@5 81.1700	
Best Prec@1: [52.170]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 18.940	Data 0.219	Loss 1.507	Prec@1 57.5600	Prec@5 86.2620	
Val: [33]	Time 1.337	Data 0.105	Loss 1.998	Prec@1 49.8900	Prec@5 79.6200	
Best Prec@1: [52.170]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 18.984	Data 0.227	Loss 1.507	Prec@1 57.2960	Prec@5 86.2500	
Val: [34]	Time 1.198	Data 0.093	Loss 1.794	Prec@1 52.1100	Prec@5 82.7700	
Best Prec@1: [52.170]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 19.092	Data 0.228	Loss 1.499	Prec@1 57.5160	Prec@5 86.6040	
Val: [35]	Time 1.320	Data 0.091	Loss 1.759	Prec@1 53.1000	Prec@5 83.0900	
Best Prec@1: [53.100]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 19.118	Data 0.222	Loss 1.493	Prec@1 58.0300	Prec@5 86.5460	
Val: [36]	Time 1.382	Data 0.100	Loss 1.825	Prec@1 52.6000	Prec@5 81.7600	
Best Prec@1: [53.100]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 19.134	Data 0.222	Loss 1.484	Prec@1 57.9340	Prec@5 86.7300	
Val: [37]	Time 1.376	Data 0.143	Loss 1.654	Prec@1 54.5100	Prec@5 84.2400	
Best Prec@1: [54.510]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 19.137	Data 0.220	Loss 1.481	Prec@1 58.1320	Prec@5 86.6800	
Val: [38]	Time 1.257	Data 0.105	Loss 1.839	Prec@1 51.5800	Prec@5 82.3000	
Best Prec@1: [54.510]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 19.038	Data 0.223	Loss 1.483	Prec@1 58.2800	Prec@5 86.8080	
Val: [39]	Time 1.323	Data 0.108	Loss 1.742	Prec@1 53.0100	Prec@5 83.3000	
Best Prec@1: [54.510]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 18.845	Data 0.219	Loss 1.474	Prec@1 58.2620	Prec@5 86.8700	
Val: [40]	Time 1.326	Data 0.107	Loss 1.763	Prec@1 53.6300	Prec@5 83.1300	
Best Prec@1: [54.510]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 19.048	Data 0.216	Loss 1.469	Prec@1 58.3720	Prec@5 87.0900	
Val: [41]	Time 1.302	Data 0.106	Loss 1.885	Prec@1 51.2300	Prec@5 81.8100	
Best Prec@1: [54.510]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 19.126	Data 0.225	Loss 1.467	Prec@1 58.6080	Prec@5 87.0300	
Val: [42]	Time 1.242	Data 0.099	Loss 1.905	Prec@1 50.4600	Prec@5 81.1500	
Best Prec@1: [54.510]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 18.756	Data 0.210	Loss 1.465	Prec@1 58.1560	Prec@5 87.0820	
Val: [43]	Time 1.296	Data 0.101	Loss 1.731	Prec@1 53.7500	Prec@5 83.2100	
Best Prec@1: [54.510]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 18.862	Data 0.221	Loss 1.455	Prec@1 58.7620	Prec@5 87.2500	
Val: [44]	Time 1.314	Data 0.091	Loss 1.888	Prec@1 50.7800	Prec@5 80.5800	
Best Prec@1: [54.510]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 19.121	Data 0.213	Loss 1.456	Prec@1 58.5580	Prec@5 87.2000	
Val: [45]	Time 1.379	Data 0.097	Loss 1.781	Prec@1 52.7700	Prec@5 82.5600	
Best Prec@1: [54.510]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 19.162	Data 0.230	Loss 1.449	Prec@1 58.7120	Prec@5 87.3000	
Val: [46]	Time 1.496	Data 0.133	Loss 1.712	Prec@1 54.2900	Prec@5 83.2500	
Best Prec@1: [54.510]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 19.192	Data 0.226	Loss 1.444	Prec@1 58.8920	Prec@5 87.3860	
Val: [47]	Time 1.238	Data 0.090	Loss 2.052	Prec@1 48.6000	Prec@5 79.5500	
Best Prec@1: [54.510]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 18.656	Data 0.220	Loss 1.445	Prec@1 58.7720	Prec@5 87.4800	
Val: [48]	Time 1.273	Data 0.097	Loss 1.799	Prec@1 52.9200	Prec@5 82.6300	
Best Prec@1: [54.510]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 19.153	Data 0.222	Loss 1.435	Prec@1 59.2580	Prec@5 87.4180	
Val: [49]	Time 1.375	Data 0.097	Loss 1.700	Prec@1 53.8000	Prec@5 83.8500	
Best Prec@1: [54.510]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 18.834	Data 0.218	Loss 1.433	Prec@1 59.2040	Prec@5 87.5140	
Val: [50]	Time 1.388	Data 0.124	Loss 1.814	Prec@1 51.9700	Prec@5 82.7000	
Best Prec@1: [54.510]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 19.177	Data 0.223	Loss 1.433	Prec@1 59.3240	Prec@5 87.7540	
Val: [51]	Time 1.432	Data 0.101	Loss 1.750	Prec@1 53.6000	Prec@5 83.2600	
Best Prec@1: [54.510]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 19.183	Data 0.215	Loss 1.435	Prec@1 59.1480	Prec@5 87.4800	
Val: [52]	Time 1.231	Data 0.090	Loss 1.726	Prec@1 54.1400	Prec@5 83.5500	
Best Prec@1: [54.510]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 19.137	Data 0.223	Loss 1.423	Prec@1 59.5080	Prec@5 87.7580	
Val: [53]	Time 1.390	Data 0.118	Loss 1.932	Prec@1 49.8700	Prec@5 80.3600	
Best Prec@1: [54.510]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 19.137	Data 0.227	Loss 1.432	Prec@1 59.1920	Prec@5 87.6520	
Val: [54]	Time 1.199	Data 0.094	Loss 1.730	Prec@1 53.9200	Prec@5 83.8800	
Best Prec@1: [54.510]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 19.106	Data 0.211	Loss 1.425	Prec@1 59.4080	Prec@5 87.6600	
Val: [55]	Time 1.237	Data 0.109	Loss 1.803	Prec@1 53.7200	Prec@5 82.7500	
Best Prec@1: [54.510]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 19.155	Data 0.222	Loss 1.422	Prec@1 59.5340	Prec@5 87.7760	
Val: [56]	Time 1.251	Data 0.095	Loss 1.854	Prec@1 52.5100	Prec@5 81.7100	
Best Prec@1: [54.510]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 19.070	Data 0.226	Loss 1.420	Prec@1 59.5220	Prec@5 87.7860	
Val: [57]	Time 1.219	Data 0.094	Loss 1.677	Prec@1 54.2600	Prec@5 84.0900	
Best Prec@1: [54.510]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 19.174	Data 0.223	Loss 1.409	Prec@1 59.8260	Prec@5 87.9400	
Val: [58]	Time 1.237	Data 0.110	Loss 1.779	Prec@1 53.9300	Prec@5 83.1500	
Best Prec@1: [54.510]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 19.070	Data 0.214	Loss 1.412	Prec@1 59.7700	Prec@5 87.7700	
Val: [59]	Time 1.366	Data 0.101	Loss 2.034	Prec@1 49.7000	Prec@5 80.8200	
Best Prec@1: [54.510]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 19.165	Data 0.225	Loss 1.415	Prec@1 59.9480	Prec@5 87.8320	
Val: [60]	Time 1.300	Data 0.106	Loss 1.737	Prec@1 54.5000	Prec@5 83.5800	
Best Prec@1: [54.510]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 19.084	Data 0.214	Loss 1.407	Prec@1 59.9100	Prec@5 87.9260	
Val: [61]	Time 1.233	Data 0.091	Loss 1.714	Prec@1 54.6200	Prec@5 83.4000	
Best Prec@1: [54.620]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 19.057	Data 0.226	Loss 1.408	Prec@1 59.6560	Prec@5 88.0200	
Val: [62]	Time 1.321	Data 0.113	Loss 1.705	Prec@1 54.3200	Prec@5 84.4500	
Best Prec@1: [54.620]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 18.825	Data 0.221	Loss 1.405	Prec@1 60.0100	Prec@5 88.0200	
Val: [63]	Time 1.253	Data 0.108	Loss 1.663	Prec@1 54.3600	Prec@5 84.1700	
Best Prec@1: [54.620]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 19.179	Data 0.233	Loss 1.404	Prec@1 60.1060	Prec@5 87.9300	
Val: [64]	Time 1.447	Data 0.121	Loss 1.841	Prec@1 52.8000	Prec@5 81.9500	
Best Prec@1: [54.620]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 19.107	Data 0.212	Loss 1.410	Prec@1 59.9900	Prec@5 87.7460	
Val: [65]	Time 1.335	Data 0.102	Loss 1.715	Prec@1 54.6700	Prec@5 84.1600	
Best Prec@1: [54.670]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 19.204	Data 0.217	Loss 1.404	Prec@1 59.9060	Prec@5 87.9220	
Val: [66]	Time 1.275	Data 0.095	Loss 1.627	Prec@1 55.8000	Prec@5 84.7500	
Best Prec@1: [55.800]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 18.652	Data 0.223	Loss 1.402	Prec@1 59.7520	Prec@5 88.0280	
Val: [67]	Time 1.284	Data 0.095	Loss 1.775	Prec@1 53.0200	Prec@5 83.6500	
Best Prec@1: [55.800]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 19.069	Data 0.215	Loss 1.393	Prec@1 60.2940	Prec@5 88.2540	
Val: [68]	Time 1.434	Data 0.114	Loss 1.810	Prec@1 53.4100	Prec@5 82.8900	
Best Prec@1: [55.800]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 19.231	Data 0.228	Loss 1.385	Prec@1 60.5180	Prec@5 88.2100	
Val: [69]	Time 1.299	Data 0.101	Loss 1.834	Prec@1 52.5700	Prec@5 82.4000	
Best Prec@1: [55.800]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 19.145	Data 0.222	Loss 1.393	Prec@1 60.1900	Prec@5 88.3040	
Val: [70]	Time 1.387	Data 0.116	Loss 1.762	Prec@1 52.8000	Prec@5 83.7800	
Best Prec@1: [55.800]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 18.718	Data 0.217	Loss 1.393	Prec@1 60.4160	Prec@5 88.0620	
Val: [71]	Time 1.319	Data 0.120	Loss 1.741	Prec@1 52.7500	Prec@5 82.9900	
Best Prec@1: [55.800]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 19.175	Data 0.222	Loss 1.393	Prec@1 60.4480	Prec@5 88.1980	
Val: [72]	Time 1.383	Data 0.128	Loss 1.748	Prec@1 54.0700	Prec@5 83.1800	
Best Prec@1: [55.800]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 19.134	Data 0.225	Loss 1.393	Prec@1 60.3980	Prec@5 88.1240	
Val: [73]	Time 1.241	Data 0.108	Loss 1.676	Prec@1 55.9800	Prec@5 84.8200	
Best Prec@1: [55.980]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 19.109	Data 0.214	Loss 1.387	Prec@1 60.6240	Prec@5 88.2520	
Val: [74]	Time 1.243	Data 0.095	Loss 1.688	Prec@1 55.8400	Prec@5 84.7100	
Best Prec@1: [55.980]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 19.080	Data 0.211	Loss 1.386	Prec@1 60.4480	Prec@5 88.3160	
Val: [75]	Time 1.276	Data 0.108	Loss 1.757	Prec@1 53.2600	Prec@5 83.9500	
Best Prec@1: [55.980]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 18.852	Data 0.219	Loss 1.384	Prec@1 60.4520	Prec@5 88.2140	
Val: [76]	Time 1.429	Data 0.122	Loss 1.862	Prec@1 51.9000	Prec@5 81.8300	
Best Prec@1: [55.980]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 19.146	Data 0.228	Loss 1.384	Prec@1 60.5480	Prec@5 88.1920	
Val: [77]	Time 1.528	Data 0.155	Loss 1.843	Prec@1 50.7900	Prec@5 82.6300	
Best Prec@1: [55.980]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 19.129	Data 0.222	Loss 1.384	Prec@1 60.5080	Prec@5 88.2860	
Val: [78]	Time 1.236	Data 0.101	Loss 1.613	Prec@1 56.5300	Prec@5 84.2900	
Best Prec@1: [56.530]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 18.586	Data 0.219	Loss 1.376	Prec@1 60.6980	Prec@5 88.2740	
Val: [79]	Time 1.285	Data 0.108	Loss 1.599	Prec@1 55.8000	Prec@5 85.1700	
Best Prec@1: [56.530]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 19.052	Data 0.222	Loss 1.384	Prec@1 60.2600	Prec@5 88.3060	
Val: [80]	Time 1.356	Data 0.110	Loss 1.834	Prec@1 52.0800	Prec@5 82.2100	
Best Prec@1: [56.530]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 18.938	Data 0.217	Loss 1.381	Prec@1 60.5260	Prec@5 88.3880	
Val: [81]	Time 1.293	Data 0.094	Loss 1.857	Prec@1 51.0000	Prec@5 80.7900	
Best Prec@1: [56.530]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 19.080	Data 0.212	Loss 1.373	Prec@1 60.7720	Prec@5 88.3880	
Val: [82]	Time 1.238	Data 0.092	Loss 1.751	Prec@1 52.8700	Prec@5 82.9700	
Best Prec@1: [56.530]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 19.107	Data 0.225	Loss 1.378	Prec@1 60.5840	Prec@5 88.3280	
Val: [83]	Time 1.393	Data 0.112	Loss 1.837	Prec@1 52.4300	Prec@5 82.1000	
Best Prec@1: [56.530]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 19.167	Data 0.217	Loss 1.373	Prec@1 60.4700	Prec@5 88.5080	
Val: [84]	Time 1.276	Data 0.109	Loss 1.752	Prec@1 53.7500	Prec@5 82.9400	
Best Prec@1: [56.530]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 19.080	Data 0.224	Loss 1.369	Prec@1 60.9880	Prec@5 88.4640	
Val: [85]	Time 1.328	Data 0.108	Loss 1.897	Prec@1 51.3300	Prec@5 80.8300	
Best Prec@1: [56.530]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 19.214	Data 0.229	Loss 1.367	Prec@1 61.1540	Prec@5 88.3680	
Val: [86]	Time 1.177	Data 0.089	Loss 1.749	Prec@1 54.3500	Prec@5 83.8800	
Best Prec@1: [56.530]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 19.177	Data 0.223	Loss 1.372	Prec@1 60.9260	Prec@5 88.3880	
Val: [87]	Time 1.396	Data 0.111	Loss 1.791	Prec@1 53.2900	Prec@5 83.0700	
Best Prec@1: [56.530]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 18.969	Data 0.213	Loss 1.366	Prec@1 61.0680	Prec@5 88.4380	
Val: [88]	Time 1.430	Data 0.134	Loss 1.774	Prec@1 53.1700	Prec@5 84.1200	
Best Prec@1: [56.530]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 19.133	Data 0.212	Loss 1.371	Prec@1 60.7240	Prec@5 88.5300	
Val: [89]	Time 1.335	Data 0.126	Loss 1.768	Prec@1 53.7000	Prec@5 82.8800	
Best Prec@1: [56.530]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 19.184	Data 0.227	Loss 1.368	Prec@1 60.8700	Prec@5 88.7140	
Val: [90]	Time 1.187	Data 0.093	Loss 1.776	Prec@1 54.0100	Prec@5 83.7100	
Best Prec@1: [56.530]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 19.171	Data 0.227	Loss 1.368	Prec@1 61.0140	Prec@5 88.6120	
Val: [91]	Time 1.503	Data 0.146	Loss 1.901	Prec@1 52.5700	Prec@5 82.9500	
Best Prec@1: [56.530]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 18.928	Data 0.213	Loss 1.363	Prec@1 60.9580	Prec@5 88.7020	
Val: [92]	Time 1.313	Data 0.116	Loss 1.660	Prec@1 55.5100	Prec@5 83.9500	
Best Prec@1: [56.530]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 19.152	Data 0.210	Loss 1.366	Prec@1 61.0380	Prec@5 88.4920	
Val: [93]	Time 1.479	Data 0.124	Loss 1.672	Prec@1 54.8700	Prec@5 84.2000	
Best Prec@1: [56.530]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 19.128	Data 0.211	Loss 1.363	Prec@1 61.0080	Prec@5 88.7700	
Val: [94]	Time 1.296	Data 0.095	Loss 1.687	Prec@1 54.8800	Prec@5 84.2400	
Best Prec@1: [56.530]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 19.135	Data 0.218	Loss 1.363	Prec@1 61.2560	Prec@5 88.3600	
Val: [95]	Time 1.340	Data 0.113	Loss 1.679	Prec@1 55.6100	Prec@5 84.4500	
Best Prec@1: [56.530]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 19.161	Data 0.226	Loss 1.361	Prec@1 60.9900	Prec@5 88.6480	
Val: [96]	Time 1.304	Data 0.106	Loss 1.801	Prec@1 53.9400	Prec@5 83.5000	
Best Prec@1: [56.530]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 19.180	Data 0.213	Loss 1.365	Prec@1 61.1600	Prec@5 88.5220	
Val: [97]	Time 1.245	Data 0.094	Loss 1.854	Prec@1 52.9900	Prec@5 82.0800	
Best Prec@1: [56.530]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 19.206	Data 0.231	Loss 1.353	Prec@1 61.1240	Prec@5 88.7540	
Val: [98]	Time 1.232	Data 0.099	Loss 1.662	Prec@1 55.6200	Prec@5 83.9800	
Best Prec@1: [56.530]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 19.018	Data 0.213	Loss 1.357	Prec@1 61.2200	Prec@5 88.5660	
Val: [99]	Time 1.345	Data 0.099	Loss 1.592	Prec@1 56.7000	Prec@5 85.2600	
Best Prec@1: [56.700]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 19.168	Data 0.239	Loss 1.358	Prec@1 61.1820	Prec@5 88.6600	
Val: [100]	Time 1.441	Data 0.117	Loss 1.687	Prec@1 54.7400	Prec@5 83.7700	
Best Prec@1: [56.700]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 18.957	Data 0.229	Loss 1.358	Prec@1 61.0420	Prec@5 88.6000	
Val: [101]	Time 1.322	Data 0.102	Loss 1.755	Prec@1 53.3300	Prec@5 83.0800	
Best Prec@1: [56.700]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 19.182	Data 0.213	Loss 1.359	Prec@1 61.0680	Prec@5 88.6440	
Val: [102]	Time 1.400	Data 0.104	Loss 1.704	Prec@1 54.8200	Prec@5 84.0000	
Best Prec@1: [56.700]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 18.961	Data 0.230	Loss 1.351	Prec@1 61.2240	Prec@5 88.6300	
Val: [103]	Time 1.479	Data 0.109	Loss 1.863	Prec@1 51.8600	Prec@5 83.0000	
Best Prec@1: [56.700]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 19.147	Data 0.223	Loss 1.351	Prec@1 61.2880	Prec@5 88.6100	
Val: [104]	Time 1.358	Data 0.112	Loss 1.744	Prec@1 53.9500	Prec@5 83.3300	
Best Prec@1: [56.700]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 18.578	Data 0.220	Loss 1.348	Prec@1 61.3140	Prec@5 88.9060	
Val: [105]	Time 1.248	Data 0.100	Loss 1.706	Prec@1 54.5000	Prec@5 84.1700	
Best Prec@1: [56.700]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 19.087	Data 0.224	Loss 1.353	Prec@1 61.2480	Prec@5 88.6320	
Val: [106]	Time 1.265	Data 0.100	Loss 1.681	Prec@1 55.7300	Prec@5 84.1600	
Best Prec@1: [56.700]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 18.764	Data 0.220	Loss 1.343	Prec@1 61.6320	Prec@5 88.9080	
Val: [107]	Time 1.414	Data 0.117	Loss 1.694	Prec@1 55.1400	Prec@5 84.3400	
Best Prec@1: [56.700]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 19.051	Data 0.213	Loss 1.351	Prec@1 61.5700	Prec@5 88.7100	
Val: [108]	Time 1.233	Data 0.091	Loss 1.711	Prec@1 53.8600	Prec@5 83.5000	
Best Prec@1: [56.700]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 18.435	Data 0.219	Loss 1.349	Prec@1 61.4080	Prec@5 88.7760	
Val: [109]	Time 1.433	Data 0.122	Loss 2.012	Prec@1 49.8500	Prec@5 81.0200	
Best Prec@1: [56.700]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 18.508	Data 0.208	Loss 1.343	Prec@1 61.6420	Prec@5 88.9080	
Val: [110]	Time 1.394	Data 0.100	Loss 1.645	Prec@1 56.1300	Prec@5 84.8000	
Best Prec@1: [56.700]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 19.180	Data 0.225	Loss 1.350	Prec@1 61.3540	Prec@5 88.7320	
Val: [111]	Time 1.330	Data 0.104	Loss 1.624	Prec@1 56.1400	Prec@5 84.9400	
Best Prec@1: [56.700]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 19.103	Data 0.214	Loss 1.348	Prec@1 61.1940	Prec@5 88.7680	
Val: [112]	Time 1.504	Data 0.150	Loss 1.639	Prec@1 55.8300	Prec@5 84.9500	
Best Prec@1: [56.700]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 18.924	Data 0.212	Loss 1.347	Prec@1 61.2060	Prec@5 88.9220	
Val: [113]	Time 1.393	Data 0.125	Loss 1.602	Prec@1 55.8600	Prec@5 85.8600	
Best Prec@1: [56.700]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 19.140	Data 0.221	Loss 1.333	Prec@1 61.9040	Prec@5 89.0420	
Val: [114]	Time 1.237	Data 0.104	Loss 1.652	Prec@1 55.1200	Prec@5 84.8300	
Best Prec@1: [56.700]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 19.095	Data 0.223	Loss 1.335	Prec@1 61.7360	Prec@5 88.8820	
Val: [115]	Time 1.274	Data 0.103	Loss 1.779	Prec@1 53.7300	Prec@5 83.1100	
Best Prec@1: [56.700]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 19.162	Data 0.230	Loss 1.345	Prec@1 61.4400	Prec@5 88.7360	
Val: [116]	Time 1.284	Data 0.099	Loss 1.665	Prec@1 55.4300	Prec@5 84.0600	
Best Prec@1: [56.700]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 19.038	Data 0.225	Loss 1.344	Prec@1 61.6460	Prec@5 88.6900	
Val: [117]	Time 1.268	Data 0.096	Loss 1.671	Prec@1 54.8600	Prec@5 84.3000	
Best Prec@1: [56.700]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 19.119	Data 0.224	Loss 1.337	Prec@1 61.6880	Prec@5 89.0220	
Val: [118]	Time 1.272	Data 0.105	Loss 1.772	Prec@1 53.1000	Prec@5 83.1100	
Best Prec@1: [56.700]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 18.850	Data 0.221	Loss 1.334	Prec@1 61.5580	Prec@5 88.9420	
Val: [119]	Time 1.305	Data 0.129	Loss 1.616	Prec@1 56.7900	Prec@5 84.5900	
Best Prec@1: [56.790]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 18.591	Data 0.222	Loss 1.339	Prec@1 61.6060	Prec@5 89.0220	
Val: [120]	Time 1.492	Data 0.111	Loss 1.757	Prec@1 54.4600	Prec@5 83.3400	
Best Prec@1: [56.790]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 19.067	Data 0.222	Loss 1.340	Prec@1 61.6080	Prec@5 88.9920	
Val: [121]	Time 1.209	Data 0.094	Loss 1.641	Prec@1 56.6500	Prec@5 84.7800	
Best Prec@1: [56.790]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 19.062	Data 0.217	Loss 1.333	Prec@1 62.0600	Prec@5 88.8360	
Val: [122]	Time 1.378	Data 0.109	Loss 1.736	Prec@1 55.3700	Prec@5 83.6900	
Best Prec@1: [56.790]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 19.039	Data 0.221	Loss 1.336	Prec@1 61.4660	Prec@5 88.8720	
Val: [123]	Time 1.305	Data 0.095	Loss 1.673	Prec@1 55.2200	Prec@5 84.6500	
Best Prec@1: [56.790]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 18.866	Data 0.221	Loss 1.334	Prec@1 61.8700	Prec@5 89.0660	
Val: [124]	Time 1.549	Data 0.136	Loss 1.783	Prec@1 52.9100	Prec@5 84.0300	
Best Prec@1: [56.790]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 19.158	Data 0.225	Loss 1.333	Prec@1 61.7040	Prec@5 88.9860	
Val: [125]	Time 1.354	Data 0.100	Loss 1.678	Prec@1 54.7700	Prec@5 84.2900	
Best Prec@1: [56.790]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 19.191	Data 0.220	Loss 1.335	Prec@1 61.8100	Prec@5 88.8580	
Val: [126]	Time 1.266	Data 0.112	Loss 2.139	Prec@1 49.3000	Prec@5 79.2700	
Best Prec@1: [56.790]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 19.185	Data 0.231	Loss 1.333	Prec@1 61.8600	Prec@5 88.8080	
Val: [127]	Time 1.435	Data 0.129	Loss 1.975	Prec@1 49.9800	Prec@5 79.9800	
Best Prec@1: [56.790]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 19.156	Data 0.210	Loss 1.335	Prec@1 61.9120	Prec@5 88.8900	
Val: [128]	Time 1.348	Data 0.098	Loss 1.797	Prec@1 53.2500	Prec@5 82.3900	
Best Prec@1: [56.790]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 19.029	Data 0.221	Loss 1.339	Prec@1 61.9140	Prec@5 88.8020	
Val: [129]	Time 1.231	Data 0.091	Loss 1.672	Prec@1 55.1500	Prec@5 84.1700	
Best Prec@1: [56.790]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 18.866	Data 0.224	Loss 1.333	Prec@1 61.6500	Prec@5 89.0780	
Val: [130]	Time 1.320	Data 0.106	Loss 1.726	Prec@1 54.3400	Prec@5 83.2400	
Best Prec@1: [56.790]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 19.044	Data 0.217	Loss 1.327	Prec@1 61.8300	Prec@5 88.9960	
Val: [131]	Time 1.287	Data 0.095	Loss 1.633	Prec@1 56.5000	Prec@5 84.7300	
Best Prec@1: [56.790]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 19.097	Data 0.222	Loss 1.332	Prec@1 61.7820	Prec@5 89.0120	
Val: [132]	Time 1.294	Data 0.097	Loss 1.870	Prec@1 52.1000	Prec@5 81.1400	
Best Prec@1: [56.790]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 18.995	Data 0.225	Loss 1.340	Prec@1 61.7000	Prec@5 88.7960	
Val: [133]	Time 1.310	Data 0.100	Loss 1.703	Prec@1 54.4400	Prec@5 84.2300	
Best Prec@1: [56.790]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 19.024	Data 0.223	Loss 1.326	Prec@1 62.0340	Prec@5 89.1580	
Val: [134]	Time 1.279	Data 0.109	Loss 1.630	Prec@1 56.6100	Prec@5 85.1300	
Best Prec@1: [56.790]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 18.528	Data 0.221	Loss 1.326	Prec@1 62.0220	Prec@5 88.9760	
Val: [135]	Time 1.467	Data 0.130	Loss 1.842	Prec@1 52.8300	Prec@5 82.3300	
Best Prec@1: [56.790]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 18.743	Data 0.213	Loss 1.330	Prec@1 61.8180	Prec@5 89.0220	
Val: [136]	Time 1.351	Data 0.123	Loss 1.870	Prec@1 52.7100	Prec@5 82.0000	
Best Prec@1: [56.790]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 18.902	Data 0.214	Loss 1.327	Prec@1 61.7560	Prec@5 89.2000	
Val: [137]	Time 1.279	Data 0.121	Loss 1.687	Prec@1 55.8800	Prec@5 84.0100	
Best Prec@1: [56.790]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 19.164	Data 0.214	Loss 1.330	Prec@1 61.7860	Prec@5 89.0120	
Val: [138]	Time 1.307	Data 0.119	Loss 1.648	Prec@1 55.5000	Prec@5 84.8400	
Best Prec@1: [56.790]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 18.982	Data 0.217	Loss 1.332	Prec@1 61.7740	Prec@5 89.0360	
Val: [139]	Time 1.219	Data 0.092	Loss 1.812	Prec@1 53.0700	Prec@5 82.8200	
Best Prec@1: [56.790]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 19.194	Data 0.215	Loss 1.331	Prec@1 61.6660	Prec@5 89.1120	
Val: [140]	Time 1.289	Data 0.097	Loss 1.663	Prec@1 55.1300	Prec@5 84.8500	
Best Prec@1: [56.790]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 19.019	Data 0.216	Loss 1.328	Prec@1 61.7120	Prec@5 89.1680	
Val: [141]	Time 1.230	Data 0.099	Loss 1.637	Prec@1 55.6300	Prec@5 84.8900	
Best Prec@1: [56.790]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 18.773	Data 0.222	Loss 1.324	Prec@1 61.9620	Prec@5 89.2760	
Val: [142]	Time 1.306	Data 0.101	Loss 1.658	Prec@1 55.6500	Prec@5 84.6800	
Best Prec@1: [56.790]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 18.497	Data 0.210	Loss 1.318	Prec@1 62.1540	Prec@5 89.1580	
Val: [143]	Time 1.295	Data 0.094	Loss 1.717	Prec@1 54.5200	Prec@5 83.9700	
Best Prec@1: [56.790]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 18.904	Data 0.224	Loss 1.328	Prec@1 61.8040	Prec@5 88.8920	
Val: [144]	Time 1.272	Data 0.104	Loss 1.628	Prec@1 55.5400	Prec@5 84.9900	
Best Prec@1: [56.790]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 18.854	Data 0.220	Loss 1.323	Prec@1 62.1220	Prec@5 89.2220	
Val: [145]	Time 1.319	Data 0.098	Loss 1.733	Prec@1 53.4000	Prec@5 83.2700	
Best Prec@1: [56.790]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 18.943	Data 0.214	Loss 1.320	Prec@1 61.9920	Prec@5 89.1660	
Val: [146]	Time 1.245	Data 0.099	Loss 1.809	Prec@1 53.4500	Prec@5 83.4300	
Best Prec@1: [56.790]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 19.116	Data 0.223	Loss 1.322	Prec@1 61.9800	Prec@5 89.0960	
Val: [147]	Time 1.317	Data 0.098	Loss 1.808	Prec@1 53.8800	Prec@5 82.3400	
Best Prec@1: [56.790]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 19.043	Data 0.224	Loss 1.325	Prec@1 61.8820	Prec@5 89.1780	
Val: [148]	Time 1.203	Data 0.102	Loss 1.686	Prec@1 54.6600	Prec@5 84.6900	
Best Prec@1: [56.790]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 18.438	Data 0.218	Loss 1.323	Prec@1 62.0620	Prec@5 89.3000	
Val: [149]	Time 1.292	Data 0.104	Loss 1.737	Prec@1 55.2400	Prec@5 84.0200	
Best Prec@1: [56.790]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 19.101	Data 0.222	Loss 1.033	Prec@1 69.9740	Prec@5 92.9140	
Val: [150]	Time 1.364	Data 0.112	Loss 1.216	Prec@1 65.8700	Prec@5 90.1500	
Best Prec@1: [65.870]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 18.666	Data 0.219	Loss 0.960	Prec@1 72.0520	Prec@5 93.7000	
Val: [151]	Time 1.242	Data 0.096	Loss 1.203	Prec@1 66.0400	Prec@5 90.2900	
Best Prec@1: [66.040]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 19.114	Data 0.211	Loss 0.932	Prec@1 72.8780	Prec@5 93.9280	
Val: [152]	Time 1.527	Data 0.131	Loss 1.198	Prec@1 66.4700	Prec@5 90.4700	
Best Prec@1: [66.470]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 19.202	Data 0.214	Loss 0.913	Prec@1 73.2460	Prec@5 94.1620	
Val: [153]	Time 1.356	Data 0.107	Loss 1.200	Prec@1 66.3000	Prec@5 90.6300	
Best Prec@1: [66.470]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 19.058	Data 0.214	Loss 0.901	Prec@1 73.4720	Prec@5 94.1400	
Val: [154]	Time 1.475	Data 0.128	Loss 1.205	Prec@1 66.5200	Prec@5 90.6400	
Best Prec@1: [66.520]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 19.108	Data 0.222	Loss 0.891	Prec@1 73.9200	Prec@5 94.3600	
Val: [155]	Time 1.438	Data 0.100	Loss 1.205	Prec@1 66.6700	Prec@5 90.6800	
Best Prec@1: [66.670]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 19.119	Data 0.225	Loss 0.877	Prec@1 74.1400	Prec@5 94.5100	
Val: [156]	Time 1.503	Data 0.128	Loss 1.222	Prec@1 65.9200	Prec@5 90.5300	
Best Prec@1: [66.670]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 19.167	Data 0.213	Loss 0.877	Prec@1 74.0840	Prec@5 94.5360	
Val: [157]	Time 1.239	Data 0.092	Loss 1.209	Prec@1 66.3800	Prec@5 90.5000	
Best Prec@1: [66.670]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 18.576	Data 0.215	Loss 0.870	Prec@1 74.1780	Prec@5 94.5720	
Val: [158]	Time 1.307	Data 0.106	Loss 1.211	Prec@1 66.6800	Prec@5 90.4700	
Best Prec@1: [66.680]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 19.018	Data 0.223	Loss 0.865	Prec@1 74.5180	Prec@5 94.6680	
Val: [159]	Time 1.416	Data 0.114	Loss 1.226	Prec@1 66.1200	Prec@5 90.2600	
Best Prec@1: [66.680]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 19.098	Data 0.224	Loss 0.855	Prec@1 74.6620	Prec@5 94.8820	
Val: [160]	Time 1.230	Data 0.098	Loss 1.216	Prec@1 66.1500	Prec@5 90.5400	
Best Prec@1: [66.680]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 18.926	Data 0.223	Loss 0.851	Prec@1 74.6680	Prec@5 94.8660	
Val: [161]	Time 1.272	Data 0.107	Loss 1.228	Prec@1 66.3000	Prec@5 90.3600	
Best Prec@1: [66.680]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 19.185	Data 0.211	Loss 0.849	Prec@1 74.7420	Prec@5 94.8740	
Val: [162]	Time 1.356	Data 0.108	Loss 1.229	Prec@1 65.9600	Prec@5 90.6300	
Best Prec@1: [66.680]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 19.180	Data 0.226	Loss 0.844	Prec@1 74.8900	Prec@5 94.9260	
Val: [163]	Time 1.243	Data 0.108	Loss 1.220	Prec@1 66.4000	Prec@5 90.6800	
Best Prec@1: [66.680]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 19.058	Data 0.212	Loss 0.840	Prec@1 74.8900	Prec@5 94.9700	
Val: [164]	Time 1.300	Data 0.103	Loss 1.214	Prec@1 66.4100	Prec@5 90.5600	
Best Prec@1: [66.680]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 19.119	Data 0.221	Loss 0.833	Prec@1 75.2160	Prec@5 95.0660	
Val: [165]	Time 1.250	Data 0.098	Loss 1.250	Prec@1 65.8600	Prec@5 90.7500	
Best Prec@1: [66.680]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 19.173	Data 0.219	Loss 0.833	Prec@1 75.0820	Prec@5 94.9620	
Val: [166]	Time 1.252	Data 0.090	Loss 1.244	Prec@1 66.2900	Prec@5 90.6000	
Best Prec@1: [66.680]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 18.682	Data 0.219	Loss 0.831	Prec@1 75.1500	Prec@5 95.0960	
Val: [167]	Time 1.405	Data 0.118	Loss 1.238	Prec@1 66.3000	Prec@5 90.4100	
Best Prec@1: [66.680]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 19.029	Data 0.224	Loss 0.827	Prec@1 75.3660	Prec@5 95.1200	
Val: [168]	Time 1.266	Data 0.103	Loss 1.238	Prec@1 66.1300	Prec@5 90.5100	
Best Prec@1: [66.680]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 19.110	Data 0.225	Loss 0.826	Prec@1 75.1900	Prec@5 95.0720	
Val: [169]	Time 1.346	Data 0.111	Loss 1.252	Prec@1 66.0700	Prec@5 90.4500	
Best Prec@1: [66.680]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 19.182	Data 0.217	Loss 0.819	Prec@1 75.3860	Prec@5 95.2460	
Val: [170]	Time 1.342	Data 0.095	Loss 1.275	Prec@1 66.0400	Prec@5 89.8300	
Best Prec@1: [66.680]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 18.765	Data 0.221	Loss 0.822	Prec@1 75.2440	Prec@5 95.2300	
Val: [171]	Time 1.384	Data 0.113	Loss 1.247	Prec@1 65.9700	Prec@5 90.2800	
Best Prec@1: [66.680]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 19.121	Data 0.224	Loss 0.818	Prec@1 75.4320	Prec@5 95.3440	
Val: [172]	Time 1.233	Data 0.102	Loss 1.252	Prec@1 66.2200	Prec@5 90.0800	
Best Prec@1: [66.680]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 18.805	Data 0.211	Loss 0.818	Prec@1 75.3480	Prec@5 95.2940	
Val: [173]	Time 1.296	Data 0.096	Loss 1.288	Prec@1 65.3900	Prec@5 89.9700	
Best Prec@1: [66.680]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 19.072	Data 0.215	Loss 0.820	Prec@1 75.3200	Prec@5 95.2820	
Val: [174]	Time 1.333	Data 0.116	Loss 1.277	Prec@1 65.5400	Prec@5 89.7700	
Best Prec@1: [66.680]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 19.081	Data 0.215	Loss 0.814	Prec@1 75.5000	Prec@5 95.3380	
Val: [175]	Time 1.305	Data 0.102	Loss 1.262	Prec@1 65.6400	Prec@5 90.2800	
Best Prec@1: [66.680]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 19.197	Data 0.225	Loss 0.815	Prec@1 75.7500	Prec@5 95.2820	
Val: [176]	Time 1.586	Data 0.182	Loss 1.287	Prec@1 65.7000	Prec@5 90.2400	
Best Prec@1: [66.680]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 19.100	Data 0.224	Loss 0.808	Prec@1 75.6320	Prec@5 95.4020	
Val: [177]	Time 1.377	Data 0.111	Loss 1.261	Prec@1 65.9300	Prec@5 90.2900	
Best Prec@1: [66.680]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 19.090	Data 0.227	Loss 0.812	Prec@1 75.8140	Prec@5 95.3180	
Val: [178]	Time 1.247	Data 0.100	Loss 1.268	Prec@1 66.0400	Prec@5 90.4600	
Best Prec@1: [66.680]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 19.166	Data 0.223	Loss 0.809	Prec@1 75.7900	Prec@5 95.3760	
Val: [179]	Time 1.252	Data 0.096	Loss 1.260	Prec@1 66.1800	Prec@5 90.3900	
Best Prec@1: [66.680]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 19.130	Data 0.214	Loss 0.814	Prec@1 75.3660	Prec@5 95.2980	
Val: [180]	Time 1.481	Data 0.152	Loss 1.265	Prec@1 66.2100	Prec@5 90.2600	
Best Prec@1: [66.680]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 19.113	Data 0.211	Loss 0.809	Prec@1 75.6820	Prec@5 95.3480	
Val: [181]	Time 1.244	Data 0.097	Loss 1.282	Prec@1 65.3000	Prec@5 90.0100	
Best Prec@1: [66.680]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 19.077	Data 0.224	Loss 0.801	Prec@1 75.7540	Prec@5 95.4560	
Val: [182]	Time 1.380	Data 0.106	Loss 1.270	Prec@1 65.6200	Prec@5 90.0900	
Best Prec@1: [66.680]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 19.141	Data 0.224	Loss 0.808	Prec@1 75.5100	Prec@5 95.3780	
Val: [183]	Time 1.299	Data 0.094	Loss 1.271	Prec@1 66.0500	Prec@5 90.3200	
Best Prec@1: [66.680]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 19.113	Data 0.215	Loss 0.814	Prec@1 75.3960	Prec@5 95.2960	
Val: [184]	Time 1.271	Data 0.095	Loss 1.278	Prec@1 65.6600	Prec@5 90.3600	
Best Prec@1: [66.680]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 19.127	Data 0.222	Loss 0.809	Prec@1 75.6240	Prec@5 95.3400	
Val: [185]	Time 1.387	Data 0.095	Loss 1.297	Prec@1 65.3600	Prec@5 89.7900	
Best Prec@1: [66.680]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 19.197	Data 0.226	Loss 0.804	Prec@1 75.9420	Prec@5 95.3240	
Val: [186]	Time 1.356	Data 0.116	Loss 1.273	Prec@1 66.0100	Prec@5 90.0800	
Best Prec@1: [66.680]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 19.180	Data 0.216	Loss 0.807	Prec@1 75.6660	Prec@5 95.5500	
Val: [187]	Time 1.413	Data 0.105	Loss 1.297	Prec@1 65.1900	Prec@5 90.0200	
Best Prec@1: [66.680]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 19.124	Data 0.228	Loss 0.805	Prec@1 75.7200	Prec@5 95.4160	
Val: [188]	Time 1.390	Data 0.129	Loss 1.284	Prec@1 65.4900	Prec@5 89.8700	
Best Prec@1: [66.680]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 19.196	Data 0.226	Loss 0.807	Prec@1 75.6280	Prec@5 95.3700	
Val: [189]	Time 1.170	Data 0.087	Loss 1.311	Prec@1 65.6300	Prec@5 89.5100	
Best Prec@1: [66.680]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 19.137	Data 0.227	Loss 0.811	Prec@1 75.5480	Prec@5 95.4360	
Val: [190]	Time 1.275	Data 0.105	Loss 1.303	Prec@1 65.7300	Prec@5 89.8400	
Best Prec@1: [66.680]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 18.943	Data 0.223	Loss 0.810	Prec@1 75.6060	Prec@5 95.4020	
Val: [191]	Time 1.306	Data 0.098	Loss 1.314	Prec@1 65.1000	Prec@5 89.7600	
Best Prec@1: [66.680]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 18.928	Data 0.222	Loss 0.806	Prec@1 75.8320	Prec@5 95.4540	
Val: [192]	Time 1.393	Data 0.112	Loss 1.311	Prec@1 65.1600	Prec@5 89.9500	
Best Prec@1: [66.680]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 19.101	Data 0.215	Loss 0.802	Prec@1 75.7820	Prec@5 95.4600	
Val: [193]	Time 1.186	Data 0.093	Loss 1.271	Prec@1 65.6700	Prec@5 90.6100	
Best Prec@1: [66.680]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 18.877	Data 0.212	Loss 0.802	Prec@1 75.9200	Prec@5 95.4620	
Val: [194]	Time 1.921	Data 0.453	Loss 1.313	Prec@1 65.4700	Prec@5 90.0300	
Best Prec@1: [66.680]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 18.887	Data 0.225	Loss 0.801	Prec@1 75.5800	Prec@5 95.5640	
Val: [195]	Time 1.234	Data 0.099	Loss 1.282	Prec@1 65.1700	Prec@5 90.3500	
Best Prec@1: [66.680]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 19.156	Data 0.222	Loss 0.806	Prec@1 75.7860	Prec@5 95.4580	
Val: [196]	Time 1.218	Data 0.091	Loss 1.319	Prec@1 65.2300	Prec@5 89.6200	
Best Prec@1: [66.680]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 19.149	Data 0.215	Loss 0.808	Prec@1 75.5880	Prec@5 95.4760	
Val: [197]	Time 1.323	Data 0.098	Loss 1.308	Prec@1 65.5000	Prec@5 90.0500	
Best Prec@1: [66.680]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 19.173	Data 0.221	Loss 0.806	Prec@1 75.7140	Prec@5 95.5120	
Val: [198]	Time 1.247	Data 0.096	Loss 1.285	Prec@1 65.5100	Prec@5 90.1000	
Best Prec@1: [66.680]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 19.159	Data 0.223	Loss 0.803	Prec@1 75.9080	Prec@5 95.4780	
Val: [199]	Time 1.267	Data 0.093	Loss 1.287	Prec@1 65.5300	Prec@5 90.2600	
Best Prec@1: [66.680]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 18.927	Data 0.222	Loss 0.806	Prec@1 75.6700	Prec@5 95.5480	
Val: [200]	Time 1.253	Data 0.092	Loss 1.339	Prec@1 64.7000	Prec@5 89.8000	
Best Prec@1: [66.680]	
Starting epoch number: 201 Learning rate: 0.010000000000000002
Train: [201]	Time 18.561	Data 0.220	Loss 0.806	Prec@1 75.4200	Prec@5 95.5020	
Val: [201]	Time 1.334	Data 0.117	Loss 1.355	Prec@1 64.2300	Prec@5 89.6400	
Best Prec@1: [66.680]	
Starting epoch number: 202 Learning rate: 0.010000000000000002
Train: [202]	Time 19.073	Data 0.218	Loss 0.802	Prec@1 75.5800	Prec@5 95.3880	
Val: [202]	Time 1.397	Data 0.098	Loss 1.324	Prec@1 65.5400	Prec@5 89.6900	
Best Prec@1: [66.680]	
Starting epoch number: 203 Learning rate: 0.010000000000000002
Train: [203]	Time 19.136	Data 0.224	Loss 0.800	Prec@1 75.6380	Prec@5 95.5080	
Val: [203]	Time 1.224	Data 0.107	Loss 1.309	Prec@1 65.4500	Prec@5 89.6400	
Best Prec@1: [66.680]	
Starting epoch number: 204 Learning rate: 0.010000000000000002
Train: [204]	Time 19.146	Data 0.209	Loss 0.805	Prec@1 75.5360	Prec@5 95.6020	
Val: [204]	Time 1.260	Data 0.115	Loss 1.318	Prec@1 64.6400	Prec@5 89.6600	
Best Prec@1: [66.680]	
Starting epoch number: 205 Learning rate: 0.010000000000000002
Train: [205]	Time 18.912	Data 0.214	Loss 0.802	Prec@1 75.7700	Prec@5 95.5700	
Val: [205]	Time 1.198	Data 0.092	Loss 1.327	Prec@1 65.1200	Prec@5 89.8700	
Best Prec@1: [66.680]	
Starting epoch number: 206 Learning rate: 0.010000000000000002
Train: [206]	Time 18.955	Data 0.222	Loss 0.803	Prec@1 75.6340	Prec@5 95.4960	
Val: [206]	Time 1.259	Data 0.109	Loss 1.304	Prec@1 65.2900	Prec@5 89.9200	
Best Prec@1: [66.680]	
Starting epoch number: 207 Learning rate: 0.010000000000000002
Train: [207]	Time 18.996	Data 0.222	Loss 0.806	Prec@1 75.6000	Prec@5 95.5860	
Val: [207]	Time 1.284	Data 0.106	Loss 1.336	Prec@1 64.8800	Prec@5 89.5700	
Best Prec@1: [66.680]	
Starting epoch number: 208 Learning rate: 0.010000000000000002
Train: [208]	Time 18.936	Data 0.212	Loss 0.809	Prec@1 75.5900	Prec@5 95.4180	
Val: [208]	Time 1.245	Data 0.089	Loss 1.331	Prec@1 65.4300	Prec@5 89.8300	
Best Prec@1: [66.680]	
Starting epoch number: 209 Learning rate: 0.010000000000000002
Train: [209]	Time 19.167	Data 0.219	Loss 0.800	Prec@1 75.6200	Prec@5 95.5060	
Val: [209]	Time 1.293	Data 0.105	Loss 1.337	Prec@1 64.7500	Prec@5 89.5100	
Best Prec@1: [66.680]	
Starting epoch number: 210 Learning rate: 0.010000000000000002
Train: [210]	Time 19.171	Data 0.210	Loss 0.807	Prec@1 75.4300	Prec@5 95.4520	
Val: [210]	Time 1.238	Data 0.096	Loss 1.303	Prec@1 65.6800	Prec@5 89.5600	
Best Prec@1: [66.680]	
Starting epoch number: 211 Learning rate: 0.010000000000000002
Train: [211]	Time 18.650	Data 0.221	Loss 0.803	Prec@1 75.6240	Prec@5 95.4080	
Val: [211]	Time 1.242	Data 0.096	Loss 1.334	Prec@1 64.8000	Prec@5 89.3800	
Best Prec@1: [66.680]	
Starting epoch number: 212 Learning rate: 0.010000000000000002
Train: [212]	Time 19.104	Data 0.218	Loss 0.806	Prec@1 75.4140	Prec@5 95.5540	
Val: [212]	Time 1.313	Data 0.103	Loss 1.304	Prec@1 65.3900	Prec@5 90.0000	
Best Prec@1: [66.680]	
Starting epoch number: 213 Learning rate: 0.010000000000000002
Train: [213]	Time 19.179	Data 0.222	Loss 0.803	Prec@1 75.5140	Prec@5 95.6480	
Val: [213]	Time 1.487	Data 0.125	Loss 1.356	Prec@1 64.7400	Prec@5 89.5000	
Best Prec@1: [66.680]	
Starting epoch number: 214 Learning rate: 0.010000000000000002
Train: [214]	Time 19.075	Data 0.212	Loss 0.801	Prec@1 75.7820	Prec@5 95.4280	
Val: [214]	Time 1.279	Data 0.089	Loss 1.316	Prec@1 65.1900	Prec@5 90.0100	
Best Prec@1: [66.680]	
Starting epoch number: 215 Learning rate: 0.010000000000000002
Train: [215]	Time 19.211	Data 0.226	Loss 0.801	Prec@1 75.7960	Prec@5 95.5040	
Val: [215]	Time 1.298	Data 0.099	Loss 1.330	Prec@1 64.5100	Prec@5 89.9500	
Best Prec@1: [66.680]	
Starting epoch number: 216 Learning rate: 0.010000000000000002
Train: [216]	Time 19.092	Data 0.218	Loss 0.805	Prec@1 75.5760	Prec@5 95.5600	
Val: [216]	Time 1.380	Data 0.105	Loss 1.323	Prec@1 65.2100	Prec@5 89.5900	
Best Prec@1: [66.680]	
Starting epoch number: 217 Learning rate: 0.010000000000000002
Train: [217]	Time 19.176	Data 0.214	Loss 0.805	Prec@1 75.6900	Prec@5 95.4900	
Val: [217]	Time 1.295	Data 0.096	Loss 1.316	Prec@1 64.5800	Prec@5 89.6800	
Best Prec@1: [66.680]	
Starting epoch number: 218 Learning rate: 0.010000000000000002
Train: [218]	Time 18.829	Data 0.210	Loss 0.796	Prec@1 75.8820	Prec@5 95.6600	
Val: [218]	Time 1.274	Data 0.113	Loss 1.352	Prec@1 64.6200	Prec@5 89.4900	
Best Prec@1: [66.680]	
Starting epoch number: 219 Learning rate: 0.010000000000000002
Train: [219]	Time 19.062	Data 0.223	Loss 0.803	Prec@1 75.6500	Prec@5 95.4800	
Val: [219]	Time 1.332	Data 0.104	Loss 1.297	Prec@1 65.5700	Prec@5 90.0900	
Best Prec@1: [66.680]	
Starting epoch number: 220 Learning rate: 0.010000000000000002
Train: [220]	Time 19.179	Data 0.216	Loss 0.803	Prec@1 75.7940	Prec@5 95.4800	
Val: [220]	Time 1.216	Data 0.099	Loss 1.328	Prec@1 64.7800	Prec@5 89.4700	
Best Prec@1: [66.680]	
Starting epoch number: 221 Learning rate: 0.010000000000000002
Train: [221]	Time 19.092	Data 0.214	Loss 0.804	Prec@1 75.5140	Prec@5 95.4940	
Val: [221]	Time 1.198	Data 0.094	Loss 1.339	Prec@1 64.6200	Prec@5 89.7200	
Best Prec@1: [66.680]	
Starting epoch number: 222 Learning rate: 0.010000000000000002
Train: [222]	Time 19.141	Data 0.216	Loss 0.797	Prec@1 75.7840	Prec@5 95.6560	
Val: [222]	Time 1.346	Data 0.108	Loss 1.346	Prec@1 64.6300	Prec@5 89.5300	
Best Prec@1: [66.680]	
Starting epoch number: 223 Learning rate: 0.010000000000000002
Train: [223]	Time 19.056	Data 0.223	Loss 0.811	Prec@1 75.2620	Prec@5 95.4280	
Val: [223]	Time 1.201	Data 0.102	Loss 1.316	Prec@1 65.6300	Prec@5 89.7400	
Best Prec@1: [66.680]	
Starting epoch number: 224 Learning rate: 0.010000000000000002
Train: [224]	Time 19.174	Data 0.225	Loss 0.805	Prec@1 75.4740	Prec@5 95.5100	
Val: [224]	Time 1.411	Data 0.117	Loss 1.375	Prec@1 63.6900	Prec@5 89.4200	
Best Prec@1: [66.680]	
Starting epoch number: 225 Learning rate: 0.0010000000000000002
Train: [225]	Time 19.129	Data 0.214	Loss 0.697	Prec@1 79.0500	Prec@5 96.5000	
Val: [225]	Time 1.336	Data 0.111	Loss 1.219	Prec@1 67.7300	Prec@5 90.7800	
Best Prec@1: [67.730]	
Starting epoch number: 226 Learning rate: 0.0010000000000000002
Train: [226]	Time 18.909	Data 0.213	Loss 0.669	Prec@1 79.9860	Prec@5 96.7200	
Val: [226]	Time 1.335	Data 0.104	Loss 1.216	Prec@1 67.9300	Prec@5 90.8500	
Best Prec@1: [67.930]	
Starting epoch number: 227 Learning rate: 0.0010000000000000002
Train: [227]	Time 19.110	Data 0.216	Loss 0.656	Prec@1 80.5500	Prec@5 96.8540	
Val: [227]	Time 1.322	Data 0.117	Loss 1.217	Prec@1 67.6400	Prec@5 90.7200	
Best Prec@1: [67.930]	
Starting epoch number: 228 Learning rate: 0.0010000000000000002
Train: [228]	Time 19.140	Data 0.225	Loss 0.655	Prec@1 80.3040	Prec@5 96.9060	
Val: [228]	Time 1.345	Data 0.110	Loss 1.207	Prec@1 67.8700	Prec@5 90.9100	
Best Prec@1: [67.930]	
Starting epoch number: 229 Learning rate: 0.0010000000000000002
Train: [229]	Time 19.080	Data 0.230	Loss 0.649	Prec@1 80.5460	Prec@5 96.8640	
Val: [229]	Time 1.234	Data 0.096	Loss 1.209	Prec@1 67.7900	Prec@5 91.1000	
Best Prec@1: [67.930]	
Starting epoch number: 230 Learning rate: 0.0010000000000000002
Train: [230]	Time 19.212	Data 0.229	Loss 0.646	Prec@1 80.5060	Prec@5 96.9780	
Val: [230]	Time 1.459	Data 0.121	Loss 1.225	Prec@1 67.9300	Prec@5 90.8300	
Best Prec@1: [67.930]	
Starting epoch number: 231 Learning rate: 0.0010000000000000002
Train: [231]	Time 18.585	Data 0.230	Loss 0.644	Prec@1 80.6920	Prec@5 97.0480	
Val: [231]	Time 1.349	Data 0.128	Loss 1.224	Prec@1 67.6100	Prec@5 90.9100	
Best Prec@1: [67.930]	
Starting epoch number: 232 Learning rate: 0.0010000000000000002
Train: [232]	Time 19.188	Data 0.229	Loss 0.640	Prec@1 80.6280	Prec@5 97.0120	
Val: [232]	Time 1.233	Data 0.093	Loss 1.223	Prec@1 67.7400	Prec@5 90.7500	
Best Prec@1: [67.930]	
Starting epoch number: 233 Learning rate: 0.0010000000000000002
Train: [233]	Time 19.058	Data 0.224	Loss 0.637	Prec@1 80.8200	Prec@5 97.0100	
Val: [233]	Time 1.486	Data 0.133	Loss 1.227	Prec@1 67.6900	Prec@5 90.9700	
Best Prec@1: [67.930]	
Starting epoch number: 234 Learning rate: 0.0010000000000000002
Train: [234]	Time 18.635	Data 0.219	Loss 0.634	Prec@1 81.1620	Prec@5 97.0840	
Val: [234]	Time 1.223	Data 0.093	Loss 1.227	Prec@1 67.7400	Prec@5 90.8100	
Best Prec@1: [67.930]	
Starting epoch number: 235 Learning rate: 0.0010000000000000002
Train: [235]	Time 18.989	Data 0.225	Loss 0.637	Prec@1 80.8840	Prec@5 97.0160	
Val: [235]	Time 1.461	Data 0.135	Loss 1.238	Prec@1 67.4500	Prec@5 90.7400	
Best Prec@1: [67.930]	
Starting epoch number: 236 Learning rate: 0.0010000000000000002
Train: [236]	Time 19.116	Data 0.223	Loss 0.632	Prec@1 81.0820	Prec@5 97.1500	
Val: [236]	Time 1.448	Data 0.102	Loss 1.233	Prec@1 67.6200	Prec@5 90.9200	
Best Prec@1: [67.930]	
Starting epoch number: 237 Learning rate: 0.0010000000000000002
Train: [237]	Time 19.208	Data 0.225	Loss 0.628	Prec@1 81.1360	Prec@5 97.0880	
Val: [237]	Time 1.307	Data 0.113	Loss 1.234	Prec@1 67.7000	Prec@5 90.9000	
Best Prec@1: [67.930]	
Starting epoch number: 238 Learning rate: 0.0010000000000000002
Train: [238]	Time 19.140	Data 0.226	Loss 0.631	Prec@1 81.0660	Prec@5 97.1780	
Val: [238]	Time 1.255	Data 0.106	Loss 1.236	Prec@1 67.4600	Prec@5 90.7700	
Best Prec@1: [67.930]	
Starting epoch number: 239 Learning rate: 0.0010000000000000002
Train: [239]	Time 19.171	Data 0.223	Loss 0.630	Prec@1 80.9260	Prec@5 97.1040	
Val: [239]	Time 1.308	Data 0.103	Loss 1.229	Prec@1 67.5200	Prec@5 90.8500	
Best Prec@1: [67.930]	
Starting epoch number: 240 Learning rate: 0.0010000000000000002
Train: [240]	Time 18.561	Data 0.220	Loss 0.630	Prec@1 80.9540	Prec@5 97.1220	
Val: [240]	Time 1.379	Data 0.102	Loss 1.239	Prec@1 67.7000	Prec@5 90.9300	
Best Prec@1: [67.930]	
Starting epoch number: 241 Learning rate: 0.0010000000000000002
Train: [241]	Time 19.164	Data 0.224	Loss 0.626	Prec@1 80.9980	Prec@5 97.2420	
Val: [241]	Time 1.345	Data 0.127	Loss 1.229	Prec@1 67.7500	Prec@5 90.8000	
Best Prec@1: [67.930]	
Starting epoch number: 242 Learning rate: 0.0010000000000000002
Train: [242]	Time 19.188	Data 0.224	Loss 0.627	Prec@1 81.0940	Prec@5 97.1120	
Val: [242]	Time 1.272	Data 0.101	Loss 1.245	Prec@1 67.4600	Prec@5 90.7300	
Best Prec@1: [67.930]	
Starting epoch number: 243 Learning rate: 0.0010000000000000002
Train: [243]	Time 18.862	Data 0.221	Loss 0.623	Prec@1 81.1020	Prec@5 97.2020	
Val: [243]	Time 1.369	Data 0.119	Loss 1.232	Prec@1 67.1900	Prec@5 90.8500	
Best Prec@1: [67.930]	
Starting epoch number: 244 Learning rate: 0.0010000000000000002
Train: [244]	Time 19.010	Data 0.223	Loss 0.623	Prec@1 81.4060	Prec@5 97.2080	
Val: [244]	Time 1.273	Data 0.104	Loss 1.237	Prec@1 67.6300	Prec@5 90.8500	
Best Prec@1: [67.930]	
Starting epoch number: 245 Learning rate: 0.0010000000000000002
Train: [245]	Time 19.146	Data 0.214	Loss 0.621	Prec@1 81.1140	Prec@5 97.1300	
Val: [245]	Time 1.263	Data 0.093	Loss 1.244	Prec@1 67.5300	Prec@5 90.7800	
Best Prec@1: [67.930]	
Starting epoch number: 246 Learning rate: 0.0010000000000000002
Train: [246]	Time 18.994	Data 0.222	Loss 0.624	Prec@1 81.2820	Prec@5 97.1200	
Val: [246]	Time 1.378	Data 0.101	Loss 1.260	Prec@1 67.2400	Prec@5 90.7400	
Best Prec@1: [67.930]	
Starting epoch number: 247 Learning rate: 0.0010000000000000002
Train: [247]	Time 18.712	Data 0.220	Loss 0.618	Prec@1 81.3700	Prec@5 97.2740	
Val: [247]	Time 1.310	Data 0.105	Loss 1.265	Prec@1 67.1100	Prec@5 90.5300	
Best Prec@1: [67.930]	
Starting epoch number: 248 Learning rate: 0.0010000000000000002
Train: [248]	Time 18.684	Data 0.229	Loss 0.624	Prec@1 81.0100	Prec@5 97.1240	
Val: [248]	Time 1.436	Data 0.121	Loss 1.255	Prec@1 67.3100	Prec@5 90.6500	
Best Prec@1: [67.930]	
Starting epoch number: 249 Learning rate: 0.0010000000000000002
Train: [249]	Time 19.197	Data 0.226	Loss 0.621	Prec@1 81.2380	Prec@5 97.1840	
Val: [249]	Time 1.296	Data 0.094	Loss 1.257	Prec@1 67.1900	Prec@5 90.7200	
Best Prec@1: [67.930]	
Starting epoch number: 250 Learning rate: 0.0010000000000000002
Train: [250]	Time 19.117	Data 0.213	Loss 0.619	Prec@1 81.1740	Prec@5 97.2760	
Val: [250]	Time 1.369	Data 0.107	Loss 1.254	Prec@1 67.3200	Prec@5 90.8100	
Best Prec@1: [67.930]	
Starting epoch number: 251 Learning rate: 0.0010000000000000002
Train: [251]	Time 18.636	Data 0.221	Loss 0.620	Prec@1 81.1700	Prec@5 97.2540	
Val: [251]	Time 1.301	Data 0.106	Loss 1.251	Prec@1 67.3400	Prec@5 90.6600	
Best Prec@1: [67.930]	
Starting epoch number: 252 Learning rate: 0.0010000000000000002
Train: [252]	Time 19.191	Data 0.224	Loss 0.616	Prec@1 81.1420	Prec@5 97.3300	
Val: [252]	Time 1.234	Data 0.099	Loss 1.261	Prec@1 67.4000	Prec@5 90.4500	
Best Prec@1: [67.930]	
Starting epoch number: 253 Learning rate: 0.0010000000000000002
Train: [253]	Time 19.184	Data 0.223	Loss 0.617	Prec@1 81.2040	Prec@5 97.2340	
Val: [253]	Time 1.492	Data 0.116	Loss 1.258	Prec@1 67.1400	Prec@5 90.6100	
Best Prec@1: [67.930]	
Starting epoch number: 254 Learning rate: 0.0010000000000000002
Train: [254]	Time 19.190	Data 0.216	Loss 0.615	Prec@1 81.4660	Prec@5 97.2660	
Val: [254]	Time 1.428	Data 0.125	Loss 1.265	Prec@1 67.3100	Prec@5 90.4500	
Best Prec@1: [67.930]	
Starting epoch number: 255 Learning rate: 0.0010000000000000002
Train: [255]	Time 19.133	Data 0.211	Loss 0.614	Prec@1 81.3020	Prec@5 97.3120	
Val: [255]	Time 1.334	Data 0.109	Loss 1.250	Prec@1 67.5000	Prec@5 90.7200	
Best Prec@1: [67.930]	
Starting epoch number: 256 Learning rate: 0.0010000000000000002
Train: [256]	Time 19.135	Data 0.217	Loss 0.614	Prec@1 81.4080	Prec@5 97.2820	
Val: [256]	Time 1.294	Data 0.099	Loss 1.249	Prec@1 67.5900	Prec@5 90.9300	
Best Prec@1: [67.930]	
Starting epoch number: 257 Learning rate: 0.0010000000000000002
Train: [257]	Time 18.699	Data 0.215	Loss 0.608	Prec@1 81.6920	Prec@5 97.4600	
Val: [257]	Time 1.236	Data 0.087	Loss 1.246	Prec@1 67.4500	Prec@5 90.7900	
Best Prec@1: [67.930]	
Starting epoch number: 258 Learning rate: 0.0010000000000000002
Train: [258]	Time 19.176	Data 0.222	Loss 0.613	Prec@1 81.3480	Prec@5 97.3700	
Val: [258]	Time 1.287	Data 0.100	Loss 1.248	Prec@1 67.6600	Prec@5 90.7300	
Best Prec@1: [67.930]	
Starting epoch number: 259 Learning rate: 0.0010000000000000002
Train: [259]	Time 18.640	Data 0.211	Loss 0.612	Prec@1 81.6380	Prec@5 97.2680	
Val: [259]	Time 1.267	Data 0.104	Loss 1.252	Prec@1 67.6900	Prec@5 90.5300	
Best Prec@1: [67.930]	
Starting epoch number: 260 Learning rate: 0.0010000000000000002
Train: [260]	Time 19.095	Data 0.226	Loss 0.611	Prec@1 81.4940	Prec@5 97.2920	
Val: [260]	Time 1.371	Data 0.107	Loss 1.249	Prec@1 67.8000	Prec@5 90.6900	
Best Prec@1: [67.930]	
Starting epoch number: 261 Learning rate: 0.0010000000000000002
Train: [261]	Time 19.101	Data 0.226	Loss 0.608	Prec@1 81.6540	Prec@5 97.3620	
Val: [261]	Time 1.344	Data 0.112	Loss 1.261	Prec@1 67.3700	Prec@5 90.7300	
Best Prec@1: [67.930]	
Starting epoch number: 262 Learning rate: 0.0010000000000000002
Train: [262]	Time 19.158	Data 0.233	Loss 0.611	Prec@1 81.5860	Prec@5 97.3380	
Val: [262]	Time 1.184	Data 0.090	Loss 1.262	Prec@1 67.4700	Prec@5 90.6100	
Best Prec@1: [67.930]	
Starting epoch number: 263 Learning rate: 0.0010000000000000002
Train: [263]	Time 19.134	Data 0.211	Loss 0.610	Prec@1 81.5020	Prec@5 97.1980	
Val: [263]	Time 1.425	Data 0.135	Loss 1.264	Prec@1 67.4500	Prec@5 90.4400	
Best Prec@1: [67.930]	
Starting epoch number: 264 Learning rate: 0.0010000000000000002
Train: [264]	Time 19.004	Data 0.211	Loss 0.613	Prec@1 81.3340	Prec@5 97.3180	
Val: [264]	Time 1.533	Data 0.127	Loss 1.269	Prec@1 67.4000	Prec@5 90.5600	
Best Prec@1: [67.930]	
Starting epoch number: 265 Learning rate: 0.0010000000000000002
Train: [265]	Time 19.111	Data 0.211	Loss 0.609	Prec@1 81.4420	Prec@5 97.3860	
Val: [265]	Time 1.518	Data 0.127	Loss 1.270	Prec@1 67.2100	Prec@5 90.6500	
Best Prec@1: [67.930]	
Starting epoch number: 266 Learning rate: 0.0010000000000000002
Train: [266]	Time 19.138	Data 0.214	Loss 0.607	Prec@1 81.5460	Prec@5 97.3440	
Val: [266]	Time 1.404	Data 0.113	Loss 1.261	Prec@1 67.6200	Prec@5 90.6500	
Best Prec@1: [67.930]	
Starting epoch number: 267 Learning rate: 0.0010000000000000002
Train: [267]	Time 18.883	Data 0.222	Loss 0.609	Prec@1 81.6260	Prec@5 97.2740	
Val: [267]	Time 1.433	Data 0.114	Loss 1.260	Prec@1 67.4700	Prec@5 90.6700	
Best Prec@1: [67.930]	
Starting epoch number: 268 Learning rate: 0.0010000000000000002
Train: [268]	Time 19.094	Data 0.212	Loss 0.605	Prec@1 81.5940	Prec@5 97.2940	
Val: [268]	Time 1.350	Data 0.128	Loss 1.250	Prec@1 67.5200	Prec@5 90.7200	
Best Prec@1: [67.930]	
Starting epoch number: 269 Learning rate: 0.0010000000000000002
Train: [269]	Time 18.680	Data 0.221	Loss 0.605	Prec@1 81.6860	Prec@5 97.3680	
Val: [269]	Time 1.343	Data 0.111	Loss 1.261	Prec@1 67.4900	Prec@5 90.6500	
Best Prec@1: [67.930]	
Starting epoch number: 270 Learning rate: 0.0010000000000000002
Train: [270]	Time 19.176	Data 0.223	Loss 0.605	Prec@1 81.6920	Prec@5 97.3300	
Val: [270]	Time 1.381	Data 0.123	Loss 1.269	Prec@1 67.3000	Prec@5 90.5000	
Best Prec@1: [67.930]	
Starting epoch number: 271 Learning rate: 0.0010000000000000002
Train: [271]	Time 18.865	Data 0.213	Loss 0.604	Prec@1 81.7480	Prec@5 97.3420	
Val: [271]	Time 1.225	Data 0.091	Loss 1.269	Prec@1 67.3100	Prec@5 90.5100	
Best Prec@1: [67.930]	
Starting epoch number: 272 Learning rate: 0.0010000000000000002
Train: [272]	Time 19.224	Data 0.224	Loss 0.602	Prec@1 81.5240	Prec@5 97.4760	
Val: [272]	Time 1.354	Data 0.117	Loss 1.283	Prec@1 67.2400	Prec@5 90.7500	
Best Prec@1: [67.930]	
Starting epoch number: 273 Learning rate: 0.0010000000000000002
Train: [273]	Time 18.592	Data 0.219	Loss 0.609	Prec@1 81.4460	Prec@5 97.3180	
Val: [273]	Time 1.396	Data 0.122	Loss 1.269	Prec@1 67.5000	Prec@5 90.6300	
Best Prec@1: [67.930]	
Starting epoch number: 274 Learning rate: 0.0010000000000000002
Train: [274]	Time 19.081	Data 0.217	Loss 0.604	Prec@1 81.7320	Prec@5 97.2960	
Val: [274]	Time 1.251	Data 0.105	Loss 1.272	Prec@1 67.3200	Prec@5 90.5300	
Best Prec@1: [67.930]	
Starting epoch number: 275 Learning rate: 0.0010000000000000002
Train: [275]	Time 19.094	Data 0.214	Loss 0.602	Prec@1 81.5360	Prec@5 97.4500	
Val: [275]	Time 1.243	Data 0.106	Loss 1.272	Prec@1 67.5700	Prec@5 90.5900	
Best Prec@1: [67.930]	
Starting epoch number: 276 Learning rate: 0.0010000000000000002
Train: [276]	Time 18.829	Data 0.209	Loss 0.604	Prec@1 81.5120	Prec@5 97.3740	
Val: [276]	Time 1.212	Data 0.093	Loss 1.276	Prec@1 67.1500	Prec@5 90.4100	
Best Prec@1: [67.930]	
Starting epoch number: 277 Learning rate: 0.0010000000000000002
Train: [277]	Time 18.858	Data 0.223	Loss 0.600	Prec@1 81.7740	Prec@5 97.4360	
Val: [277]	Time 1.247	Data 0.094	Loss 1.277	Prec@1 67.1500	Prec@5 90.6100	
Best Prec@1: [67.930]	
Starting epoch number: 278 Learning rate: 0.0010000000000000002
Train: [278]	Time 18.674	Data 0.220	Loss 0.601	Prec@1 81.5320	Prec@5 97.3740	
Val: [278]	Time 1.318	Data 0.117	Loss 1.282	Prec@1 67.3100	Prec@5 90.4600	
Best Prec@1: [67.930]	
Starting epoch number: 279 Learning rate: 0.0010000000000000002
Train: [279]	Time 19.128	Data 0.218	Loss 0.600	Prec@1 81.5740	Prec@5 97.4080	
Val: [279]	Time 1.263	Data 0.090	Loss 1.275	Prec@1 67.3700	Prec@5 90.5100	
Best Prec@1: [67.930]	
Starting epoch number: 280 Learning rate: 0.0010000000000000002
Train: [280]	Time 19.185	Data 0.224	Loss 0.602	Prec@1 81.7360	Prec@5 97.3460	
Val: [280]	Time 1.383	Data 0.106	Loss 1.279	Prec@1 67.0500	Prec@5 90.4300	
Best Prec@1: [67.930]	
Starting epoch number: 281 Learning rate: 0.0010000000000000002
Train: [281]	Time 19.214	Data 0.224	Loss 0.599	Prec@1 81.5320	Prec@5 97.4320	
Val: [281]	Time 1.365	Data 0.112	Loss 1.276	Prec@1 67.4100	Prec@5 90.6200	
Best Prec@1: [67.930]	
Starting epoch number: 282 Learning rate: 0.0010000000000000002
Train: [282]	Time 19.100	Data 0.216	Loss 0.603	Prec@1 81.8140	Prec@5 97.3440	
Val: [282]	Time 1.224	Data 0.093	Loss 1.277	Prec@1 67.2400	Prec@5 90.3500	
Best Prec@1: [67.930]	
Starting epoch number: 283 Learning rate: 0.0010000000000000002
Train: [283]	Time 18.859	Data 0.223	Loss 0.598	Prec@1 81.9020	Prec@5 97.4880	
Val: [283]	Time 1.202	Data 0.090	Loss 1.285	Prec@1 67.3900	Prec@5 90.6900	
Best Prec@1: [67.930]	
Starting epoch number: 284 Learning rate: 0.0010000000000000002
Train: [284]	Time 18.862	Data 0.223	Loss 0.596	Prec@1 81.7140	Prec@5 97.4600	
Val: [284]	Time 1.386	Data 0.115	Loss 1.283	Prec@1 67.4400	Prec@5 90.6300	
Best Prec@1: [67.930]	
Starting epoch number: 285 Learning rate: 0.0010000000000000002
Train: [285]	Time 19.144	Data 0.220	Loss 0.599	Prec@1 81.5580	Prec@5 97.4940	
Val: [285]	Time 1.311	Data 0.113	Loss 1.279	Prec@1 67.0800	Prec@5 90.6600	
Best Prec@1: [67.930]	
Starting epoch number: 286 Learning rate: 0.0010000000000000002
Train: [286]	Time 19.256	Data 0.217	Loss 0.599	Prec@1 81.7500	Prec@5 97.4660	
Val: [286]	Time 1.265	Data 0.099	Loss 1.279	Prec@1 67.0100	Prec@5 90.6000	
Best Prec@1: [67.930]	
Starting epoch number: 287 Learning rate: 0.0010000000000000002
Train: [287]	Time 19.015	Data 0.223	Loss 0.594	Prec@1 81.8160	Prec@5 97.5040	
Val: [287]	Time 1.454	Data 0.106	Loss 1.276	Prec@1 67.3400	Prec@5 90.5600	
Best Prec@1: [67.930]	
Starting epoch number: 288 Learning rate: 0.0010000000000000002
Train: [288]	Time 19.151	Data 0.211	Loss 0.599	Prec@1 81.7440	Prec@5 97.4320	
Val: [288]	Time 1.285	Data 0.102	Loss 1.287	Prec@1 67.1100	Prec@5 90.3300	
Best Prec@1: [67.930]	
Starting epoch number: 289 Learning rate: 0.0010000000000000002
Train: [289]	Time 19.123	Data 0.214	Loss 0.597	Prec@1 81.8200	Prec@5 97.3980	
Val: [289]	Time 1.469	Data 0.118	Loss 1.283	Prec@1 67.4800	Prec@5 90.5600	
Best Prec@1: [67.930]	
Starting epoch number: 290 Learning rate: 0.0010000000000000002
Train: [290]	Time 19.212	Data 0.217	Loss 0.600	Prec@1 81.8060	Prec@5 97.4620	
Val: [290]	Time 1.180	Data 0.088	Loss 1.298	Prec@1 67.1000	Prec@5 90.5300	
Best Prec@1: [67.930]	
Starting epoch number: 291 Learning rate: 0.0010000000000000002
Train: [291]	Time 19.123	Data 0.224	Loss 0.596	Prec@1 81.8980	Prec@5 97.3880	
Val: [291]	Time 1.317	Data 0.095	Loss 1.283	Prec@1 67.4300	Prec@5 90.5800	
Best Prec@1: [67.930]	
Starting epoch number: 292 Learning rate: 0.0010000000000000002
Train: [292]	Time 19.240	Data 0.225	Loss 0.596	Prec@1 81.7920	Prec@5 97.5160	
Val: [292]	Time 1.268	Data 0.105	Loss 1.285	Prec@1 67.2900	Prec@5 90.6300	
Best Prec@1: [67.930]	
Starting epoch number: 293 Learning rate: 0.0010000000000000002
Train: [293]	Time 19.049	Data 0.222	Loss 0.595	Prec@1 81.8800	Prec@5 97.4300	
Val: [293]	Time 1.328	Data 0.109	Loss 1.284	Prec@1 67.1700	Prec@5 90.6200	
Best Prec@1: [67.930]	
Starting epoch number: 294 Learning rate: 0.0010000000000000002
Train: [294]	Time 18.849	Data 0.211	Loss 0.595	Prec@1 81.8660	Prec@5 97.4320	
Val: [294]	Time 1.362	Data 0.102	Loss 1.285	Prec@1 66.9700	Prec@5 90.4300	
Best Prec@1: [67.930]	
Starting epoch number: 295 Learning rate: 0.0010000000000000002
Train: [295]	Time 18.827	Data 0.210	Loss 0.595	Prec@1 81.8780	Prec@5 97.3920	
Val: [295]	Time 1.378	Data 0.097	Loss 1.283	Prec@1 67.2900	Prec@5 90.5600	
Best Prec@1: [67.930]	
Starting epoch number: 296 Learning rate: 0.0010000000000000002
Train: [296]	Time 19.129	Data 0.228	Loss 0.594	Prec@1 81.8040	Prec@5 97.4900	
Val: [296]	Time 1.447	Data 0.118	Loss 1.302	Prec@1 67.0100	Prec@5 90.5500	
Best Prec@1: [67.930]	
Starting epoch number: 297 Learning rate: 0.0010000000000000002
Train: [297]	Time 18.941	Data 0.213	Loss 0.595	Prec@1 81.8860	Prec@5 97.5140	
Val: [297]	Time 1.186	Data 0.099	Loss 1.276	Prec@1 67.2500	Prec@5 90.7000	
Best Prec@1: [67.930]	
Starting epoch number: 298 Learning rate: 0.0010000000000000002
Train: [298]	Time 18.597	Data 0.221	Loss 0.595	Prec@1 81.7760	Prec@5 97.4160	
Val: [298]	Time 1.259	Data 0.108	Loss 1.278	Prec@1 67.4300	Prec@5 90.6500	
Best Prec@1: [67.930]	
Starting epoch number: 299 Learning rate: 0.0010000000000000002
Train: [299]	Time 19.151	Data 0.224	Loss 0.592	Prec@1 81.7960	Prec@5 97.5240	
Val: [299]	Time 1.223	Data 0.094	Loss 1.278	Prec@1 67.3200	Prec@5 90.6800	
Best Prec@1: [67.930]	
