Namespace(acc_type='class', augment=True, batch_size=64, bottleneck=True, cachemode=True, cardinality=8, criterion='crossentropy', cuda=True, data_dir='../data', dataset='cifar100', decayinterval=None, decaylevel=None, droprate=0, epochs=300, evaluate=False, expandConfig=None, expandSize=2, from_modelzoo=False, growth=4, layers=40, learningratescheduler='cifarschedular', logdir='../logs/densenetbc_cifar100_40_4', lr=0.1, manualSeed=123, maxlr=0.1, minlr=0.0005, model_def='densenet_cifar', momentum=0.9, name='densenetbc_cifar100_40_4', nclasses=100, nesterov=True, ngpus=1, optimType='sgd', pretrained=False, pretrained_file='', printfreq=200, reduce=0.5, resume='', start_epoch=0, store='', tenCrop=False, tensorboard=True, testOnly=False, verbose=False, weightDecay=0.0001, weight_init=False, widen_factor=4, workers=2)
DenseNet3 (
  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(12, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(28, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans1): TransitionBlock (
    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block2): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(28, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(36, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (trans2): TransitionBlock (
    (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU (inplace)
    (conv1): Conv2d(40, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (block3): DenseBlock (
    (layer): Sequential (
      (0): BottleneckBlock (
        (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (1): BottleneckBlock (
        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BottleneckBlock (
        (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(28, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BottleneckBlock (
        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (4): BottleneckBlock (
        (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(36, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (5): BottleneckBlock (
        (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU (inplace)
        (conv1): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True)
  (relu): ReLU (inplace)
  (fc): Linear (44 -> 100)
)
Files already downloaded and verified
Starting epoch number: 0 Learning rate: 0.1
Train: [0]	Time 19.053	Data 0.295	Loss 4.010	Prec@1 8.1380	Prec@5 26.0380	
Val: [0]	Time 1.351	Data 0.097	Loss 3.698	Prec@1 12.1500	Prec@5 36.9200	
Best Prec@1: [12.150]	
Starting epoch number: 1 Learning rate: 0.1
Train: [1]	Time 17.934	Data 0.208	Loss 3.473	Prec@1 15.4700	Prec@5 42.4540	
Val: [1]	Time 1.258	Data 0.088	Loss 3.372	Prec@1 17.4700	Prec@5 46.4100	
Best Prec@1: [17.470]	
Starting epoch number: 2 Learning rate: 0.1
Train: [2]	Time 17.969	Data 0.211	Loss 3.084	Prec@1 22.2680	Prec@5 52.9920	
Val: [2]	Time 1.417	Data 0.108	Loss 3.120	Prec@1 22.2200	Prec@5 53.2600	
Best Prec@1: [22.220]	
Starting epoch number: 3 Learning rate: 0.1
Train: [3]	Time 17.824	Data 0.220	Loss 2.874	Prec@1 26.6420	Prec@5 58.5140	
Val: [3]	Time 1.456	Data 0.139	Loss 2.857	Prec@1 27.0500	Prec@5 59.7200	
Best Prec@1: [27.050]	
Starting epoch number: 4 Learning rate: 0.1
Train: [4]	Time 17.847	Data 0.224	Loss 2.722	Prec@1 29.9740	Prec@5 62.3300	
Val: [4]	Time 1.363	Data 0.099	Loss 2.835	Prec@1 28.1400	Prec@5 60.4000	
Best Prec@1: [28.140]	
Starting epoch number: 5 Learning rate: 0.1
Train: [5]	Time 17.837	Data 0.222	Loss 2.616	Prec@1 31.8680	Prec@5 64.9420	
Val: [5]	Time 1.304	Data 0.096	Loss 2.741	Prec@1 30.1400	Prec@5 62.0200	
Best Prec@1: [30.140]	
Starting epoch number: 6 Learning rate: 0.1
Train: [6]	Time 17.858	Data 0.217	Loss 2.526	Prec@1 33.9700	Prec@5 66.9680	
Val: [6]	Time 1.345	Data 0.104	Loss 2.734	Prec@1 31.0100	Prec@5 63.3700	
Best Prec@1: [31.010]	
Starting epoch number: 7 Learning rate: 0.1
Train: [7]	Time 17.796	Data 0.214	Loss 2.458	Prec@1 35.1560	Prec@5 68.6800	
Val: [7]	Time 1.443	Data 0.109	Loss 2.609	Prec@1 32.8700	Prec@5 66.4300	
Best Prec@1: [32.870]	
Starting epoch number: 8 Learning rate: 0.1
Train: [8]	Time 17.861	Data 0.219	Loss 2.400	Prec@1 36.5800	Prec@5 69.6580	
Val: [8]	Time 1.343	Data 0.106	Loss 2.532	Prec@1 35.4600	Prec@5 68.2700	
Best Prec@1: [35.460]	
Starting epoch number: 9 Learning rate: 0.1
Train: [9]	Time 17.881	Data 0.216	Loss 2.352	Prec@1 37.5560	Prec@5 70.8600	
Val: [9]	Time 1.315	Data 0.101	Loss 2.514	Prec@1 35.4200	Prec@5 68.5600	
Best Prec@1: [35.460]	
Starting epoch number: 10 Learning rate: 0.1
Train: [10]	Time 18.001	Data 0.229	Loss 2.304	Prec@1 38.6900	Prec@5 71.6240	
Val: [10]	Time 1.438	Data 0.112	Loss 2.509	Prec@1 36.9900	Prec@5 69.3900	
Best Prec@1: [36.990]	
Starting epoch number: 11 Learning rate: 0.1
Train: [11]	Time 17.448	Data 0.215	Loss 2.271	Prec@1 39.1700	Prec@5 72.4760	
Val: [11]	Time 1.373	Data 0.127	Loss 2.511	Prec@1 36.0800	Prec@5 68.3900	
Best Prec@1: [36.990]	
Starting epoch number: 12 Learning rate: 0.1
Train: [12]	Time 17.650	Data 0.208	Loss 2.249	Prec@1 39.9360	Prec@5 72.8900	
Val: [12]	Time 1.422	Data 0.109	Loss 2.520	Prec@1 36.2900	Prec@5 69.3800	
Best Prec@1: [36.990]	
Starting epoch number: 13 Learning rate: 0.1
Train: [13]	Time 17.845	Data 0.216	Loss 2.219	Prec@1 40.5620	Prec@5 73.5980	
Val: [13]	Time 1.316	Data 0.092	Loss 2.522	Prec@1 36.9800	Prec@5 69.9000	
Best Prec@1: [36.990]	
Starting epoch number: 14 Learning rate: 0.1
Train: [14]	Time 17.836	Data 0.215	Loss 2.186	Prec@1 41.3680	Prec@5 74.2560	
Val: [14]	Time 1.490	Data 0.124	Loss 2.527	Prec@1 36.2500	Prec@5 68.8600	
Best Prec@1: [36.990]	
Starting epoch number: 15 Learning rate: 0.1
Train: [15]	Time 17.842	Data 0.214	Loss 2.173	Prec@1 41.4060	Prec@5 74.5860	
Val: [15]	Time 1.535	Data 0.142	Loss 2.536	Prec@1 37.3500	Prec@5 69.7600	
Best Prec@1: [37.350]	
Starting epoch number: 16 Learning rate: 0.1
Train: [16]	Time 17.809	Data 0.226	Loss 2.152	Prec@1 41.6840	Prec@5 75.0000	
Val: [16]	Time 1.411	Data 0.099	Loss 2.509	Prec@1 36.8000	Prec@5 70.2900	
Best Prec@1: [37.350]	
Starting epoch number: 17 Learning rate: 0.1
Train: [17]	Time 17.988	Data 0.206	Loss 2.140	Prec@1 42.0020	Prec@5 75.1860	
Val: [17]	Time 1.350	Data 0.091	Loss 2.345	Prec@1 39.2600	Prec@5 72.3300	
Best Prec@1: [39.260]	
Starting epoch number: 18 Learning rate: 0.1
Train: [18]	Time 17.573	Data 0.210	Loss 2.122	Prec@1 42.4880	Prec@5 75.5500	
Val: [18]	Time 1.449	Data 0.107	Loss 2.700	Prec@1 32.3800	Prec@5 64.6000	
Best Prec@1: [39.260]	
Starting epoch number: 19 Learning rate: 0.1
Train: [19]	Time 17.804	Data 0.214	Loss 2.109	Prec@1 42.7860	Prec@5 75.7920	
Val: [19]	Time 1.294	Data 0.096	Loss 2.341	Prec@1 39.8300	Prec@5 73.0000	
Best Prec@1: [39.830]	
Starting epoch number: 20 Learning rate: 0.1
Train: [20]	Time 17.627	Data 0.215	Loss 2.095	Prec@1 43.2100	Prec@5 76.1920	
Val: [20]	Time 1.412	Data 0.117	Loss 2.371	Prec@1 39.0600	Prec@5 72.0200	
Best Prec@1: [39.830]	
Starting epoch number: 21 Learning rate: 0.1
Train: [21]	Time 17.615	Data 0.215	Loss 2.086	Prec@1 43.5520	Prec@5 76.2260	
Val: [21]	Time 1.270	Data 0.094	Loss 2.161	Prec@1 42.5300	Prec@5 75.5000	
Best Prec@1: [42.530]	
Starting epoch number: 22 Learning rate: 0.1
Train: [22]	Time 17.820	Data 0.206	Loss 2.072	Prec@1 43.8800	Prec@5 76.6360	
Val: [22]	Time 1.462	Data 0.116	Loss 2.139	Prec@1 42.8300	Prec@5 75.5000	
Best Prec@1: [42.830]	
Starting epoch number: 23 Learning rate: 0.1
Train: [23]	Time 17.737	Data 0.207	Loss 2.064	Prec@1 44.0640	Prec@5 76.8760	
Val: [23]	Time 1.307	Data 0.096	Loss 2.359	Prec@1 39.5300	Prec@5 71.9500	
Best Prec@1: [42.830]	
Starting epoch number: 24 Learning rate: 0.1
Train: [24]	Time 17.882	Data 0.222	Loss 2.053	Prec@1 44.3280	Prec@5 76.9080	
Val: [24]	Time 1.344	Data 0.115	Loss 2.248	Prec@1 41.7300	Prec@5 74.9000	
Best Prec@1: [42.830]	
Starting epoch number: 25 Learning rate: 0.1
Train: [25]	Time 17.263	Data 0.215	Loss 2.043	Prec@1 44.5900	Prec@5 77.0180	
Val: [25]	Time 1.330	Data 0.112	Loss 2.188	Prec@1 41.8100	Prec@5 75.1700	
Best Prec@1: [42.830]	
Starting epoch number: 26 Learning rate: 0.1
Train: [26]	Time 17.663	Data 0.215	Loss 2.035	Prec@1 44.5380	Prec@5 77.1320	
Val: [26]	Time 1.487	Data 0.115	Loss 2.158	Prec@1 42.9000	Prec@5 76.3500	
Best Prec@1: [42.900]	
Starting epoch number: 27 Learning rate: 0.1
Train: [27]	Time 17.888	Data 0.216	Loss 2.028	Prec@1 44.8360	Prec@5 77.4420	
Val: [27]	Time 1.330	Data 0.107	Loss 2.081	Prec@1 44.3600	Prec@5 76.9000	
Best Prec@1: [44.360]	
Starting epoch number: 28 Learning rate: 0.1
Train: [28]	Time 17.880	Data 0.214	Loss 2.023	Prec@1 44.8700	Prec@5 77.6340	
Val: [28]	Time 1.367	Data 0.107	Loss 2.247	Prec@1 39.8800	Prec@5 73.5200	
Best Prec@1: [44.360]	
Starting epoch number: 29 Learning rate: 0.1
Train: [29]	Time 17.733	Data 0.216	Loss 2.018	Prec@1 45.2560	Prec@5 77.6260	
Val: [29]	Time 1.321	Data 0.110	Loss 2.105	Prec@1 43.7100	Prec@5 76.8100	
Best Prec@1: [44.360]	
Starting epoch number: 30 Learning rate: 0.1
Train: [30]	Time 17.839	Data 0.216	Loss 2.011	Prec@1 45.0500	Prec@5 77.8780	
Val: [30]	Time 1.274	Data 0.092	Loss 2.202	Prec@1 42.3800	Prec@5 75.1900	
Best Prec@1: [44.360]	
Starting epoch number: 31 Learning rate: 0.1
Train: [31]	Time 17.707	Data 0.218	Loss 1.999	Prec@1 45.3020	Prec@5 78.0580	
Val: [31]	Time 1.321	Data 0.099	Loss 2.283	Prec@1 41.0700	Prec@5 74.7500	
Best Prec@1: [44.360]	
Starting epoch number: 32 Learning rate: 0.1
Train: [32]	Time 17.813	Data 0.225	Loss 1.997	Prec@1 45.3380	Prec@5 78.0280	
Val: [32]	Time 1.252	Data 0.091	Loss 2.241	Prec@1 42.8200	Prec@5 74.9500	
Best Prec@1: [44.360]	
Starting epoch number: 33 Learning rate: 0.1
Train: [33]	Time 17.857	Data 0.208	Loss 1.990	Prec@1 45.6560	Prec@5 78.0320	
Val: [33]	Time 1.369	Data 0.103	Loss 2.299	Prec@1 40.4700	Prec@5 73.2300	
Best Prec@1: [44.360]	
Starting epoch number: 34 Learning rate: 0.1
Train: [34]	Time 17.887	Data 0.219	Loss 1.984	Prec@1 46.1900	Prec@5 78.3060	
Val: [34]	Time 1.418	Data 0.101	Loss 2.308	Prec@1 41.1300	Prec@5 73.1500	
Best Prec@1: [44.360]	
Starting epoch number: 35 Learning rate: 0.1
Train: [35]	Time 17.872	Data 0.216	Loss 1.978	Prec@1 45.9380	Prec@5 78.3680	
Val: [35]	Time 1.308	Data 0.099	Loss 2.238	Prec@1 41.7000	Prec@5 75.0600	
Best Prec@1: [44.360]	
Starting epoch number: 36 Learning rate: 0.1
Train: [36]	Time 17.838	Data 0.207	Loss 1.975	Prec@1 46.0140	Prec@5 78.4480	
Val: [36]	Time 1.272	Data 0.105	Loss 2.452	Prec@1 39.2000	Prec@5 72.0900	
Best Prec@1: [44.360]	
Starting epoch number: 37 Learning rate: 0.1
Train: [37]	Time 17.717	Data 0.213	Loss 1.970	Prec@1 45.8860	Prec@5 78.6160	
Val: [37]	Time 1.288	Data 0.093	Loss 2.204	Prec@1 41.9800	Prec@5 74.4500	
Best Prec@1: [44.360]	
Starting epoch number: 38 Learning rate: 0.1
Train: [38]	Time 17.598	Data 0.214	Loss 1.968	Prec@1 46.1620	Prec@5 78.5620	
Val: [38]	Time 1.369	Data 0.101	Loss 2.108	Prec@1 43.9400	Prec@5 76.1900	
Best Prec@1: [44.360]	
Starting epoch number: 39 Learning rate: 0.1
Train: [39]	Time 17.810	Data 0.209	Loss 1.957	Prec@1 46.6880	Prec@5 78.6300	
Val: [39]	Time 1.300	Data 0.093	Loss 2.255	Prec@1 41.1200	Prec@5 74.7300	
Best Prec@1: [44.360]	
Starting epoch number: 40 Learning rate: 0.1
Train: [40]	Time 17.861	Data 0.216	Loss 1.952	Prec@1 46.5300	Prec@5 79.0000	
Val: [40]	Time 1.303	Data 0.103	Loss 2.117	Prec@1 43.7300	Prec@5 75.3900	
Best Prec@1: [44.360]	
Starting epoch number: 41 Learning rate: 0.1
Train: [41]	Time 17.798	Data 0.213	Loss 1.954	Prec@1 46.2880	Prec@5 78.7160	
Val: [41]	Time 1.310	Data 0.101	Loss 2.194	Prec@1 42.6600	Prec@5 75.3600	
Best Prec@1: [44.360]	
Starting epoch number: 42 Learning rate: 0.1
Train: [42]	Time 17.946	Data 0.216	Loss 1.957	Prec@1 46.8720	Prec@5 78.6660	
Val: [42]	Time 1.252	Data 0.088	Loss 2.233	Prec@1 42.1800	Prec@5 74.4200	
Best Prec@1: [44.360]	
Starting epoch number: 43 Learning rate: 0.1
Train: [43]	Time 17.892	Data 0.215	Loss 1.941	Prec@1 46.9120	Prec@5 79.1000	
Val: [43]	Time 1.411	Data 0.122	Loss 2.084	Prec@1 44.4200	Prec@5 76.7000	
Best Prec@1: [44.420]	
Starting epoch number: 44 Learning rate: 0.1
Train: [44]	Time 17.888	Data 0.211	Loss 1.940	Prec@1 46.7340	Prec@5 79.0680	
Val: [44]	Time 1.377	Data 0.107	Loss 2.316	Prec@1 40.5500	Prec@5 73.6300	
Best Prec@1: [44.420]	
Starting epoch number: 45 Learning rate: 0.1
Train: [45]	Time 17.915	Data 0.216	Loss 1.935	Prec@1 46.8180	Prec@5 79.3020	
Val: [45]	Time 1.333	Data 0.105	Loss 2.096	Prec@1 44.5200	Prec@5 76.0100	
Best Prec@1: [44.520]	
Starting epoch number: 46 Learning rate: 0.1
Train: [46]	Time 17.959	Data 0.208	Loss 1.938	Prec@1 47.1640	Prec@5 79.3120	
Val: [46]	Time 1.426	Data 0.104	Loss 2.159	Prec@1 43.5700	Prec@5 75.8200	
Best Prec@1: [44.520]	
Starting epoch number: 47 Learning rate: 0.1
Train: [47]	Time 17.957	Data 0.206	Loss 1.928	Prec@1 46.8760	Prec@5 79.5180	
Val: [47]	Time 1.375	Data 0.114	Loss 2.188	Prec@1 43.3700	Prec@5 74.8200	
Best Prec@1: [44.520]	
Starting epoch number: 48 Learning rate: 0.1
Train: [48]	Time 17.904	Data 0.214	Loss 1.935	Prec@1 47.3280	Prec@5 79.1940	
Val: [48]	Time 1.332	Data 0.121	Loss 2.015	Prec@1 46.1400	Prec@5 78.3900	
Best Prec@1: [46.140]	
Starting epoch number: 49 Learning rate: 0.1
Train: [49]	Time 17.827	Data 0.211	Loss 1.926	Prec@1 47.1500	Prec@5 79.2660	
Val: [49]	Time 1.279	Data 0.088	Loss 2.228	Prec@1 42.4900	Prec@5 74.2600	
Best Prec@1: [46.140]	
Starting epoch number: 50 Learning rate: 0.1
Train: [50]	Time 17.859	Data 0.218	Loss 1.924	Prec@1 47.2360	Prec@5 79.3220	
Val: [50]	Time 1.363	Data 0.114	Loss 2.530	Prec@1 39.1200	Prec@5 72.0300	
Best Prec@1: [46.140]	
Starting epoch number: 51 Learning rate: 0.1
Train: [51]	Time 17.898	Data 0.207	Loss 1.924	Prec@1 47.2920	Prec@5 79.3480	
Val: [51]	Time 1.415	Data 0.124	Loss 2.030	Prec@1 45.4400	Prec@5 77.4800	
Best Prec@1: [46.140]	
Starting epoch number: 52 Learning rate: 0.1
Train: [52]	Time 17.838	Data 0.207	Loss 1.921	Prec@1 47.0700	Prec@5 79.3840	
Val: [52]	Time 1.377	Data 0.099	Loss 2.148	Prec@1 43.7600	Prec@5 75.6100	
Best Prec@1: [46.140]	
Starting epoch number: 53 Learning rate: 0.1
Train: [53]	Time 17.855	Data 0.215	Loss 1.917	Prec@1 47.5560	Prec@5 79.4160	
Val: [53]	Time 1.456	Data 0.099	Loss 2.274	Prec@1 42.4600	Prec@5 73.9200	
Best Prec@1: [46.140]	
Starting epoch number: 54 Learning rate: 0.1
Train: [54]	Time 17.277	Data 0.205	Loss 1.917	Prec@1 47.3020	Prec@5 79.4840	
Val: [54]	Time 1.521	Data 0.100	Loss 2.082	Prec@1 43.7800	Prec@5 76.4600	
Best Prec@1: [46.140]	
Starting epoch number: 55 Learning rate: 0.1
Train: [55]	Time 17.962	Data 0.216	Loss 1.911	Prec@1 47.3820	Prec@5 79.6640	
Val: [55]	Time 1.393	Data 0.108	Loss 2.083	Prec@1 45.1500	Prec@5 76.8300	
Best Prec@1: [46.140]	
Starting epoch number: 56 Learning rate: 0.1
Train: [56]	Time 17.823	Data 0.206	Loss 1.915	Prec@1 47.4700	Prec@5 79.6200	
Val: [56]	Time 1.449	Data 0.111	Loss 2.167	Prec@1 42.6800	Prec@5 76.0200	
Best Prec@1: [46.140]	
Starting epoch number: 57 Learning rate: 0.1
Train: [57]	Time 17.859	Data 0.220	Loss 1.905	Prec@1 47.2840	Prec@5 79.8380	
Val: [57]	Time 1.322	Data 0.102	Loss 2.079	Prec@1 43.9300	Prec@5 77.4600	
Best Prec@1: [46.140]	
Starting epoch number: 58 Learning rate: 0.1
Train: [58]	Time 17.808	Data 0.207	Loss 1.900	Prec@1 47.7440	Prec@5 79.7980	
Val: [58]	Time 1.303	Data 0.096	Loss 2.166	Prec@1 43.4100	Prec@5 76.5100	
Best Prec@1: [46.140]	
Starting epoch number: 59 Learning rate: 0.1
Train: [59]	Time 17.917	Data 0.216	Loss 1.901	Prec@1 47.7500	Prec@5 79.8580	
Val: [59]	Time 1.497	Data 0.146	Loss 2.136	Prec@1 42.9200	Prec@5 76.9000	
Best Prec@1: [46.140]	
Starting epoch number: 60 Learning rate: 0.1
Train: [60]	Time 17.853	Data 0.219	Loss 1.904	Prec@1 47.4980	Prec@5 79.7040	
Val: [60]	Time 1.330	Data 0.110	Loss 2.206	Prec@1 42.7700	Prec@5 75.4400	
Best Prec@1: [46.140]	
Starting epoch number: 61 Learning rate: 0.1
Train: [61]	Time 17.936	Data 0.206	Loss 1.903	Prec@1 47.6680	Prec@5 79.7900	
Val: [61]	Time 1.431	Data 0.106	Loss 2.221	Prec@1 41.4400	Prec@5 74.8900	
Best Prec@1: [46.140]	
Starting epoch number: 62 Learning rate: 0.1
Train: [62]	Time 17.635	Data 0.206	Loss 1.905	Prec@1 47.7700	Prec@5 79.7780	
Val: [62]	Time 1.393	Data 0.107	Loss 2.096	Prec@1 44.1300	Prec@5 77.2200	
Best Prec@1: [46.140]	
Starting epoch number: 63 Learning rate: 0.1
Train: [63]	Time 17.610	Data 0.214	Loss 1.895	Prec@1 47.8140	Prec@5 79.9660	
Val: [63]	Time 1.282	Data 0.096	Loss 2.132	Prec@1 44.0400	Prec@5 76.2400	
Best Prec@1: [46.140]	
Starting epoch number: 64 Learning rate: 0.1
Train: [64]	Time 17.659	Data 0.214	Loss 1.892	Prec@1 47.8800	Prec@5 80.2660	
Val: [64]	Time 1.263	Data 0.095	Loss 2.079	Prec@1 45.1400	Prec@5 77.1200	
Best Prec@1: [46.140]	
Starting epoch number: 65 Learning rate: 0.1
Train: [65]	Time 17.962	Data 0.219	Loss 1.897	Prec@1 47.7880	Prec@5 79.8840	
Val: [65]	Time 1.378	Data 0.095	Loss 2.099	Prec@1 45.2000	Prec@5 76.7700	
Best Prec@1: [46.140]	
Starting epoch number: 66 Learning rate: 0.1
Train: [66]	Time 17.911	Data 0.208	Loss 1.896	Prec@1 48.0940	Prec@5 79.8320	
Val: [66]	Time 1.415	Data 0.098	Loss 2.223	Prec@1 42.4400	Prec@5 74.6900	
Best Prec@1: [46.140]	
Starting epoch number: 67 Learning rate: 0.1
Train: [67]	Time 17.935	Data 0.209	Loss 1.890	Prec@1 47.8220	Prec@5 80.1120	
Val: [67]	Time 1.554	Data 0.133	Loss 2.245	Prec@1 43.2400	Prec@5 74.9800	
Best Prec@1: [46.140]	
Starting epoch number: 68 Learning rate: 0.1
Train: [68]	Time 17.923	Data 0.208	Loss 1.886	Prec@1 47.9500	Prec@5 80.1960	
Val: [68]	Time 1.409	Data 0.104	Loss 2.127	Prec@1 43.3000	Prec@5 76.2700	
Best Prec@1: [46.140]	
Starting epoch number: 69 Learning rate: 0.1
Train: [69]	Time 17.925	Data 0.217	Loss 1.882	Prec@1 48.2200	Prec@5 80.2640	
Val: [69]	Time 1.353	Data 0.105	Loss 2.047	Prec@1 46.1200	Prec@5 77.6500	
Best Prec@1: [46.140]	
Starting epoch number: 70 Learning rate: 0.1
Train: [70]	Time 17.873	Data 0.218	Loss 1.881	Prec@1 48.1680	Prec@5 80.2080	
Val: [70]	Time 1.340	Data 0.103	Loss 2.102	Prec@1 44.1000	Prec@5 76.8800	
Best Prec@1: [46.140]	
Starting epoch number: 71 Learning rate: 0.1
Train: [71]	Time 17.910	Data 0.206	Loss 1.888	Prec@1 48.0760	Prec@5 79.8680	
Val: [71]	Time 1.520	Data 0.125	Loss 2.121	Prec@1 44.1100	Prec@5 76.2100	
Best Prec@1: [46.140]	
Starting epoch number: 72 Learning rate: 0.1
Train: [72]	Time 18.036	Data 0.226	Loss 1.876	Prec@1 48.2680	Prec@5 80.3860	
Val: [72]	Time 1.479	Data 0.119	Loss 1.999	Prec@1 45.7200	Prec@5 78.7800	
Best Prec@1: [46.140]	
Starting epoch number: 73 Learning rate: 0.1
Train: [73]	Time 18.142	Data 0.209	Loss 1.882	Prec@1 48.3160	Prec@5 80.1020	
Val: [73]	Time 1.303	Data 0.091	Loss 2.281	Prec@1 43.0400	Prec@5 74.2000	
Best Prec@1: [46.140]	
Starting epoch number: 74 Learning rate: 0.1
Train: [74]	Time 17.890	Data 0.225	Loss 1.880	Prec@1 48.3440	Prec@5 80.1620	
Val: [74]	Time 1.357	Data 0.095	Loss 2.255	Prec@1 43.7700	Prec@5 75.6200	
Best Prec@1: [46.140]	
Starting epoch number: 75 Learning rate: 0.1
Train: [75]	Time 18.015	Data 0.219	Loss 1.882	Prec@1 48.2900	Prec@5 80.1800	
Val: [75]	Time 1.878	Data 0.390	Loss 2.047	Prec@1 45.8400	Prec@5 77.7400	
Best Prec@1: [46.140]	
Starting epoch number: 76 Learning rate: 0.1
Train: [76]	Time 18.040	Data 0.212	Loss 1.876	Prec@1 48.3120	Prec@5 80.2460	
Val: [76]	Time 1.313	Data 0.091	Loss 2.164	Prec@1 44.3600	Prec@5 76.3900	
Best Prec@1: [46.140]	
Starting epoch number: 77 Learning rate: 0.1
Train: [77]	Time 17.879	Data 0.206	Loss 1.879	Prec@1 48.4760	Prec@5 80.2780	
Val: [77]	Time 1.344	Data 0.095	Loss 2.213	Prec@1 43.4400	Prec@5 76.0300	
Best Prec@1: [46.140]	
Starting epoch number: 78 Learning rate: 0.1
Train: [78]	Time 17.874	Data 0.214	Loss 1.875	Prec@1 48.1840	Prec@5 80.1300	
Val: [78]	Time 1.333	Data 0.108	Loss 2.055	Prec@1 45.3900	Prec@5 77.8800	
Best Prec@1: [46.140]	
Starting epoch number: 79 Learning rate: 0.1
Train: [79]	Time 17.994	Data 0.208	Loss 1.872	Prec@1 48.1120	Prec@5 80.2880	
Val: [79]	Time 1.359	Data 0.104	Loss 2.276	Prec@1 41.6400	Prec@5 74.3100	
Best Prec@1: [46.140]	
Starting epoch number: 80 Learning rate: 0.1
Train: [80]	Time 17.881	Data 0.212	Loss 1.874	Prec@1 48.2180	Prec@5 80.3400	
Val: [80]	Time 1.210	Data 0.087	Loss 2.050	Prec@1 45.8800	Prec@5 77.4300	
Best Prec@1: [46.140]	
Starting epoch number: 81 Learning rate: 0.1
Train: [81]	Time 17.927	Data 0.218	Loss 1.867	Prec@1 48.6320	Prec@5 80.5080	
Val: [81]	Time 1.319	Data 0.119	Loss 2.154	Prec@1 44.5000	Prec@5 76.3700	
Best Prec@1: [46.140]	
Starting epoch number: 82 Learning rate: 0.1
Train: [82]	Time 17.975	Data 0.221	Loss 1.871	Prec@1 48.5260	Prec@5 80.3400	
Val: [82]	Time 1.418	Data 0.095	Loss 2.209	Prec@1 43.0100	Prec@5 74.3100	
Best Prec@1: [46.140]	
Starting epoch number: 83 Learning rate: 0.1
Train: [83]	Time 18.007	Data 0.210	Loss 1.864	Prec@1 48.6260	Prec@5 80.4040	
Val: [83]	Time 1.273	Data 0.097	Loss 2.046	Prec@1 45.3500	Prec@5 78.1800	
Best Prec@1: [46.140]	
Starting epoch number: 84 Learning rate: 0.1
Train: [84]	Time 17.932	Data 0.217	Loss 1.870	Prec@1 48.1200	Prec@5 80.3160	
Val: [84]	Time 1.261	Data 0.088	Loss 2.083	Prec@1 44.6800	Prec@5 77.0700	
Best Prec@1: [46.140]	
Starting epoch number: 85 Learning rate: 0.1
Train: [85]	Time 18.067	Data 0.209	Loss 1.864	Prec@1 48.3940	Prec@5 80.4200	
Val: [85]	Time 1.472	Data 0.111	Loss 2.109	Prec@1 44.8900	Prec@5 77.3400	
Best Prec@1: [46.140]	
Starting epoch number: 86 Learning rate: 0.1
Train: [86]	Time 17.971	Data 0.207	Loss 1.859	Prec@1 48.6580	Prec@5 80.4880	
Val: [86]	Time 1.284	Data 0.090	Loss 2.234	Prec@1 42.6600	Prec@5 75.2000	
Best Prec@1: [46.140]	
Starting epoch number: 87 Learning rate: 0.1
Train: [87]	Time 17.879	Data 0.206	Loss 1.863	Prec@1 48.6840	Prec@5 80.5680	
Val: [87]	Time 1.286	Data 0.094	Loss 2.128	Prec@1 44.4100	Prec@5 76.3800	
Best Prec@1: [46.140]	
Starting epoch number: 88 Learning rate: 0.1
Train: [88]	Time 17.873	Data 0.209	Loss 1.860	Prec@1 48.5020	Prec@5 80.7600	
Val: [88]	Time 1.407	Data 0.102	Loss 2.058	Prec@1 45.6600	Prec@5 76.9500	
Best Prec@1: [46.140]	
Starting epoch number: 89 Learning rate: 0.1
Train: [89]	Time 17.656	Data 0.217	Loss 1.865	Prec@1 48.5240	Prec@5 80.5720	
Val: [89]	Time 1.396	Data 0.126	Loss 2.444	Prec@1 39.7700	Prec@5 72.0200	
Best Prec@1: [46.140]	
Starting epoch number: 90 Learning rate: 0.1
Train: [90]	Time 17.931	Data 0.219	Loss 1.861	Prec@1 48.6240	Prec@5 80.4360	
Val: [90]	Time 1.431	Data 0.102	Loss 2.160	Prec@1 43.6900	Prec@5 76.3200	
Best Prec@1: [46.140]	
Starting epoch number: 91 Learning rate: 0.1
Train: [91]	Time 16.224	Data 0.204	Loss 1.854	Prec@1 48.7840	Prec@5 80.5980	
Val: [91]	Time 1.254	Data 0.099	Loss 2.169	Prec@1 43.5000	Prec@5 76.0800	
Best Prec@1: [46.140]	
Starting epoch number: 92 Learning rate: 0.1
Train: [92]	Time 15.601	Data 0.201	Loss 1.860	Prec@1 48.6220	Prec@5 80.4600	
Val: [92]	Time 1.286	Data 0.091	Loss 2.210	Prec@1 43.2100	Prec@5 75.5400	
Best Prec@1: [46.140]	
Starting epoch number: 93 Learning rate: 0.1
Train: [93]	Time 15.822	Data 0.200	Loss 1.855	Prec@1 48.8260	Prec@5 80.6680	
Val: [93]	Time 1.334	Data 0.101	Loss 1.977	Prec@1 47.3600	Prec@5 78.7400	
Best Prec@1: [47.360]	
Starting epoch number: 94 Learning rate: 0.1
Train: [94]	Time 15.777	Data 0.204	Loss 1.861	Prec@1 48.8300	Prec@5 80.4940	
Val: [94]	Time 1.273	Data 0.099	Loss 2.069	Prec@1 45.3700	Prec@5 77.8200	
Best Prec@1: [47.360]	
Starting epoch number: 95 Learning rate: 0.1
Train: [95]	Time 16.376	Data 0.210	Loss 1.857	Prec@1 48.9400	Prec@5 80.5280	
Val: [95]	Time 1.279	Data 0.099	Loss 2.200	Prec@1 42.6800	Prec@5 75.8200	
Best Prec@1: [47.360]	
Starting epoch number: 96 Learning rate: 0.1
Train: [96]	Time 15.959	Data 0.203	Loss 1.854	Prec@1 48.8360	Prec@5 80.5200	
Val: [96]	Time 1.219	Data 0.088	Loss 1.940	Prec@1 47.5200	Prec@5 79.7600	
Best Prec@1: [47.520]	
Starting epoch number: 97 Learning rate: 0.1
Train: [97]	Time 15.694	Data 0.201	Loss 1.853	Prec@1 48.7700	Prec@5 80.7160	
Val: [97]	Time 1.296	Data 0.093	Loss 2.143	Prec@1 43.2000	Prec@5 75.6500	
Best Prec@1: [47.520]	
Starting epoch number: 98 Learning rate: 0.1
Train: [98]	Time 15.542	Data 0.202	Loss 1.853	Prec@1 48.8800	Prec@5 80.7160	
Val: [98]	Time 1.273	Data 0.095	Loss 2.157	Prec@1 44.1300	Prec@5 76.5300	
Best Prec@1: [47.520]	
Starting epoch number: 99 Learning rate: 0.1
Train: [99]	Time 15.915	Data 0.206	Loss 1.857	Prec@1 48.8900	Prec@5 80.6700	
Val: [99]	Time 1.290	Data 0.098	Loss 2.202	Prec@1 44.4300	Prec@5 75.0700	
Best Prec@1: [47.520]	
Starting epoch number: 100 Learning rate: 0.1
Train: [100]	Time 15.813	Data 0.210	Loss 1.852	Prec@1 48.9080	Prec@5 80.8200	
Val: [100]	Time 1.261	Data 0.104	Loss 2.054	Prec@1 45.8000	Prec@5 77.6800	
Best Prec@1: [47.520]	
Starting epoch number: 101 Learning rate: 0.1
Train: [101]	Time 15.605	Data 0.205	Loss 1.855	Prec@1 48.7460	Prec@5 80.7900	
Val: [101]	Time 1.260	Data 0.096	Loss 2.221	Prec@1 42.7500	Prec@5 75.9200	
Best Prec@1: [47.520]	
Starting epoch number: 102 Learning rate: 0.1
Train: [102]	Time 16.047	Data 0.203	Loss 1.848	Prec@1 49.0500	Prec@5 80.8200	
Val: [102]	Time 1.353	Data 0.101	Loss 2.235	Prec@1 42.9200	Prec@5 75.1600	
Best Prec@1: [47.520]	
Starting epoch number: 103 Learning rate: 0.1
Train: [103]	Time 16.003	Data 0.207	Loss 1.847	Prec@1 48.9980	Prec@5 80.6800	
Val: [103]	Time 1.269	Data 0.101	Loss 2.142	Prec@1 43.9900	Prec@5 76.8900	
Best Prec@1: [47.520]	
Starting epoch number: 104 Learning rate: 0.1
Train: [104]	Time 15.611	Data 0.201	Loss 1.849	Prec@1 49.1660	Prec@5 80.5480	
Val: [104]	Time 1.296	Data 0.093	Loss 2.089	Prec@1 45.3000	Prec@5 76.9400	
Best Prec@1: [47.520]	
Starting epoch number: 105 Learning rate: 0.1
Train: [105]	Time 15.540	Data 0.207	Loss 1.844	Prec@1 49.1180	Prec@5 80.9200	
Val: [105]	Time 1.311	Data 0.100	Loss 2.014	Prec@1 46.7800	Prec@5 78.4800	
Best Prec@1: [47.520]	
Starting epoch number: 106 Learning rate: 0.1
Train: [106]	Time 15.906	Data 0.206	Loss 1.847	Prec@1 48.7700	Prec@5 80.8640	
Val: [106]	Time 1.285	Data 0.101	Loss 2.113	Prec@1 44.7800	Prec@5 77.8700	
Best Prec@1: [47.520]	
Starting epoch number: 107 Learning rate: 0.1
Train: [107]	Time 16.067	Data 0.201	Loss 1.854	Prec@1 48.8840	Prec@5 80.6220	
Val: [107]	Time 1.338	Data 0.101	Loss 2.165	Prec@1 44.0500	Prec@5 75.8500	
Best Prec@1: [47.520]	
Starting epoch number: 108 Learning rate: 0.1
Train: [108]	Time 15.857	Data 0.199	Loss 1.841	Prec@1 49.2980	Prec@5 80.8980	
Val: [108]	Time 1.257	Data 0.094	Loss 2.151	Prec@1 44.1200	Prec@5 76.6800	
Best Prec@1: [47.520]	
Starting epoch number: 109 Learning rate: 0.1
Train: [109]	Time 15.937	Data 0.201	Loss 1.841	Prec@1 49.1000	Prec@5 80.7920	
Val: [109]	Time 1.326	Data 0.109	Loss 2.240	Prec@1 43.7800	Prec@5 75.8400	
Best Prec@1: [47.520]	
Starting epoch number: 110 Learning rate: 0.1
Train: [110]	Time 15.972	Data 0.216	Loss 1.845	Prec@1 49.1480	Prec@5 80.9140	
Val: [110]	Time 1.341	Data 0.104	Loss 2.061	Prec@1 45.9200	Prec@5 77.4700	
Best Prec@1: [47.520]	
Starting epoch number: 111 Learning rate: 0.1
Train: [111]	Time 15.886	Data 0.207	Loss 1.841	Prec@1 49.0520	Prec@5 80.8620	
Val: [111]	Time 1.270	Data 0.096	Loss 2.058	Prec@1 45.1800	Prec@5 77.1800	
Best Prec@1: [47.520]	
Starting epoch number: 112 Learning rate: 0.1
Train: [112]	Time 15.486	Data 0.204	Loss 1.836	Prec@1 48.9660	Prec@5 81.0580	
Val: [112]	Time 1.306	Data 0.102	Loss 2.013	Prec@1 46.8500	Prec@5 78.6300	
Best Prec@1: [47.520]	
Starting epoch number: 113 Learning rate: 0.1
Train: [113]	Time 15.546	Data 0.201	Loss 1.839	Prec@1 49.2300	Prec@5 80.8700	
Val: [113]	Time 1.246	Data 0.090	Loss 2.131	Prec@1 44.4900	Prec@5 76.2600	
Best Prec@1: [47.520]	
Starting epoch number: 114 Learning rate: 0.1
Train: [114]	Time 15.777	Data 0.208	Loss 1.839	Prec@1 49.0140	Prec@5 80.9220	
Val: [114]	Time 1.317	Data 0.104	Loss 2.169	Prec@1 44.5400	Prec@5 76.9700	
Best Prec@1: [47.520]	
Starting epoch number: 115 Learning rate: 0.1
Train: [115]	Time 16.134	Data 0.212	Loss 1.844	Prec@1 48.9220	Prec@5 80.8580	
Val: [115]	Time 1.285	Data 0.108	Loss 2.084	Prec@1 44.4900	Prec@5 77.9600	
Best Prec@1: [47.520]	
Starting epoch number: 116 Learning rate: 0.1
Train: [116]	Time 15.933	Data 0.217	Loss 1.840	Prec@1 49.1480	Prec@5 81.0060	
Val: [116]	Time 1.296	Data 0.092	Loss 2.032	Prec@1 46.2200	Prec@5 78.0000	
Best Prec@1: [47.520]	
Starting epoch number: 117 Learning rate: 0.1
Train: [117]	Time 16.532	Data 0.208	Loss 1.837	Prec@1 49.3080	Prec@5 80.9380	
Val: [117]	Time 1.313	Data 0.107	Loss 2.216	Prec@1 42.5600	Prec@5 75.2500	
Best Prec@1: [47.520]	
Starting epoch number: 118 Learning rate: 0.1
Train: [118]	Time 15.871	Data 0.201	Loss 1.841	Prec@1 49.1180	Prec@5 80.8220	
Val: [118]	Time 1.317	Data 0.102	Loss 2.188	Prec@1 43.0600	Prec@5 75.6500	
Best Prec@1: [47.520]	
Starting epoch number: 119 Learning rate: 0.1
Train: [119]	Time 15.955	Data 0.201	Loss 1.840	Prec@1 49.3100	Prec@5 80.6960	
Val: [119]	Time 1.243	Data 0.100	Loss 2.080	Prec@1 44.9000	Prec@5 77.6300	
Best Prec@1: [47.520]	
Starting epoch number: 120 Learning rate: 0.1
Train: [120]	Time 15.692	Data 0.204	Loss 1.839	Prec@1 49.1400	Prec@5 80.8540	
Val: [120]	Time 1.293	Data 0.099	Loss 1.969	Prec@1 46.6200	Prec@5 78.6300	
Best Prec@1: [47.520]	
Starting epoch number: 121 Learning rate: 0.1
Train: [121]	Time 16.038	Data 0.205	Loss 1.839	Prec@1 49.2340	Prec@5 80.8900	
Val: [121]	Time 1.303	Data 0.099	Loss 1.980	Prec@1 47.1700	Prec@5 78.8500	
Best Prec@1: [47.520]	
Starting epoch number: 122 Learning rate: 0.1
Train: [122]	Time 15.771	Data 0.203	Loss 1.832	Prec@1 49.3000	Prec@5 81.0200	
Val: [122]	Time 1.295	Data 0.099	Loss 2.006	Prec@1 46.1700	Prec@5 78.8300	
Best Prec@1: [47.520]	
Starting epoch number: 123 Learning rate: 0.1
Train: [123]	Time 15.554	Data 0.204	Loss 1.843	Prec@1 49.1680	Prec@5 80.9340	
Val: [123]	Time 1.303	Data 0.105	Loss 1.997	Prec@1 46.2500	Prec@5 78.5000	
Best Prec@1: [47.520]	
Starting epoch number: 124 Learning rate: 0.1
Train: [124]	Time 15.836	Data 0.201	Loss 1.837	Prec@1 49.3300	Prec@5 80.8440	
Val: [124]	Time 1.316	Data 0.109	Loss 2.255	Prec@1 42.4400	Prec@5 74.9800	
Best Prec@1: [47.520]	
Starting epoch number: 125 Learning rate: 0.1
Train: [125]	Time 15.705	Data 0.201	Loss 1.831	Prec@1 49.3200	Prec@5 81.0920	
Val: [125]	Time 1.392	Data 0.117	Loss 2.224	Prec@1 43.3100	Prec@5 75.4400	
Best Prec@1: [47.520]	
Starting epoch number: 126 Learning rate: 0.1
Train: [126]	Time 16.065	Data 0.202	Loss 1.836	Prec@1 49.2340	Prec@5 81.2140	
Val: [126]	Time 1.354	Data 0.096	Loss 1.980	Prec@1 47.1700	Prec@5 78.4600	
Best Prec@1: [47.520]	
Starting epoch number: 127 Learning rate: 0.1
Train: [127]	Time 15.746	Data 0.199	Loss 1.834	Prec@1 49.1020	Prec@5 81.0400	
Val: [127]	Time 1.281	Data 0.108	Loss 1.941	Prec@1 47.8300	Prec@5 80.1300	
Best Prec@1: [47.830]	
Starting epoch number: 128 Learning rate: 0.1
Train: [128]	Time 15.711	Data 0.219	Loss 1.828	Prec@1 49.8540	Prec@5 80.8940	
Val: [128]	Time 1.236	Data 0.097	Loss 2.357	Prec@1 41.1200	Prec@5 73.4500	
Best Prec@1: [47.830]	
Starting epoch number: 129 Learning rate: 0.1
Train: [129]	Time 15.857	Data 0.205	Loss 1.831	Prec@1 49.2280	Prec@5 81.2700	
Val: [129]	Time 1.287	Data 0.091	Loss 2.116	Prec@1 45.3000	Prec@5 77.7900	
Best Prec@1: [47.830]	
Starting epoch number: 130 Learning rate: 0.1
Train: [130]	Time 15.856	Data 0.204	Loss 1.831	Prec@1 49.2920	Prec@5 80.9440	
Val: [130]	Time 1.276	Data 0.098	Loss 2.197	Prec@1 43.9600	Prec@5 75.7500	
Best Prec@1: [47.830]	
Starting epoch number: 131 Learning rate: 0.1
Train: [131]	Time 15.685	Data 0.200	Loss 1.832	Prec@1 49.3660	Prec@5 81.1320	
Val: [131]	Time 1.263	Data 0.110	Loss 2.340	Prec@1 42.2900	Prec@5 74.2500	
Best Prec@1: [47.830]	
Starting epoch number: 132 Learning rate: 0.1
Train: [132]	Time 15.725	Data 0.206	Loss 1.834	Prec@1 49.4020	Prec@5 80.8800	
Val: [132]	Time 1.331	Data 0.108	Loss 1.921	Prec@1 47.8900	Prec@5 80.0300	
Best Prec@1: [47.890]	
Starting epoch number: 133 Learning rate: 0.1
Train: [133]	Time 16.038	Data 0.205	Loss 1.831	Prec@1 49.2420	Prec@5 80.8980	
Val: [133]	Time 1.265	Data 0.091	Loss 2.186	Prec@1 44.0300	Prec@5 75.7500	
Best Prec@1: [47.890]	
Starting epoch number: 134 Learning rate: 0.1
Train: [134]	Time 15.809	Data 0.205	Loss 1.825	Prec@1 49.6940	Prec@5 80.9460	
Val: [134]	Time 1.362	Data 0.114	Loss 2.332	Prec@1 42.3800	Prec@5 75.1000	
Best Prec@1: [47.890]	
Starting epoch number: 135 Learning rate: 0.1
Train: [135]	Time 16.090	Data 0.202	Loss 1.831	Prec@1 49.6960	Prec@5 80.9740	
Val: [135]	Time 1.257	Data 0.096	Loss 1.980	Prec@1 47.1000	Prec@5 79.2300	
Best Prec@1: [47.890]	
Starting epoch number: 136 Learning rate: 0.1
Train: [136]	Time 15.557	Data 0.207	Loss 1.830	Prec@1 49.2740	Prec@5 80.9360	
Val: [136]	Time 1.244	Data 0.097	Loss 2.243	Prec@1 43.4700	Prec@5 75.2300	
Best Prec@1: [47.890]	
Starting epoch number: 137 Learning rate: 0.1
Train: [137]	Time 15.815	Data 0.202	Loss 1.828	Prec@1 49.5500	Prec@5 81.2220	
Val: [137]	Time 1.251	Data 0.095	Loss 2.030	Prec@1 46.5400	Prec@5 77.9800	
Best Prec@1: [47.890]	
Starting epoch number: 138 Learning rate: 0.1
Train: [138]	Time 16.055	Data 0.202	Loss 1.823	Prec@1 49.5440	Prec@5 81.1440	
Val: [138]	Time 1.290	Data 0.102	Loss 2.015	Prec@1 46.0900	Prec@5 78.3600	
Best Prec@1: [47.890]	
Starting epoch number: 139 Learning rate: 0.1
Train: [139]	Time 15.481	Data 0.205	Loss 1.831	Prec@1 49.2900	Prec@5 81.2020	
Val: [139]	Time 1.335	Data 0.104	Loss 2.018	Prec@1 45.2100	Prec@5 78.2500	
Best Prec@1: [47.890]	
Starting epoch number: 140 Learning rate: 0.1
Train: [140]	Time 15.938	Data 0.201	Loss 1.823	Prec@1 49.4280	Prec@5 81.0780	
Val: [140]	Time 1.295	Data 0.107	Loss 1.951	Prec@1 47.4100	Prec@5 79.1500	
Best Prec@1: [47.890]	
Starting epoch number: 141 Learning rate: 0.1
Train: [141]	Time 15.632	Data 0.198	Loss 1.826	Prec@1 49.5620	Prec@5 81.0780	
Val: [141]	Time 1.301	Data 0.094	Loss 2.098	Prec@1 45.0000	Prec@5 76.7400	
Best Prec@1: [47.890]	
Starting epoch number: 142 Learning rate: 0.1
Train: [142]	Time 15.712	Data 0.208	Loss 1.825	Prec@1 49.3800	Prec@5 81.0940	
Val: [142]	Time 1.256	Data 0.099	Loss 1.989	Prec@1 46.9000	Prec@5 78.7900	
Best Prec@1: [47.890]	
Starting epoch number: 143 Learning rate: 0.1
Train: [143]	Time 15.824	Data 0.199	Loss 1.825	Prec@1 49.5800	Prec@5 81.2280	
Val: [143]	Time 1.232	Data 0.095	Loss 2.099	Prec@1 44.7400	Prec@5 76.9000	
Best Prec@1: [47.890]	
Starting epoch number: 144 Learning rate: 0.1
Train: [144]	Time 15.745	Data 0.212	Loss 1.818	Prec@1 49.5860	Prec@5 81.3400	
Val: [144]	Time 1.246	Data 0.093	Loss 1.942	Prec@1 47.5600	Prec@5 79.3900	
Best Prec@1: [47.890]	
Starting epoch number: 145 Learning rate: 0.1
Train: [145]	Time 15.638	Data 0.201	Loss 1.817	Prec@1 49.8340	Prec@5 81.1020	
Val: [145]	Time 1.341	Data 0.105	Loss 2.023	Prec@1 46.0000	Prec@5 78.6900	
Best Prec@1: [47.890]	
Starting epoch number: 146 Learning rate: 0.1
Train: [146]	Time 15.961	Data 0.212	Loss 1.823	Prec@1 49.4860	Prec@5 81.2660	
Val: [146]	Time 1.358	Data 0.117	Loss 2.098	Prec@1 45.2900	Prec@5 77.6200	
Best Prec@1: [47.890]	
Starting epoch number: 147 Learning rate: 0.1
Train: [147]	Time 16.033	Data 0.205	Loss 1.822	Prec@1 49.7120	Prec@5 81.2260	
Val: [147]	Time 1.254	Data 0.095	Loss 2.431	Prec@1 41.5400	Prec@5 73.1800	
Best Prec@1: [47.890]	
Starting epoch number: 148 Learning rate: 0.1
Train: [148]	Time 15.645	Data 0.205	Loss 1.825	Prec@1 49.5160	Prec@5 81.0280	
Val: [148]	Time 1.298	Data 0.099	Loss 2.245	Prec@1 43.7400	Prec@5 75.2700	
Best Prec@1: [47.890]	
Starting epoch number: 149 Learning rate: 0.1
Train: [149]	Time 15.931	Data 0.216	Loss 1.824	Prec@1 49.5280	Prec@5 81.2700	
Val: [149]	Time 1.256	Data 0.093	Loss 1.973	Prec@1 46.9300	Prec@5 78.6500	
Best Prec@1: [47.890]	
Starting epoch number: 150 Learning rate: 0.010000000000000002
Train: [150]	Time 16.352	Data 0.207	Loss 1.574	Prec@1 56.0620	Prec@5 85.3920	
Val: [150]	Time 1.295	Data 0.101	Loss 1.611	Prec@1 55.3800	Prec@5 84.9800	
Best Prec@1: [55.380]	
Starting epoch number: 151 Learning rate: 0.010000000000000002
Train: [151]	Time 15.880	Data 0.202	Loss 1.514	Prec@1 57.5620	Prec@5 86.1600	
Val: [151]	Time 1.335	Data 0.100	Loss 1.582	Prec@1 56.5200	Prec@5 85.1700	
Best Prec@1: [56.520]	
Starting epoch number: 152 Learning rate: 0.010000000000000002
Train: [152]	Time 15.740	Data 0.212	Loss 1.497	Prec@1 58.0640	Prec@5 86.4780	
Val: [152]	Time 1.302	Data 0.097	Loss 1.610	Prec@1 55.7100	Prec@5 85.1500	
Best Prec@1: [56.520]	
Starting epoch number: 153 Learning rate: 0.010000000000000002
Train: [153]	Time 15.569	Data 0.211	Loss 1.496	Prec@1 57.8860	Prec@5 86.4900	
Val: [153]	Time 1.279	Data 0.098	Loss 1.600	Prec@1 55.9200	Prec@5 85.0400	
Best Prec@1: [56.520]	
Starting epoch number: 154 Learning rate: 0.010000000000000002
Train: [154]	Time 15.918	Data 0.202	Loss 1.482	Prec@1 58.2160	Prec@5 86.7340	
Val: [154]	Time 1.315	Data 0.111	Loss 1.592	Prec@1 55.7600	Prec@5 85.2100	
Best Prec@1: [56.520]	
Starting epoch number: 155 Learning rate: 0.010000000000000002
Train: [155]	Time 15.768	Data 0.205	Loss 1.476	Prec@1 58.2800	Prec@5 86.8160	
Val: [155]	Time 1.281	Data 0.103	Loss 1.585	Prec@1 56.4800	Prec@5 85.2900	
Best Prec@1: [56.520]	
Starting epoch number: 156 Learning rate: 0.010000000000000002
Train: [156]	Time 15.548	Data 0.204	Loss 1.475	Prec@1 58.3360	Prec@5 86.6880	
Val: [156]	Time 1.273	Data 0.092	Loss 1.602	Prec@1 56.4200	Prec@5 85.2100	
Best Prec@1: [56.520]	
Starting epoch number: 157 Learning rate: 0.010000000000000002
Train: [157]	Time 15.570	Data 0.202	Loss 1.467	Prec@1 58.3220	Prec@5 86.8740	
Val: [157]	Time 1.309	Data 0.096	Loss 1.586	Prec@1 56.1000	Prec@5 85.3800	
Best Prec@1: [56.520]	
Starting epoch number: 158 Learning rate: 0.010000000000000002
Train: [158]	Time 15.650	Data 0.204	Loss 1.464	Prec@1 58.5400	Prec@5 86.9500	
Val: [158]	Time 1.359	Data 0.100	Loss 1.594	Prec@1 56.3600	Prec@5 85.1700	
Best Prec@1: [56.520]	
Starting epoch number: 159 Learning rate: 0.010000000000000002
Train: [159]	Time 15.585	Data 0.203	Loss 1.451	Prec@1 59.0080	Prec@5 87.1700	
Val: [159]	Time 1.300	Data 0.095	Loss 1.584	Prec@1 56.3800	Prec@5 85.3400	
Best Prec@1: [56.520]	
Starting epoch number: 160 Learning rate: 0.010000000000000002
Train: [160]	Time 15.742	Data 0.210	Loss 1.460	Prec@1 58.6360	Prec@5 86.9400	
Val: [160]	Time 1.308	Data 0.094	Loss 1.583	Prec@1 56.5500	Prec@5 85.1400	
Best Prec@1: [56.550]	
Starting epoch number: 161 Learning rate: 0.010000000000000002
Train: [161]	Time 15.878	Data 0.211	Loss 1.458	Prec@1 58.7940	Prec@5 87.1320	
Val: [161]	Time 1.253	Data 0.093	Loss 1.608	Prec@1 56.1200	Prec@5 85.0900	
Best Prec@1: [56.550]	
Starting epoch number: 162 Learning rate: 0.010000000000000002
Train: [162]	Time 16.018	Data 0.202	Loss 1.452	Prec@1 58.9800	Prec@5 87.0340	
Val: [162]	Time 1.294	Data 0.106	Loss 1.595	Prec@1 56.2900	Prec@5 85.2900	
Best Prec@1: [56.550]	
Starting epoch number: 163 Learning rate: 0.010000000000000002
Train: [163]	Time 15.694	Data 0.216	Loss 1.454	Prec@1 58.8840	Prec@5 87.2200	
Val: [163]	Time 1.268	Data 0.096	Loss 1.593	Prec@1 56.0100	Prec@5 85.3200	
Best Prec@1: [56.550]	
Starting epoch number: 164 Learning rate: 0.010000000000000002
Train: [164]	Time 16.135	Data 0.211	Loss 1.452	Prec@1 58.8540	Prec@5 87.0300	
Val: [164]	Time 1.293	Data 0.098	Loss 1.606	Prec@1 56.2700	Prec@5 85.2200	
Best Prec@1: [56.550]	
Starting epoch number: 165 Learning rate: 0.010000000000000002
Train: [165]	Time 15.941	Data 0.210	Loss 1.447	Prec@1 58.9940	Prec@5 87.2680	
Val: [165]	Time 1.303	Data 0.094	Loss 1.605	Prec@1 55.8500	Prec@5 85.4000	
Best Prec@1: [56.550]	
Starting epoch number: 166 Learning rate: 0.010000000000000002
Train: [166]	Time 15.753	Data 0.207	Loss 1.448	Prec@1 59.0000	Prec@5 87.2160	
Val: [166]	Time 1.297	Data 0.096	Loss 1.596	Prec@1 56.4000	Prec@5 85.3000	
Best Prec@1: [56.550]	
Starting epoch number: 167 Learning rate: 0.010000000000000002
Train: [167]	Time 16.201	Data 0.206	Loss 1.449	Prec@1 58.9680	Prec@5 87.1660	
Val: [167]	Time 1.298	Data 0.114	Loss 1.624	Prec@1 56.2200	Prec@5 85.3400	
Best Prec@1: [56.550]	
Starting epoch number: 168 Learning rate: 0.010000000000000002
Train: [168]	Time 16.111	Data 0.205	Loss 1.446	Prec@1 58.9640	Prec@5 87.2440	
Val: [168]	Time 1.229	Data 0.095	Loss 1.607	Prec@1 55.8100	Prec@5 84.9700	
Best Prec@1: [56.550]	
Starting epoch number: 169 Learning rate: 0.010000000000000002
Train: [169]	Time 16.525	Data 0.216	Loss 1.449	Prec@1 58.8620	Prec@5 86.9420	
Val: [169]	Time 1.263	Data 0.110	Loss 1.585	Prec@1 56.9000	Prec@5 85.3200	
Best Prec@1: [56.900]	
Starting epoch number: 170 Learning rate: 0.010000000000000002
Train: [170]	Time 16.094	Data 0.212	Loss 1.443	Prec@1 59.0520	Prec@5 87.2780	
Val: [170]	Time 1.312	Data 0.103	Loss 1.609	Prec@1 56.0900	Prec@5 84.9600	
Best Prec@1: [56.900]	
Starting epoch number: 171 Learning rate: 0.010000000000000002
Train: [171]	Time 16.327	Data 0.215	Loss 1.444	Prec@1 59.0520	Prec@5 87.2380	
Val: [171]	Time 1.295	Data 0.104	Loss 1.585	Prec@1 56.3900	Prec@5 85.1900	
Best Prec@1: [56.900]	
Starting epoch number: 172 Learning rate: 0.010000000000000002
Train: [172]	Time 15.762	Data 0.203	Loss 1.440	Prec@1 59.1400	Prec@5 87.4300	
Val: [172]	Time 1.286	Data 0.101	Loss 1.586	Prec@1 56.1400	Prec@5 85.3300	
Best Prec@1: [56.900]	
Starting epoch number: 173 Learning rate: 0.010000000000000002
Train: [173]	Time 15.825	Data 0.202	Loss 1.441	Prec@1 58.8860	Prec@5 87.1940	
Val: [173]	Time 1.282	Data 0.102	Loss 1.589	Prec@1 56.6400	Prec@5 85.3800	
Best Prec@1: [56.900]	
Starting epoch number: 174 Learning rate: 0.010000000000000002
Train: [174]	Time 15.911	Data 0.208	Loss 1.444	Prec@1 59.0260	Prec@5 87.2840	
Val: [174]	Time 1.383	Data 0.111	Loss 1.610	Prec@1 56.6700	Prec@5 85.1700	
Best Prec@1: [56.900]	
Starting epoch number: 175 Learning rate: 0.010000000000000002
Train: [175]	Time 15.820	Data 0.211	Loss 1.441	Prec@1 58.9780	Prec@5 87.1800	
Val: [175]	Time 1.232	Data 0.093	Loss 1.621	Prec@1 56.4200	Prec@5 84.9600	
Best Prec@1: [56.900]	
Starting epoch number: 176 Learning rate: 0.010000000000000002
Train: [176]	Time 15.771	Data 0.202	Loss 1.441	Prec@1 59.2080	Prec@5 87.3420	
Val: [176]	Time 1.319	Data 0.098	Loss 1.601	Prec@1 55.8800	Prec@5 85.1000	
Best Prec@1: [56.900]	
Starting epoch number: 177 Learning rate: 0.010000000000000002
Train: [177]	Time 15.759	Data 0.202	Loss 1.441	Prec@1 59.2220	Prec@5 87.3260	
Val: [177]	Time 1.299	Data 0.108	Loss 1.577	Prec@1 56.5800	Prec@5 85.0400	
Best Prec@1: [56.900]	
Starting epoch number: 178 Learning rate: 0.010000000000000002
Train: [178]	Time 15.740	Data 0.202	Loss 1.441	Prec@1 59.2560	Prec@5 87.2200	
Val: [178]	Time 1.409	Data 0.111	Loss 1.559	Prec@1 57.1400	Prec@5 85.4400	
Best Prec@1: [57.140]	
Starting epoch number: 179 Learning rate: 0.010000000000000002
Train: [179]	Time 16.088	Data 0.210	Loss 1.443	Prec@1 58.9840	Prec@5 87.4020	
Val: [179]	Time 1.291	Data 0.112	Loss 1.596	Prec@1 56.5400	Prec@5 85.3800	
Best Prec@1: [57.140]	
Starting epoch number: 180 Learning rate: 0.010000000000000002
Train: [180]	Time 15.631	Data 0.203	Loss 1.439	Prec@1 59.2600	Prec@5 87.1600	
Val: [180]	Time 1.255	Data 0.100	Loss 1.596	Prec@1 56.6000	Prec@5 85.3300	
Best Prec@1: [57.140]	
Starting epoch number: 181 Learning rate: 0.010000000000000002
Train: [181]	Time 15.650	Data 0.199	Loss 1.444	Prec@1 59.0020	Prec@5 87.1220	
Val: [181]	Time 1.320	Data 0.111	Loss 1.670	Prec@1 54.9200	Prec@5 84.0800	
Best Prec@1: [57.140]	
Starting epoch number: 182 Learning rate: 0.010000000000000002
Train: [182]	Time 16.280	Data 0.214	Loss 1.446	Prec@1 59.0280	Prec@5 87.1660	
Val: [182]	Time 1.353	Data 0.094	Loss 1.573	Prec@1 56.5300	Prec@5 85.3000	
Best Prec@1: [57.140]	
Starting epoch number: 183 Learning rate: 0.010000000000000002
Train: [183]	Time 16.060	Data 0.215	Loss 1.439	Prec@1 59.0620	Prec@5 87.3960	
Val: [183]	Time 1.346	Data 0.102	Loss 1.639	Prec@1 55.2900	Prec@5 84.5300	
Best Prec@1: [57.140]	
Starting epoch number: 184 Learning rate: 0.010000000000000002
Train: [184]	Time 16.145	Data 0.203	Loss 1.441	Prec@1 59.1000	Prec@5 87.2780	
Val: [184]	Time 1.290	Data 0.092	Loss 1.645	Prec@1 55.9100	Prec@5 84.6600	
Best Prec@1: [57.140]	
Starting epoch number: 185 Learning rate: 0.010000000000000002
Train: [185]	Time 15.868	Data 0.209	Loss 1.439	Prec@1 59.4820	Prec@5 87.3300	
Val: [185]	Time 1.291	Data 0.096	Loss 1.641	Prec@1 55.5100	Prec@5 84.8500	
Best Prec@1: [57.140]	
Starting epoch number: 186 Learning rate: 0.010000000000000002
Train: [186]	Time 15.980	Data 0.211	Loss 1.439	Prec@1 59.2120	Prec@5 87.3680	
Val: [186]	Time 1.339	Data 0.111	Loss 1.626	Prec@1 56.0300	Prec@5 84.4900	
Best Prec@1: [57.140]	
Starting epoch number: 187 Learning rate: 0.010000000000000002
Train: [187]	Time 15.972	Data 0.212	Loss 1.442	Prec@1 58.8900	Prec@5 87.2540	
Val: [187]	Time 1.289	Data 0.090	Loss 1.598	Prec@1 56.1700	Prec@5 85.3000	
Best Prec@1: [57.140]	
Starting epoch number: 188 Learning rate: 0.010000000000000002
Train: [188]	Time 15.738	Data 0.206	Loss 1.438	Prec@1 59.3360	Prec@5 87.3300	
Val: [188]	Time 1.272	Data 0.096	Loss 1.618	Prec@1 56.1000	Prec@5 85.2100	
Best Prec@1: [57.140]	
Starting epoch number: 189 Learning rate: 0.010000000000000002
Train: [189]	Time 16.034	Data 0.201	Loss 1.444	Prec@1 59.0260	Prec@5 87.1980	
Val: [189]	Time 1.262	Data 0.095	Loss 1.638	Prec@1 55.8100	Prec@5 84.9400	
Best Prec@1: [57.140]	
Starting epoch number: 190 Learning rate: 0.010000000000000002
Train: [190]	Time 15.745	Data 0.205	Loss 1.439	Prec@1 59.1320	Prec@5 87.2380	
Val: [190]	Time 1.376	Data 0.108	Loss 1.595	Prec@1 56.3300	Prec@5 85.4100	
Best Prec@1: [57.140]	
Starting epoch number: 191 Learning rate: 0.010000000000000002
Train: [191]	Time 16.045	Data 0.202	Loss 1.441	Prec@1 58.9480	Prec@5 87.3760	
Val: [191]	Time 1.253	Data 0.098	Loss 1.564	Prec@1 56.4700	Prec@5 85.3900	
Best Prec@1: [57.140]	
Starting epoch number: 192 Learning rate: 0.010000000000000002
Train: [192]	Time 16.196	Data 0.215	Loss 1.444	Prec@1 59.0980	Prec@5 87.2300	
Val: [192]	Time 1.362	Data 0.096	Loss 1.606	Prec@1 55.4400	Prec@5 84.8800	
Best Prec@1: [57.140]	
Starting epoch number: 193 Learning rate: 0.010000000000000002
Train: [193]	Time 15.785	Data 0.207	Loss 1.441	Prec@1 59.1220	Prec@5 87.3300	
Val: [193]	Time 1.248	Data 0.100	Loss 1.627	Prec@1 55.3700	Prec@5 85.0200	
Best Prec@1: [57.140]	
Starting epoch number: 194 Learning rate: 0.010000000000000002
Train: [194]	Time 15.855	Data 0.208	Loss 1.444	Prec@1 58.9860	Prec@5 87.1820	
Val: [194]	Time 1.352	Data 0.107	Loss 1.582	Prec@1 56.4500	Prec@5 85.5400	
Best Prec@1: [57.140]	
Starting epoch number: 195 Learning rate: 0.010000000000000002
Train: [195]	Time 15.968	Data 0.200	Loss 1.442	Prec@1 59.0060	Prec@5 87.2820	
Val: [195]	Time 1.275	Data 0.091	Loss 1.626	Prec@1 55.4000	Prec@5 85.0300	
Best Prec@1: [57.140]	
Starting epoch number: 196 Learning rate: 0.010000000000000002
Train: [196]	Time 15.663	Data 0.200	Loss 1.439	Prec@1 59.0060	Prec@5 87.4140	
Val: [196]	Time 1.272	Data 0.094	Loss 1.619	Prec@1 56.0500	Prec@5 84.8900	
Best Prec@1: [57.140]	
Starting epoch number: 197 Learning rate: 0.010000000000000002
Train: [197]	Time 15.973	Data 0.206	Loss 1.438	Prec@1 59.2320	Prec@5 87.3020	
Val: [197]	Time 1.307	Data 0.095	Loss 1.647	Prec@1 54.8900	Prec@5 84.5800	
Best Prec@1: [57.140]	
Starting epoch number: 198 Learning rate: 0.010000000000000002
Train: [198]	Time 16.296	Data 0.203	Loss 1.441	Prec@1 58.9380	Prec@5 87.3700	
Val: [198]	Time 1.346	Data 0.107	Loss 1.585	Prec@1 56.2000	Prec@5 85.4800	
Best Prec@1: [57.140]	
Starting epoch number: 199 Learning rate: 0.010000000000000002
Train: [199]	Time 15.852	Data 0.200	Loss 1.440	Prec@1 59.3100	Prec@5 87.3780	
Val: [199]	Time 1.254	Data 0.089	Loss 1.631	Prec@1 55.5600	Prec@5 84.7600	
Best Prec@1: [57.140]	
Starting epoch number: 200 Learning rate: 0.010000000000000002
Train: [200]	Time 15.791	Data 0.211	Loss 1.446	Prec@1 59.0420	Prec@5 87.3580	
Val: [200]	Time 1.289	Data 0.102	Loss 1.599	Prec@1 55.9000	Prec@5 85.0800	
Best Prec@1: [57.140]	
Starting epoch number: 201 Learning rate: 0.010000000000000002
Train: [201]	Time 16.100	Data 0.202	Loss 1.436	Prec@1 59.1060	Prec@5 87.3800	
Val: [201]	Time 1.297	Data 0.102	Loss 1.642	Prec@1 54.6900	Prec@5 84.4900	
Best Prec@1: [57.140]	
Starting epoch number: 202 Learning rate: 0.010000000000000002
Train: [202]	Time 16.185	Data 0.203	Loss 1.448	Prec@1 59.0220	Prec@5 87.1820	
Val: [202]	Time 1.284	Data 0.099	Loss 1.623	Prec@1 55.5200	Prec@5 84.5100	
Best Prec@1: [57.140]	
Starting epoch number: 203 Learning rate: 0.010000000000000002
Train: [203]	Time 15.896	Data 0.211	Loss 1.441	Prec@1 59.1080	Prec@5 87.0900	
Val: [203]	Time 1.279	Data 0.104	Loss 1.577	Prec@1 56.6600	Prec@5 85.5700	
Best Prec@1: [57.140]	
Starting epoch number: 204 Learning rate: 0.010000000000000002
Train: [204]	Time 15.517	Data 0.203	Loss 1.442	Prec@1 59.0280	Prec@5 87.1520	
Val: [204]	Time 1.313	Data 0.098	Loss 1.597	Prec@1 55.7300	Prec@5 85.1500	
Best Prec@1: [57.140]	
Starting epoch number: 205 Learning rate: 0.010000000000000002
Train: [205]	Time 15.870	Data 0.208	Loss 1.440	Prec@1 58.8860	Prec@5 87.4820	
Val: [205]	Time 1.261	Data 0.103	Loss 1.645	Prec@1 55.0700	Prec@5 84.8300	
Best Prec@1: [57.140]	
Starting epoch number: 206 Learning rate: 0.010000000000000002
Train: [206]	Time 15.586	Data 0.200	Loss 1.447	Prec@1 59.1200	Prec@5 87.0960	
Val: [206]	Time 1.247	Data 0.096	Loss 1.592	Prec@1 56.1000	Prec@5 85.2900	
Best Prec@1: [57.140]	
Starting epoch number: 207 Learning rate: 0.010000000000000002
Train: [207]	Time 15.891	Data 0.208	Loss 1.446	Prec@1 58.8700	Prec@5 87.3220	
Val: [207]	Time 1.310	Data 0.112	Loss 1.621	Prec@1 55.2900	Prec@5 84.8400	
Best Prec@1: [57.140]	
Starting epoch number: 208 Learning rate: 0.010000000000000002
Train: [208]	Time 15.897	Data 0.206	Loss 1.444	Prec@1 59.1100	Prec@5 87.2440	
Val: [208]	Time 1.323	Data 0.100	Loss 1.665	Prec@1 55.1200	Prec@5 84.4200	
Best Prec@1: [57.140]	
Starting epoch number: 209 Learning rate: 0.010000000000000002
Train: [209]	Time 15.773	Data 0.207	Loss 1.443	Prec@1 58.8720	Prec@5 87.3560	
Val: [209]	Time 1.265	Data 0.091	Loss 1.617	Prec@1 55.5900	Prec@5 84.8800	
Best Prec@1: [57.140]	
Starting epoch number: 210 Learning rate: 0.010000000000000002
Train: [210]	Time 15.795	Data 0.210	Loss 1.445	Prec@1 59.1020	Prec@5 87.2000	
Val: [210]	Time 1.270	Data 0.105	Loss 1.627	Prec@1 55.8200	Prec@5 84.8500	
Best Prec@1: [57.140]	
Starting epoch number: 211 Learning rate: 0.010000000000000002
Train: [211]	Time 15.819	Data 0.209	Loss 1.449	Prec@1 58.7300	Prec@5 87.1580	
Val: [211]	Time 1.291	Data 0.100	Loss 1.697	Prec@1 55.2900	Prec@5 83.9900	
Best Prec@1: [57.140]	
Starting epoch number: 212 Learning rate: 0.010000000000000002
Train: [212]	Time 15.778	Data 0.206	Loss 1.448	Prec@1 58.8540	Prec@5 87.2460	
Val: [212]	Time 1.343	Data 0.118	Loss 1.657	Prec@1 55.3800	Prec@5 84.2000	
Best Prec@1: [57.140]	
Starting epoch number: 213 Learning rate: 0.010000000000000002
Train: [213]	Time 15.772	Data 0.208	Loss 1.444	Prec@1 58.7860	Prec@5 87.3060	
Val: [213]	Time 1.296	Data 0.102	Loss 1.667	Prec@1 54.4200	Prec@5 84.1100	
Best Prec@1: [57.140]	
Starting epoch number: 214 Learning rate: 0.010000000000000002
Train: [214]	Time 15.706	Data 0.211	Loss 1.446	Prec@1 58.8600	Prec@5 87.1580	
Val: [214]	Time 1.279	Data 0.102	Loss 1.629	Prec@1 55.2000	Prec@5 84.7400	
Best Prec@1: [57.140]	
Starting epoch number: 215 Learning rate: 0.010000000000000002
Train: [215]	Time 15.551	Data 0.198	Loss 1.445	Prec@1 59.1780	Prec@5 87.2100	
Val: [215]	Time 1.278	Data 0.103	Loss 1.651	Prec@1 54.9300	Prec@5 84.5900	
Best Prec@1: [57.140]	
Starting epoch number: 216 Learning rate: 0.010000000000000002
Train: [216]	Time 15.799	Data 0.200	Loss 1.443	Prec@1 58.9540	Prec@5 87.2700	
Val: [216]	Time 1.288	Data 0.096	Loss 1.663	Prec@1 55.3800	Prec@5 84.0500	
Best Prec@1: [57.140]	
Starting epoch number: 217 Learning rate: 0.010000000000000002
Train: [217]	Time 15.704	Data 0.210	Loss 1.447	Prec@1 58.7280	Prec@5 87.2160	
Val: [217]	Time 1.278	Data 0.098	Loss 1.661	Prec@1 55.6100	Prec@5 84.2400	
Best Prec@1: [57.140]	
Starting epoch number: 218 Learning rate: 0.010000000000000002
Train: [218]	Time 15.929	Data 0.201	Loss 1.446	Prec@1 58.8380	Prec@5 87.2360	
Val: [218]	Time 1.296	Data 0.096	Loss 1.630	Prec@1 55.3900	Prec@5 85.0600	
Best Prec@1: [57.140]	
Starting epoch number: 219 Learning rate: 0.010000000000000002
Train: [219]	Time 15.773	Data 0.214	Loss 1.448	Prec@1 58.5960	Prec@5 87.3480	
Val: [219]	Time 1.354	Data 0.106	Loss 1.638	Prec@1 55.4400	Prec@5 84.4000	
Best Prec@1: [57.140]	
Starting epoch number: 220 Learning rate: 0.010000000000000002
Train: [220]	Time 15.833	Data 0.205	Loss 1.443	Prec@1 58.6340	Prec@5 87.2360	
Val: [220]	Time 1.324	Data 0.107	Loss 1.633	Prec@1 55.4600	Prec@5 84.9200	
Best Prec@1: [57.140]	
Starting epoch number: 221 Learning rate: 0.010000000000000002
Train: [221]	Time 15.654	Data 0.205	Loss 1.450	Prec@1 58.9540	Prec@5 87.2140	
Val: [221]	Time 1.296	Data 0.098	Loss 1.668	Prec@1 54.2400	Prec@5 84.5000	
Best Prec@1: [57.140]	
Starting epoch number: 222 Learning rate: 0.010000000000000002
Train: [222]	Time 16.142	Data 0.203	Loss 1.445	Prec@1 58.9940	Prec@5 87.4140	
Val: [222]	Time 1.341	Data 0.101	Loss 1.669	Prec@1 54.6000	Prec@5 84.8000	
Best Prec@1: [57.140]	
Starting epoch number: 223 Learning rate: 0.010000000000000002
Train: [223]	Time 16.281	Data 0.214	Loss 1.444	Prec@1 58.8760	Prec@5 87.2180	
Val: [223]	Time 1.277	Data 0.090	Loss 1.682	Prec@1 55.1600	Prec@5 84.3400	
Best Prec@1: [57.140]	
Starting epoch number: 224 Learning rate: 0.010000000000000002
Train: [224]	Time 15.961	Data 0.204	Loss 1.444	Prec@1 58.7540	Prec@5 87.3440	
Val: [224]	Time 1.292	Data 0.100	Loss 1.662	Prec@1 55.4800	Prec@5 84.3300	
Best Prec@1: [57.140]	
Starting epoch number: 225 Learning rate: 0.0010000000000000002
Train: [225]	Time 15.454	Data 0.208	Loss 1.362	Prec@1 61.2340	Prec@5 88.4340	
Val: [225]	Time 1.307	Data 0.102	Loss 1.517	Prec@1 58.2200	Prec@5 86.3800	
Best Prec@1: [58.220]	
Starting epoch number: 226 Learning rate: 0.0010000000000000002
Train: [226]	Time 15.728	Data 0.199	Loss 1.346	Prec@1 61.7900	Prec@5 88.6960	
Val: [226]	Time 1.318	Data 0.100	Loss 1.523	Prec@1 58.1300	Prec@5 86.1100	
Best Prec@1: [58.220]	
Starting epoch number: 227 Learning rate: 0.0010000000000000002
Train: [227]	Time 15.461	Data 0.203	Loss 1.344	Prec@1 61.5400	Prec@5 88.7480	
Val: [227]	Time 1.309	Data 0.117	Loss 1.508	Prec@1 58.5000	Prec@5 86.3700	
Best Prec@1: [58.500]	
Starting epoch number: 228 Learning rate: 0.0010000000000000002
Train: [228]	Time 16.161	Data 0.199	Loss 1.342	Prec@1 61.7960	Prec@5 88.7140	
Val: [228]	Time 1.264	Data 0.101	Loss 1.515	Prec@1 58.2700	Prec@5 86.3200	
Best Prec@1: [58.500]	
Starting epoch number: 229 Learning rate: 0.0010000000000000002
Train: [229]	Time 15.724	Data 0.201	Loss 1.334	Prec@1 61.9260	Prec@5 88.8200	
Val: [229]	Time 1.247	Data 0.097	Loss 1.519	Prec@1 58.4000	Prec@5 86.3400	
Best Prec@1: [58.500]	
Starting epoch number: 230 Learning rate: 0.0010000000000000002
Train: [230]	Time 15.979	Data 0.205	Loss 1.332	Prec@1 61.8860	Prec@5 88.8540	
Val: [230]	Time 1.327	Data 0.105	Loss 1.506	Prec@1 58.6300	Prec@5 86.4700	
Best Prec@1: [58.630]	
Starting epoch number: 231 Learning rate: 0.0010000000000000002
Train: [231]	Time 15.868	Data 0.205	Loss 1.333	Prec@1 62.1520	Prec@5 88.8780	
Val: [231]	Time 1.226	Data 0.094	Loss 1.515	Prec@1 58.7300	Prec@5 86.4500	
Best Prec@1: [58.730]	
Starting epoch number: 232 Learning rate: 0.0010000000000000002
Train: [232]	Time 15.999	Data 0.203	Loss 1.331	Prec@1 62.1660	Prec@5 88.9620	
Val: [232]	Time 1.304	Data 0.103	Loss 1.529	Prec@1 57.9200	Prec@5 86.4800	
Best Prec@1: [58.730]	
Starting epoch number: 233 Learning rate: 0.0010000000000000002
Train: [233]	Time 15.638	Data 0.203	Loss 1.330	Prec@1 62.1740	Prec@5 89.0360	
Val: [233]	Time 1.306	Data 0.108	Loss 1.522	Prec@1 58.2800	Prec@5 86.3800	
Best Prec@1: [58.730]	
Starting epoch number: 234 Learning rate: 0.0010000000000000002
Train: [234]	Time 15.763	Data 0.208	Loss 1.326	Prec@1 62.1320	Prec@5 89.0400	
Val: [234]	Time 1.298	Data 0.095	Loss 1.506	Prec@1 58.4800	Prec@5 86.4800	
Best Prec@1: [58.730]	
Starting epoch number: 235 Learning rate: 0.0010000000000000002
Train: [235]	Time 15.660	Data 0.209	Loss 1.327	Prec@1 62.1140	Prec@5 89.0800	
Val: [235]	Time 1.335	Data 0.101	Loss 1.514	Prec@1 58.2400	Prec@5 86.6100	
Best Prec@1: [58.730]	
Starting epoch number: 236 Learning rate: 0.0010000000000000002
Train: [236]	Time 15.731	Data 0.216	Loss 1.329	Prec@1 62.0840	Prec@5 88.9260	
Val: [236]	Time 1.294	Data 0.102	Loss 1.509	Prec@1 58.5500	Prec@5 86.3000	
Best Prec@1: [58.730]	
Starting epoch number: 237 Learning rate: 0.0010000000000000002
Train: [237]	Time 15.497	Data 0.198	Loss 1.325	Prec@1 62.0500	Prec@5 88.8900	
Val: [237]	Time 1.241	Data 0.102	Loss 1.520	Prec@1 58.3000	Prec@5 86.3500	
Best Prec@1: [58.730]	
Starting epoch number: 238 Learning rate: 0.0010000000000000002
Train: [238]	Time 15.683	Data 0.208	Loss 1.328	Prec@1 62.1880	Prec@5 88.9700	
Val: [238]	Time 1.318	Data 0.111	Loss 1.516	Prec@1 58.3700	Prec@5 86.3800	
Best Prec@1: [58.730]	
Starting epoch number: 239 Learning rate: 0.0010000000000000002
Train: [239]	Time 15.646	Data 0.200	Loss 1.326	Prec@1 61.9040	Prec@5 88.9160	
Val: [239]	Time 1.296	Data 0.100	Loss 1.509	Prec@1 58.5900	Prec@5 86.6200	
Best Prec@1: [58.730]	
Starting epoch number: 240 Learning rate: 0.0010000000000000002
Train: [240]	Time 15.808	Data 0.208	Loss 1.329	Prec@1 62.0300	Prec@5 88.7540	
Val: [240]	Time 1.236	Data 0.094	Loss 1.504	Prec@1 58.4100	Prec@5 86.6400	
Best Prec@1: [58.730]	
Starting epoch number: 241 Learning rate: 0.0010000000000000002
Train: [241]	Time 16.119	Data 0.209	Loss 1.322	Prec@1 62.2720	Prec@5 89.0420	
Val: [241]	Time 1.255	Data 0.098	Loss 1.514	Prec@1 58.3500	Prec@5 86.4000	
Best Prec@1: [58.730]	
Starting epoch number: 242 Learning rate: 0.0010000000000000002
Train: [242]	Time 15.664	Data 0.198	Loss 1.323	Prec@1 62.1920	Prec@5 89.0080	
Val: [242]	Time 1.321	Data 0.109	Loss 1.508	Prec@1 58.4000	Prec@5 86.4400	
Best Prec@1: [58.730]	
Starting epoch number: 243 Learning rate: 0.0010000000000000002
Train: [243]	Time 15.660	Data 0.208	Loss 1.326	Prec@1 62.1940	Prec@5 89.0260	
Val: [243]	Time 1.361	Data 0.110	Loss 1.523	Prec@1 57.9900	Prec@5 86.4300	
Best Prec@1: [58.730]	
Starting epoch number: 244 Learning rate: 0.0010000000000000002
Train: [244]	Time 15.784	Data 0.199	Loss 1.325	Prec@1 62.2660	Prec@5 89.0180	
Val: [244]	Time 1.307	Data 0.104	Loss 1.510	Prec@1 58.6500	Prec@5 86.6100	
Best Prec@1: [58.730]	
Starting epoch number: 245 Learning rate: 0.0010000000000000002
Train: [245]	Time 15.966	Data 0.210	Loss 1.325	Prec@1 62.1500	Prec@5 88.9820	
Val: [245]	Time 1.266	Data 0.097	Loss 1.510	Prec@1 58.2700	Prec@5 86.4200	
Best Prec@1: [58.730]	
Starting epoch number: 246 Learning rate: 0.0010000000000000002
Train: [246]	Time 16.360	Data 0.201	Loss 1.324	Prec@1 62.2780	Prec@5 88.8820	
Val: [246]	Time 1.287	Data 0.107	Loss 1.511	Prec@1 58.3900	Prec@5 86.5000	
Best Prec@1: [58.730]	
Starting epoch number: 247 Learning rate: 0.0010000000000000002
Train: [247]	Time 15.870	Data 0.207	Loss 1.318	Prec@1 62.4760	Prec@5 89.1240	
Val: [247]	Time 1.308	Data 0.100	Loss 1.510	Prec@1 58.7300	Prec@5 86.4100	
Best Prec@1: [58.730]	
Starting epoch number: 248 Learning rate: 0.0010000000000000002
Train: [248]	Time 15.749	Data 0.199	Loss 1.324	Prec@1 62.2360	Prec@5 88.9520	
Val: [248]	Time 1.380	Data 0.103	Loss 1.519	Prec@1 58.2000	Prec@5 86.4200	
Best Prec@1: [58.730]	
Starting epoch number: 249 Learning rate: 0.0010000000000000002
Train: [249]	Time 16.054	Data 0.207	Loss 1.323	Prec@1 62.2340	Prec@5 88.9160	
Val: [249]	Time 1.257	Data 0.094	Loss 1.512	Prec@1 58.4000	Prec@5 86.4700	
Best Prec@1: [58.730]	
Starting epoch number: 250 Learning rate: 0.0010000000000000002
Train: [250]	Time 15.694	Data 0.199	Loss 1.323	Prec@1 62.1920	Prec@5 89.0100	
Val: [250]	Time 1.371	Data 0.107	Loss 1.523	Prec@1 58.1100	Prec@5 86.4000	
Best Prec@1: [58.730]	
Starting epoch number: 251 Learning rate: 0.0010000000000000002
Train: [251]	Time 15.746	Data 0.204	Loss 1.323	Prec@1 62.1460	Prec@5 88.9160	
Val: [251]	Time 1.294	Data 0.096	Loss 1.517	Prec@1 58.1000	Prec@5 86.4800	
Best Prec@1: [58.730]	
Starting epoch number: 252 Learning rate: 0.0010000000000000002
Train: [252]	Time 15.780	Data 0.199	Loss 1.318	Prec@1 61.9920	Prec@5 89.1680	
Val: [252]	Time 1.292	Data 0.102	Loss 1.507	Prec@1 58.4400	Prec@5 86.7600	
Best Prec@1: [58.730]	
Starting epoch number: 253 Learning rate: 0.0010000000000000002
Train: [253]	Time 16.018	Data 0.213	Loss 1.321	Prec@1 62.2660	Prec@5 89.0280	
Val: [253]	Time 1.292	Data 0.105	Loss 1.518	Prec@1 58.4500	Prec@5 86.4200	
Best Prec@1: [58.730]	
Starting epoch number: 254 Learning rate: 0.0010000000000000002
Train: [254]	Time 15.734	Data 0.210	Loss 1.322	Prec@1 62.2480	Prec@5 88.9540	
Val: [254]	Time 1.225	Data 0.095	Loss 1.507	Prec@1 58.5000	Prec@5 86.5300	
Best Prec@1: [58.730]	
Starting epoch number: 255 Learning rate: 0.0010000000000000002
Train: [255]	Time 15.631	Data 0.202	Loss 1.323	Prec@1 62.2140	Prec@5 88.8260	
Val: [255]	Time 1.254	Data 0.097	Loss 1.534	Prec@1 57.9900	Prec@5 86.4100	
Best Prec@1: [58.730]	
Starting epoch number: 256 Learning rate: 0.0010000000000000002
Train: [256]	Time 15.796	Data 0.198	Loss 1.319	Prec@1 62.4020	Prec@5 89.0600	
Val: [256]	Time 1.266	Data 0.092	Loss 1.511	Prec@1 58.4700	Prec@5 86.6800	
Best Prec@1: [58.730]	
Starting epoch number: 257 Learning rate: 0.0010000000000000002
Train: [257]	Time 15.866	Data 0.211	Loss 1.320	Prec@1 62.1420	Prec@5 88.9380	
Val: [257]	Time 1.248	Data 0.102	Loss 1.521	Prec@1 58.3800	Prec@5 86.3300	
Best Prec@1: [58.730]	
Starting epoch number: 258 Learning rate: 0.0010000000000000002
Train: [258]	Time 16.285	Data 0.201	Loss 1.320	Prec@1 62.1200	Prec@5 88.9800	
Val: [258]	Time 1.332	Data 0.097	Loss 1.513	Prec@1 58.3900	Prec@5 86.4800	
Best Prec@1: [58.730]	
Starting epoch number: 259 Learning rate: 0.0010000000000000002
Train: [259]	Time 15.656	Data 0.202	Loss 1.315	Prec@1 62.0980	Prec@5 89.0680	
Val: [259]	Time 1.349	Data 0.107	Loss 1.523	Prec@1 58.3900	Prec@5 86.4500	
Best Prec@1: [58.730]	
Starting epoch number: 260 Learning rate: 0.0010000000000000002
Train: [260]	Time 15.551	Data 0.209	Loss 1.319	Prec@1 62.2220	Prec@5 89.0460	
Val: [260]	Time 1.355	Data 0.116	Loss 1.518	Prec@1 58.3600	Prec@5 86.6200	
Best Prec@1: [58.730]	
Starting epoch number: 261 Learning rate: 0.0010000000000000002
Train: [261]	Time 15.807	Data 0.200	Loss 1.323	Prec@1 62.2540	Prec@5 89.0960	
Val: [261]	Time 1.210	Data 0.083	Loss 1.530	Prec@1 58.2400	Prec@5 86.5400	
Best Prec@1: [58.730]	
Starting epoch number: 262 Learning rate: 0.0010000000000000002
Train: [262]	Time 15.719	Data 0.202	Loss 1.317	Prec@1 62.4460	Prec@5 89.0700	
Val: [262]	Time 1.304	Data 0.099	Loss 1.528	Prec@1 58.1200	Prec@5 86.5100	
Best Prec@1: [58.730]	
Starting epoch number: 263 Learning rate: 0.0010000000000000002
Train: [263]	Time 15.580	Data 0.208	Loss 1.312	Prec@1 62.4720	Prec@5 89.1660	
Val: [263]	Time 1.273	Data 0.092	Loss 1.523	Prec@1 58.5100	Prec@5 86.3200	
Best Prec@1: [58.730]	
Starting epoch number: 264 Learning rate: 0.0010000000000000002
Train: [264]	Time 16.496	Data 0.215	Loss 1.313	Prec@1 62.4280	Prec@5 89.0280	
Val: [264]	Time 1.230	Data 0.092	Loss 1.516	Prec@1 58.3700	Prec@5 86.4300	
Best Prec@1: [58.730]	
Starting epoch number: 265 Learning rate: 0.0010000000000000002
Train: [265]	Time 15.962	Data 0.220	Loss 1.313	Prec@1 62.4120	Prec@5 89.1640	
Val: [265]	Time 1.319	Data 0.111	Loss 1.515	Prec@1 58.4600	Prec@5 86.3600	
Best Prec@1: [58.730]	
Starting epoch number: 266 Learning rate: 0.0010000000000000002
Train: [266]	Time 16.051	Data 0.209	Loss 1.317	Prec@1 62.3640	Prec@5 88.9900	
Val: [266]	Time 1.270	Data 0.096	Loss 1.523	Prec@1 58.5100	Prec@5 86.2600	
Best Prec@1: [58.730]	
Starting epoch number: 267 Learning rate: 0.0010000000000000002
Train: [267]	Time 15.697	Data 0.199	Loss 1.315	Prec@1 62.3320	Prec@5 89.0680	
Val: [267]	Time 1.270	Data 0.097	Loss 1.525	Prec@1 58.3000	Prec@5 86.2800	
Best Prec@1: [58.730]	
Starting epoch number: 268 Learning rate: 0.0010000000000000002
Train: [268]	Time 15.823	Data 0.213	Loss 1.313	Prec@1 62.4580	Prec@5 89.0980	
Val: [268]	Time 1.355	Data 0.109	Loss 1.514	Prec@1 58.4100	Prec@5 86.4600	
Best Prec@1: [58.730]	
Starting epoch number: 269 Learning rate: 0.0010000000000000002
Train: [269]	Time 15.779	Data 0.205	Loss 1.315	Prec@1 62.4720	Prec@5 89.1240	
Val: [269]	Time 1.298	Data 0.108	Loss 1.513	Prec@1 58.0000	Prec@5 86.4900	
Best Prec@1: [58.730]	
Starting epoch number: 270 Learning rate: 0.0010000000000000002
Train: [270]	Time 15.816	Data 0.199	Loss 1.313	Prec@1 62.5260	Prec@5 89.0420	
Val: [270]	Time 1.358	Data 0.101	Loss 1.526	Prec@1 58.1000	Prec@5 86.4000	
Best Prec@1: [58.730]	
Starting epoch number: 271 Learning rate: 0.0010000000000000002
Train: [271]	Time 15.803	Data 0.203	Loss 1.321	Prec@1 62.2800	Prec@5 89.0380	
Val: [271]	Time 1.285	Data 0.103	Loss 1.523	Prec@1 58.1500	Prec@5 86.3200	
Best Prec@1: [58.730]	
Starting epoch number: 272 Learning rate: 0.0010000000000000002
Train: [272]	Time 15.813	Data 0.212	Loss 1.314	Prec@1 62.5680	Prec@5 89.0460	
Val: [272]	Time 1.358	Data 0.097	Loss 1.526	Prec@1 57.9700	Prec@5 86.2200	
Best Prec@1: [58.730]	
Starting epoch number: 273 Learning rate: 0.0010000000000000002
Train: [273]	Time 15.677	Data 0.199	Loss 1.317	Prec@1 62.3340	Prec@5 89.0800	
Val: [273]	Time 1.284	Data 0.099	Loss 1.526	Prec@1 58.2700	Prec@5 86.5000	
Best Prec@1: [58.730]	
Starting epoch number: 274 Learning rate: 0.0010000000000000002
Train: [274]	Time 15.857	Data 0.210	Loss 1.314	Prec@1 62.3700	Prec@5 89.1920	
Val: [274]	Time 1.295	Data 0.100	Loss 1.519	Prec@1 58.5800	Prec@5 86.2700	
Best Prec@1: [58.730]	
Starting epoch number: 275 Learning rate: 0.0010000000000000002
Train: [275]	Time 15.816	Data 0.198	Loss 1.312	Prec@1 62.5240	Prec@5 89.1460	
Val: [275]	Time 1.291	Data 0.107	Loss 1.543	Prec@1 57.9200	Prec@5 86.1500	
Best Prec@1: [58.730]	
Starting epoch number: 276 Learning rate: 0.0010000000000000002
Train: [276]	Time 15.869	Data 0.204	Loss 1.316	Prec@1 62.3000	Prec@5 89.0360	
Val: [276]	Time 1.300	Data 0.098	Loss 1.510	Prec@1 58.3400	Prec@5 86.4000	
Best Prec@1: [58.730]	
Starting epoch number: 277 Learning rate: 0.0010000000000000002
Train: [277]	Time 15.991	Data 0.212	Loss 1.314	Prec@1 62.6340	Prec@5 89.1540	
Val: [277]	Time 1.297	Data 0.104	Loss 1.513	Prec@1 58.4600	Prec@5 86.6000	
Best Prec@1: [58.730]	
Starting epoch number: 278 Learning rate: 0.0010000000000000002
Train: [278]	Time 15.760	Data 0.209	Loss 1.319	Prec@1 62.1220	Prec@5 89.1060	
Val: [278]	Time 1.328	Data 0.099	Loss 1.516	Prec@1 58.2600	Prec@5 86.4800	
Best Prec@1: [58.730]	
Starting epoch number: 279 Learning rate: 0.0010000000000000002
Train: [279]	Time 16.265	Data 0.203	Loss 1.315	Prec@1 62.4880	Prec@5 89.0280	
Val: [279]	Time 1.310	Data 0.100	Loss 1.515	Prec@1 58.5400	Prec@5 86.5400	
Best Prec@1: [58.730]	
Starting epoch number: 280 Learning rate: 0.0010000000000000002
Train: [280]	Time 15.778	Data 0.200	Loss 1.314	Prec@1 62.4040	Prec@5 89.0040	
Val: [280]	Time 1.256	Data 0.097	Loss 1.532	Prec@1 57.9100	Prec@5 86.1100	
Best Prec@1: [58.730]	
Starting epoch number: 281 Learning rate: 0.0010000000000000002
Train: [281]	Time 16.079	Data 0.203	Loss 1.317	Prec@1 62.2600	Prec@5 89.0880	
Val: [281]	Time 1.321	Data 0.093	Loss 1.531	Prec@1 58.2600	Prec@5 86.2600	
Best Prec@1: [58.730]	
Starting epoch number: 282 Learning rate: 0.0010000000000000002
Train: [282]	Time 15.773	Data 0.198	Loss 1.315	Prec@1 62.4560	Prec@5 89.0540	
Val: [282]	Time 1.310	Data 0.100	Loss 1.528	Prec@1 58.2100	Prec@5 86.4100	
Best Prec@1: [58.730]	
Starting epoch number: 283 Learning rate: 0.0010000000000000002
Train: [283]	Time 16.253	Data 0.214	Loss 1.314	Prec@1 62.2900	Prec@5 89.1020	
Val: [283]	Time 1.260	Data 0.096	Loss 1.518	Prec@1 58.4300	Prec@5 86.3300	
Best Prec@1: [58.730]	
Starting epoch number: 284 Learning rate: 0.0010000000000000002
Train: [284]	Time 15.953	Data 0.207	Loss 1.316	Prec@1 62.2840	Prec@5 89.1920	
Val: [284]	Time 1.292	Data 0.105	Loss 1.522	Prec@1 58.2200	Prec@5 86.4800	
Best Prec@1: [58.730]	
Starting epoch number: 285 Learning rate: 0.0010000000000000002
Train: [285]	Time 16.025	Data 0.201	Loss 1.313	Prec@1 62.3020	Prec@5 89.0700	
Val: [285]	Time 1.257	Data 0.094	Loss 1.529	Prec@1 58.0300	Prec@5 86.1600	
Best Prec@1: [58.730]	
Starting epoch number: 286 Learning rate: 0.0010000000000000002
Train: [286]	Time 16.045	Data 0.202	Loss 1.313	Prec@1 62.5920	Prec@5 89.1200	
Val: [286]	Time 1.291	Data 0.100	Loss 1.521	Prec@1 58.1800	Prec@5 86.5100	
Best Prec@1: [58.730]	
Starting epoch number: 287 Learning rate: 0.0010000000000000002
Train: [287]	Time 15.889	Data 0.207	Loss 1.314	Prec@1 62.3600	Prec@5 89.0180	
Val: [287]	Time 1.269	Data 0.101	Loss 1.513	Prec@1 58.1900	Prec@5 86.2200	
Best Prec@1: [58.730]	
Starting epoch number: 288 Learning rate: 0.0010000000000000002
Train: [288]	Time 15.760	Data 0.203	Loss 1.309	Prec@1 62.4840	Prec@5 89.1940	
Val: [288]	Time 1.309	Data 0.101	Loss 1.517	Prec@1 57.9500	Prec@5 86.2100	
Best Prec@1: [58.730]	
Starting epoch number: 289 Learning rate: 0.0010000000000000002
Train: [289]	Time 15.685	Data 0.209	Loss 1.313	Prec@1 62.6180	Prec@5 89.2600	
Val: [289]	Time 1.262	Data 0.098	Loss 1.516	Prec@1 58.2700	Prec@5 86.1800	
Best Prec@1: [58.730]	
Starting epoch number: 290 Learning rate: 0.0010000000000000002
Train: [290]	Time 15.602	Data 0.209	Loss 1.315	Prec@1 62.3200	Prec@5 89.0120	
Val: [290]	Time 1.331	Data 0.108	Loss 1.531	Prec@1 58.0400	Prec@5 86.0900	
Best Prec@1: [58.730]	
Starting epoch number: 291 Learning rate: 0.0010000000000000002
Train: [291]	Time 15.600	Data 0.200	Loss 1.310	Prec@1 62.3020	Prec@5 89.3260	
Val: [291]	Time 1.297	Data 0.107	Loss 1.536	Prec@1 58.1500	Prec@5 86.5100	
Best Prec@1: [58.730]	
Starting epoch number: 292 Learning rate: 0.0010000000000000002
Train: [292]	Time 15.842	Data 0.204	Loss 1.314	Prec@1 62.4520	Prec@5 89.0560	
Val: [292]	Time 1.260	Data 0.095	Loss 1.526	Prec@1 57.9000	Prec@5 86.4800	
Best Prec@1: [58.730]	
Starting epoch number: 293 Learning rate: 0.0010000000000000002
Train: [293]	Time 15.977	Data 0.203	Loss 1.309	Prec@1 62.5140	Prec@5 89.1580	
Val: [293]	Time 1.243	Data 0.094	Loss 1.519	Prec@1 58.3200	Prec@5 86.1600	
Best Prec@1: [58.730]	
Starting epoch number: 294 Learning rate: 0.0010000000000000002
Train: [294]	Time 16.198	Data 0.210	Loss 1.308	Prec@1 62.4420	Prec@5 89.1480	
Val: [294]	Time 1.346	Data 0.108	Loss 1.525	Prec@1 58.5000	Prec@5 86.0400	
Best Prec@1: [58.730]	
Starting epoch number: 295 Learning rate: 0.0010000000000000002
Train: [295]	Time 16.120	Data 0.204	Loss 1.310	Prec@1 62.3440	Prec@5 89.2040	
Val: [295]	Time 1.363	Data 0.108	Loss 1.511	Prec@1 58.5400	Prec@5 86.2500	
Best Prec@1: [58.730]	
Starting epoch number: 296 Learning rate: 0.0010000000000000002
Train: [296]	Time 15.512	Data 0.197	Loss 1.309	Prec@1 62.4460	Prec@5 89.3220	
Val: [296]	Time 1.291	Data 0.102	Loss 1.516	Prec@1 58.2300	Prec@5 86.3600	
Best Prec@1: [58.730]	
Starting epoch number: 297 Learning rate: 0.0010000000000000002
Train: [297]	Time 16.115	Data 0.212	Loss 1.310	Prec@1 62.5720	Prec@5 89.0920	
Val: [297]	Time 1.292	Data 0.099	Loss 1.514	Prec@1 58.3000	Prec@5 86.5100	
Best Prec@1: [58.730]	
Starting epoch number: 298 Learning rate: 0.0010000000000000002
Train: [298]	Time 15.743	Data 0.201	Loss 1.313	Prec@1 62.4860	Prec@5 89.0680	
Val: [298]	Time 1.316	Data 0.105	Loss 1.528	Prec@1 58.2600	Prec@5 86.3400	
Best Prec@1: [58.730]	
Starting epoch number: 299 Learning rate: 0.0010000000000000002
Train: [299]	Time 15.872	Data 0.208	Loss 1.314	Prec@1 62.4160	Prec@5 89.0260	
Val: [299]	Time 1.283	Data 0.102	Loss 1.527	Prec@1 58.0800	Prec@5 86.2200	
Best Prec@1: [58.730]	
